# Claude Code 教程系列 35：AI模型集成与定制

## 引言

在现代软件开发中，AI模型的集成与定制已成为构建智能应用的关键环节。Claude Code 作为强大的AI编程助手，不仅能够帮助开发者快速集成各种AI模型，还能指导我们进行模型的定制化开发。本文将深入探讨如何使用 Claude Code 来实现AI模型的集成、微调和定制化部署。

## 目录
1. [AI模型集成基础](#ai模型集成基础)
2. [Claude API深度集成](#claude-api深度集成)
3. [多模型架构设计](#多模型架构设计)
4. [模型微调与定制](#模型微调与定制)
5. [RAG系统构建](#rag系统构建)
6. [模型性能优化](#模型性能优化)
7. [生产环境部署](#生产环境部署)
8. [实战案例分析](#实战案例分析)

## AI模型集成基础

### 1.1 AI模型生态概览

让我们首先了解当前AI模型的生态系统，并使用 Claude Code 创建一个模型管理框架：

```python
# models/ai_model_manager.py
import asyncio
import logging
from typing import Dict, Any, Optional, List
from enum import Enum
from dataclasses import dataclass
from abc import ABC, abstractmethod

class ModelType(Enum):
    """AI模型类型枚举"""
    LANGUAGE_MODEL = "language_model"
    EMBEDDING_MODEL = "embedding_model"
    VISION_MODEL = "vision_model"
    AUDIO_MODEL = "audio_model"
    MULTIMODAL_MODEL = "multimodal_model"

class ModelProvider(Enum):
    """模型提供商枚举"""
    ANTHROPIC = "anthropic"
    OPENAI = "openai"
    HUGGINGFACE = "huggingface"
    GOOGLE = "google"
    LOCAL = "local"

@dataclass
class ModelConfig:
    """模型配置类"""
    name: str
    provider: ModelProvider
    model_type: ModelType
    api_endpoint: Optional[str] = None
    api_key: Optional[str] = None
    max_tokens: int = 4096
    temperature: float = 0.7
    context_window: int = 100000
    cost_per_token: float = 0.0
    features: List[str] = None

    def __post_init__(self):
        if self.features is None:
            self.features = []

class BaseAIModel(ABC):
    """AI模型基类"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.logger = logging.getLogger(f"{__name__}.{config.name}")
        
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        """生成文本"""
        pass
    
    @abstractmethod
    async def embed(self, text: str) -> List[float]:
        """生成嵌入向量"""
        pass
    
    @abstractmethod
    def validate_config(self) -> bool:
        """验证配置"""
        pass

class ModelManager:
    """AI模型管理器"""
    
    def __init__(self):
        self.models: Dict[str, BaseAIModel] = {}
        self.default_model: Optional[str] = None
        self.logger = logging.getLogger(__name__)
    
    def register_model(self, model: BaseAIModel) -> None:
        """注册模型"""
        if model.validate_config():
            self.models[model.config.name] = model
            if self.default_model is None:
                self.default_model = model.config.name
            self.logger.info(f"已注册模型: {model.config.name}")
        else:
            raise ValueError(f"模型配置无效: {model.config.name}")
    
    def get_model(self, name: Optional[str] = None) -> BaseAIModel:
        """获取模型实例"""
        model_name = name or self.default_model
        if model_name not in self.models:
            raise ValueError(f"模型未找到: {model_name}")
        return self.models[model_name]
    
    def list_models(self) -> List[str]:
        """列出所有注册的模型"""
        return list(self.models.keys())
    
    async def health_check(self) -> Dict[str, bool]:
        """模型健康检查"""
        results = {}
        for name, model in self.models.items():
            try:
                # 简单的健康检查
                await model.generate("Hello", max_tokens=1)
                results[name] = True
            except Exception as e:
                self.logger.error(f"模型 {name} 健康检查失败: {e}")
                results[name] = False
        return results

# 使用示例
model_manager = ModelManager()
```

### 1.2 Claude API集成实现

现在让我们实现 Claude API 的具体集成：

```python
# models/claude_model.py
import aiohttp
import json
from typing import Dict, Any, List, Optional, AsyncGenerator
from models.ai_model_manager import BaseAIModel, ModelConfig, ModelProvider

class ClaudeModel(BaseAIModel):
    """Claude模型实现"""
    
    def __init__(self, config: ModelConfig):
        super().__init__(config)
        self.session: Optional[aiohttp.ClientSession] = None
        self.base_url = "https://api.anthropic.com/v1"
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    def validate_config(self) -> bool:
        """验证Claude配置"""
        required_fields = ['name', 'api_key']
        for field in required_fields:
            if not hasattr(self.config, field) or not getattr(self.config, field):
                return False
        return True
    
    async def generate(self, prompt: str, **kwargs) -> str:
        """生成文本"""
        if not self.session:
            self.session = aiohttp.ClientSession()
            
        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.config.api_key,
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": self.config.name,
            "max_tokens": kwargs.get('max_tokens', self.config.max_tokens),
            "temperature": kwargs.get('temperature', self.config.temperature),
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }
        
        try:
            async with self.session.post(
                f"{self.base_url}/messages",
                headers=headers,
                json=payload
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return result['content'][0]['text']
                else:
                    error_text = await response.text()
                    raise Exception(f"Claude API错误: {response.status} - {error_text}")
                    
        except Exception as e:
            self.logger.error(f"Claude生成失败: {e}")
            raise
    
    async def generate_stream(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        """流式生成文本"""
        if not self.session:
            self.session = aiohttp.ClientSession()
            
        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.config.api_key,
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": self.config.name,
            "max_tokens": kwargs.get('max_tokens', self.config.max_tokens),
            "temperature": kwargs.get('temperature', self.config.temperature),
            "stream": True,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }
        
        try:
            async with self.session.post(
                f"{self.base_url}/messages",
                headers=headers,
                json=payload
            ) as response:
                if response.status == 200:
                    async for line in response.content:
                        line = line.decode('utf-8').strip()
                        if line.startswith('data: '):
                            data = line[6:]
                            if data == '[DONE]':
                                break
                            try:
                                chunk = json.loads(data)
                                if chunk.get('type') == 'content_block_delta':
                                    if 'delta' in chunk and 'text' in chunk['delta']:
                                        yield chunk['delta']['text']
                            except json.JSONDecodeError:
                                continue
                else:
                    error_text = await response.text()
                    raise Exception(f"Claude Stream API错误: {response.status} - {error_text}")
                    
        except Exception as e:
            self.logger.error(f"Claude流式生成失败: {e}")
            raise
    
    async def embed(self, text: str) -> List[float]:
        """Claude目前不直接提供嵌入功能，这里抛出异常"""
        raise NotImplementedError("Claude模型不支持嵌入功能")
    
    async def function_calling(self, prompt: str, functions: List[Dict[str, Any]], **kwargs) -> Dict[str, Any]:
        """Claude函数调用"""
        if not self.session:
            self.session = aiohttp.ClientSession()
            
        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.config.api_key,
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": self.config.name,
            "max_tokens": kwargs.get('max_tokens', self.config.max_tokens),
            "temperature": kwargs.get('temperature', self.config.temperature),
            "tools": functions,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }
        
        try:
            async with self.session.post(
                f"{self.base_url}/messages",
                headers=headers,
                json=payload
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    return result
                else:
                    error_text = await response.text()
                    raise Exception(f"Claude函数调用错误: {response.status} - {error_text}")
                    
        except Exception as e:
            self.logger.error(f"Claude函数调用失败: {e}")
            raise

# Claude模型配置示例
def create_claude_config() -> ModelConfig:
    """创建Claude模型配置"""
    return ModelConfig(
        name="claude-3-sonnet-20240229",
        provider=ModelProvider.ANTHROPIC,
        model_type=ModelType.LANGUAGE_MODEL,
        api_key="your-api-key-here",
        max_tokens=4096,
        temperature=0.7,
        context_window=200000,
        cost_per_token=0.000015,
        features=["text_generation", "function_calling", "vision"]
    )

# 使用示例
async def test_claude_integration():
    """测试Claude集成"""
    config = create_claude_config()
    
    async with ClaudeModel(config) as claude:
        # 基本文本生成
        response = await claude.generate("解释什么是机器学习")
        print(f"Claude响应: {response}")
        
        # 流式生成
        print("流式响应:")
        async for chunk in claude.generate_stream("写一首关于编程的诗"):
            print(chunk, end='', flush=True)
        print()
        
        # 函数调用示例
        functions = [
            {
                "name": "get_weather",
                "description": "获取天气信息",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "location": {
                            "type": "string",
                            "description": "城市名称"
                        }
                    },
                    "required": ["location"]
                }
            }
        ]
        
        function_response = await claude.function_calling(
            "北京今天天气如何？",
            functions
        )
        print(f"函数调用响应: {function_response}")

# 运行测试
# asyncio.run(test_claude_integration())
```

## Claude API深度集成

### 2.1 高级API功能实现

让我们实现Claude API的高级功能，包括多轮对话、上下文管理和错误处理：

```python
# models/advanced_claude.py
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import asyncio
import hashlib
from models.claude_model import ClaudeModel

@dataclass
class ConversationMessage:
    """对话消息"""
    role: str  # "user", "assistant", "system"
    content: str
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ConversationContext:
    """对话上下文"""
    conversation_id: str
    messages: List[ConversationMessage] = field(default_factory=list)
    max_context_length: int = 100000
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)

class AdvancedClaudeModel(ClaudeModel):
    """高级Claude模型，支持对话管理和上下文缓存"""
    
    def __init__(self, config):
        super().__init__(config)
        self.conversations: Dict[str, ConversationContext] = {}
        self.context_cache: Dict[str, Any] = {}
        self.cache_ttl = timedelta(hours=1)
    
    def create_conversation(self, conversation_id: Optional[str] = None) -> str:
        """创建新对话"""
        if conversation_id is None:
            conversation_id = hashlib.md5(
                f"{datetime.now().isoformat()}".encode()
            ).hexdigest()[:12]
        
        self.conversations[conversation_id] = ConversationContext(
            conversation_id=conversation_id
        )
        
        self.logger.info(f"创建对话: {conversation_id}")
        return conversation_id
    
    def add_message(self, conversation_id: str, role: str, content: str, metadata: Dict[str, Any] = None):
        """添加消息到对话"""
        if conversation_id not in self.conversations:
            raise ValueError(f"对话不存在: {conversation_id}")
        
        message = ConversationMessage(
            role=role,
            content=content,
            metadata=metadata or {}
        )
        
        context = self.conversations[conversation_id]
        context.messages.append(message)
        context.updated_at = datetime.now()
        
        # 管理上下文长度
        self._manage_context_length(context)
    
    def _manage_context_length(self, context: ConversationContext):
        """管理上下文长度，防止超出限制"""
        total_length = sum(len(msg.content) for msg in context.messages)
        
        while total_length > context.max_context_length and len(context.messages) > 1:
            # 保留系统消息，删除最早的用户/助手消息
            for i, msg in enumerate(context.messages):
                if msg.role != "system":
                    removed_msg = context.messages.pop(i)
                    total_length -= len(removed_msg.content)
                    self.logger.info(f"移除消息以控制上下文长度: {removed_msg.content[:50]}...")
                    break
    
    async def chat(self, conversation_id: str, user_input: str, **kwargs) -> str:
        """进行对话"""
        if conversation_id not in self.conversations:
            raise ValueError(f"对话不存在: {conversation_id}")
        
        # 添加用户消息
        self.add_message(conversation_id, "user", user_input)
        
        # 构建消息列表
        context = self.conversations[conversation_id]
        messages = []
        
        for msg in context.messages:
            messages.append({
                "role": msg.role,
                "content": msg.content
            })
        
        # 调用Claude API
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.config.api_key,
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": self.config.name,
            "max_tokens": kwargs.get('max_tokens', self.config.max_tokens),
            "temperature": kwargs.get('temperature', self.config.temperature),
            "messages": messages
        }
        
        try:
            async with self.session.post(
                f"{self.base_url}/messages",
                headers=headers,
                json=payload
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    assistant_response = result['content'][0]['text']
                    
                    # 添加助手响应
                    self.add_message(conversation_id, "assistant", assistant_response)
                    
                    return assistant_response
                else:
                    error_text = await response.text()
                    raise Exception(f"Claude Chat API错误: {response.status} - {error_text}")
                    
        except Exception as e:
            self.logger.error(f"Claude对话失败: {e}")
            raise
    
    def get_conversation_history(self, conversation_id: str) -> List[ConversationMessage]:
        """获取对话历史"""
        if conversation_id not in self.conversations:
            raise ValueError(f"对话不存在: {conversation_id}")
        
        return self.conversations[conversation_id].messages.copy()
    
    def clear_conversation(self, conversation_id: str):
        """清空对话"""
        if conversation_id in self.conversations:
            self.conversations[conversation_id].messages.clear()
            self.logger.info(f"清空对话: {conversation_id}")
    
    def delete_conversation(self, conversation_id: str):
        """删除对话"""
        if conversation_id in self.conversations:
            del self.conversations[conversation_id]
            self.logger.info(f"删除对话: {conversation_id}")
    
    async def summarize_conversation(self, conversation_id: str) -> str:
        """总结对话内容"""
        if conversation_id not in self.conversations:
            raise ValueError(f"对话不存在: {conversation_id}")
        
        context = self.conversations[conversation_id]
        if not context.messages:
            return "对话为空"
        
        # 构建总结提示
        conversation_text = ""
        for msg in context.messages:
            if msg.role != "system":
                conversation_text += f"{msg.role.upper()}: {msg.content}\n\n"
        
        summary_prompt = f"""请总结以下对话的主要内容和要点：

{conversation_text}

总结要求：
1. 简洁明了，突出重点
2. 包含主要话题和结论
3. 长度控制在200字以内
"""
        
        # 使用临时对话进行总结
        temp_conversation_id = self.create_conversation()
        try:
            summary = await self.chat(temp_conversation_id, summary_prompt)
            return summary
        finally:
            self.delete_conversation(temp_conversation_id)

# 多模型协作管理器
class MultiModelOrchestrator:
    """多模型协作管理器"""
    
    def __init__(self, model_manager):
        self.model_manager = model_manager
        self.routing_rules: Dict[str, str] = {}
        self.logger = logging.getLogger(__name__)
    
    def add_routing_rule(self, pattern: str, model_name: str):
        """添加路由规则"""
        self.routing_rules[pattern] = model_name
    
    def route_request(self, prompt: str) -> str:
        """根据提示内容路由到合适的模型"""
        prompt_lower = prompt.lower()
        
        # 简单的关键词匹配路由
        for pattern, model_name in self.routing_rules.items():
            if pattern.lower() in prompt_lower:
                return model_name
        
        # 默认返回第一个可用模型
        models = self.model_manager.list_models()
        return models[0] if models else None
    
    async def collaborative_generation(self, 
                                     prompt: str, 
                                     model_names: List[str],
                                     voting_strategy: str = "majority") -> str:
        """多模型协作生成"""
        responses = []
        
        # 并行调用多个模型
        tasks = []
        for model_name in model_names:
            model = self.model_manager.get_model(model_name)
            task = model.generate(prompt)
            tasks.append(task)
        
        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if not isinstance(result, Exception):
                    responses.append({
                        'model': model_names[i],
                        'response': result
                    })
                else:
                    self.logger.error(f"模型 {model_names[i]} 生成失败: {result}")
        
        except Exception as e:
            self.logger.error(f"协作生成失败: {e}")
            raise
        
        # 应用投票策略
        if voting_strategy == "majority":
            # 简单实现：返回第一个成功的响应
            return responses[0]['response'] if responses else "所有模型生成失败"
        elif voting_strategy == "best_quality":
            # 可以基于质量评分选择最佳响应
            return self._select_best_response(responses)
        else:
            return responses[0]['response'] if responses else "所有模型生成失败"
    
    def _select_best_response(self, responses: List[Dict[str, Any]]) -> str:
        """选择最佳响应（简单实现）"""
        if not responses:
            return "无可用响应"
        
        # 简单策略：选择最长的响应（假设更详细的响应质量更好）
        best_response = max(responses, key=lambda x: len(x['response']))
        return best_response['response']

# 使用示例
async def test_advanced_claude():
    """测试高级Claude功能"""
    config = create_claude_config()
    
    async with AdvancedClaudeModel(config) as advanced_claude:
        # 创建对话
        conv_id = advanced_claude.create_conversation()
        
        # 多轮对话
        response1 = await advanced_claude.chat(conv_id, "你好，我想学习Python编程")
        print(f"Claude: {response1}")
        
        response2 = await advanced_claude.chat(conv_id, "从哪里开始学习比较好？")
        print(f"Claude: {response2}")
        
        response3 = await advanced_claude.chat(conv_id, "推荐一些练习项目")
        print(f"Claude: {response3}")
        
        # 查看对话历史
        history = advanced_claude.get_conversation_history(conv_id)
        print(f"对话历史包含 {len(history)} 条消息")
        
        # 总结对话
        summary = await advanced_claude.summarize_conversation(conv_id)
        print(f"对话总结: {summary}")

# 运行测试
# asyncio.run(test_advanced_claude())
```

## 多模型架构设计

### 3.1 模型适配器模式

让我们实现一个统一的模型适配器，支持不同提供商的模型：

```python
# models/model_adapters.py
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, AsyncGenerator
import aiohttp
import json
from models.ai_model_manager import BaseAIModel, ModelConfig

class OpenAIAdapter(BaseAIModel):
    """OpenAI模型适配器"""
    
    def __init__(self, config: ModelConfig):
        super().__init__(config)
        self.base_url = "https://api.openai.com/v1"
    
    def validate_config(self) -> bool:
        return bool(self.config.api_key)
    
    async def generate(self, prompt: str, **kwargs) -> str:
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.config.name,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": kwargs.get('max_tokens', self.config.max_tokens),
            "temperature": kwargs.get('temperature', self.config.temperature)
        }
        
        async with self.session.post(
            f"{self.base_url}/chat/completions",
            headers=headers,
            json=payload
        ) as response:
            if response.status == 200:
                result = await response.json()
                return result['choices'][0]['message']['content']
            else:
                error_text = await response.text()
                raise Exception(f"OpenAI API错误: {response.status} - {error_text}")
    
    async def embed(self, text: str) -> List[float]:
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "input": text,
            "model": "text-embedding-ada-002"
        }
        
        async with self.session.post(
            f"{self.base_url}/embeddings",
            headers=headers,
            json=payload
        ) as response:
            if response.status == 200:
                result = await response.json()
                return result['data'][0]['embedding']
            else:
                error_text = await response.text()
                raise Exception(f"OpenAI Embedding API错误: {response.status} - {error_text}")

class HuggingFaceAdapter(BaseAIModel):
    """HuggingFace模型适配器"""
    
    def __init__(self, config: ModelConfig):
        super().__init__(config)
        self.base_url = "https://api-inference.huggingface.co/models"
    
    def validate_config(self) -> bool:
        return bool(self.config.api_key)
    
    async def generate(self, prompt: str, **kwargs) -> str:
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": kwargs.get('max_tokens', self.config.max_tokens),
                "temperature": kwargs.get('temperature', self.config.temperature)
            }
        }
        
        async with self.session.post(
            f"{self.base_url}/{self.config.name}",
            headers=headers,
            json=payload
        ) as response:
            if response.status == 200:
                result = await response.json()
                return result[0]['generated_text']
            else:
                error_text = await response.text()
                raise Exception(f"HuggingFace API错误: {response.status} - {error_text}")
    
    async def embed(self, text: str) -> List[float]:
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {"inputs": text}
        
        async with self.session.post(
            f"{self.base_url}/sentence-transformers/all-MiniLM-L6-v2",
            headers=headers,
            json=payload
        ) as response:
            if response.status == 200:
                result = await response.json()
                return result
            else:
                error_text = await response.text()
                raise Exception(f"HuggingFace Embedding API错误: {response.status} - {error_text}")

# 模型工厂
class ModelFactory:
    """模型工厂类"""
    
    @staticmethod
    def create_model(config: ModelConfig) -> BaseAIModel:
        """根据配置创建模型实例"""
        if config.provider == ModelProvider.ANTHROPIC:
            return ClaudeModel(config)
        elif config.provider == ModelProvider.OPENAI:
            return OpenAIAdapter(config)
        elif config.provider == ModelProvider.HUGGINGFACE:
            return HuggingFaceAdapter(config)
        else:
            raise ValueError(f"不支持的模型提供商: {config.provider}")
    
    @staticmethod
    def create_from_dict(config_dict: Dict[str, Any]) -> BaseAIModel:
        """从字典创建模型"""
        config = ModelConfig(**config_dict)
        return ModelFactory.create_model(config)

# 配置管理
class ModelConfigManager:
    """模型配置管理器"""
    
    def __init__(self, config_file: Optional[str] = None):
        self.config_file = config_file
        self.configs: Dict[str, ModelConfig] = {}
        if config_file:
            self.load_from_file(config_file)
    
    def load_from_file(self, file_path: str):
        """从文件加载配置"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            for name, config_data in data.items():
                config = ModelConfig(**config_data)
                self.configs[name] = config
                
        except Exception as e:
            self.logger.error(f"加载配置文件失败: {e}")
            raise
    
    def save_to_file(self, file_path: str):
        """保存配置到文件"""
        try:
            data = {}
            for name, config in self.configs.items():
                data[name] = {
                    'name': config.name,
                    'provider': config.provider.value,
                    'model_type': config.model_type.value,
                    'api_endpoint': config.api_endpoint,
                    'max_tokens': config.max_tokens,
                    'temperature': config.temperature,
                    'context_window': config.context_window,
                    'cost_per_token': config.cost_per_token,
                    'features': config.features
                }
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            self.logger.error(f"保存配置文件失败: {e}")
            raise
    
    def add_config(self, name: str, config: ModelConfig):
        """添加配置"""
        self.configs[name] = config
    
    def get_config(self, name: str) -> ModelConfig:
        """获取配置"""
        if name not in self.configs:
            raise ValueError(f"配置不存在: {name}")
        return self.configs[name]
    
    def list_configs(self) -> List[str]:
        """列出所有配置"""
        return list(self.configs.keys())

# 示例配置文件 models_config.json
model_configs_example = {
    "claude-sonnet": {
        "name": "claude-3-sonnet-20240229",
        "provider": "anthropic",
        "model_type": "language_model",
        "api_key": "${ANTHROPIC_API_KEY}",
        "max_tokens": 4096,
        "temperature": 0.7,
        "context_window": 200000,
        "cost_per_token": 0.000015,
        "features": ["text_generation", "function_calling"]
    },
    "gpt-4": {
        "name": "gpt-4-turbo-preview",
        "provider": "openai",
        "model_type": "language_model",
        "api_key": "${OPENAI_API_KEY}",
        "max_tokens": 4096,
        "temperature": 0.7,
        "context_window": 128000,
        "cost_per_token": 0.00003,
        "features": ["text_generation", "function_calling", "vision"]
    }
}
```

## 模型微调与定制

### 4.1 微调流程实现

让我们实现一个完整的模型微调流程：

```python
# fine_tuning/fine_tuning_manager.py
import json
import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import pandas as pd
from datetime import datetime

@dataclass
class FineTuningConfig:
    """微调配置"""
    model_name: str
    training_file: str
    validation_file: Optional[str] = None
    learning_rate: float = 0.0001
    batch_size: int = 4
    num_epochs: int = 3
    max_seq_length: int = 512
    output_dir: str = "./fine_tuned_models"
    save_steps: int = 500
    eval_steps: int = 100
    warmup_steps: int = 100

@dataclass
class TrainingData:
    """训练数据"""
    input_text: str
    target_text: str
    metadata: Dict[str, Any] = None

class DataPreprocessor:
    """数据预处理器"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def prepare_conversation_data(self, conversations: List[Dict[str, Any]]) -> List[TrainingData]:
        """准备对话数据"""
        training_data = []
        
        for conv in conversations:
            messages = conv.get('messages', [])
            
            # 构建对话上下文
            context = ""
            for i, msg in enumerate(messages):
                if msg['role'] == 'user':
                    context += f"用户: {msg['content']}\n"
                    
                    # 寻找对应的助手回复
                    if i + 1 < len(messages) and messages[i + 1]['role'] == 'assistant':
                        assistant_response = messages[i + 1]['content']
                        
                        training_data.append(TrainingData(
                            input_text=context.strip(),
                            target_text=assistant_response,
                            metadata={'conversation_id': conv.get('id')}
                        ))
                        
                        context += f"助手: {assistant_response}\n"
        
        return training_data
    
    def prepare_classification_data(self, data: List[Dict[str, Any]]) -> List[TrainingData]:
        """准备分类数据"""
        training_data = []
        
        for item in data:
            text = item.get('text', '')
            label = item.get('label', '')
            
            training_data.append(TrainingData(
                input_text=text,
                target_text=label,
                metadata={'category': item.get('category')}
            ))
        
        return training_data
    
    def save_training_data(self, data: List[TrainingData], file_path: str, format: str = 'jsonl'):
        """保存训练数据"""
        if format == 'jsonl':
            with open(file_path, 'w', encoding='utf-8') as f:
                for item in data:
                    record = {
                        'input': item.input_text,
                        'output': item.target_text
                    }
                    if item.metadata:
                        record['metadata'] = item.metadata
                    
                    f.write(json.dumps(record, ensure_ascii=False) + '\n')
        
        elif format == 'csv':
            df = pd.DataFrame([
                {
                    'input': item.input_text,
                    'output': item.target_text,
                    'metadata': json.dumps(item.metadata) if item.metadata else ''
                }
                for item in data
            ])
            df.to_csv(file_path, index=False, encoding='utf-8')
        
        self.logger.info(f"训练数据已保存到: {file_path}")

class FineTuningManager:
    """微调管理器"""
    
    def __init__(self, config: FineTuningConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.training_history: List[Dict[str, Any]] = []
    
    async def prepare_data(self, raw_data: List[Dict[str, Any]], data_type: str = 'conversation') -> str:
        """准备训练数据"""
        preprocessor = DataPreprocessor()
        
        if data_type == 'conversation':
            training_data = preprocessor.prepare_conversation_data(raw_data)
        elif data_type == 'classification':
            training_data = preprocessor.prepare_classification_data(raw_data)
        else:
            raise ValueError(f"不支持的数据类型: {data_type}")
        
        # 数据分割
        train_size = int(len(training_data) * 0.8)
        train_data = training_data[:train_size]
        val_data = training_data[train_size:]
        
        # 保存数据
        train_file = f"{self.config.output_dir}/train.jsonl"
        val_file = f"{self.config.output_dir}/val.jsonl"
        
        Path(self.config.output_dir).mkdir(parents=True, exist_ok=True)
        
        preprocessor.save_training_data(train_data, train_file)
        preprocessor.save_training_data(val_data, val_file)
        
        self.config.training_file = train_file
        self.config.validation_file = val_file
        
        return train_file
    
    async def start_fine_tuning(self, model_manager, base_model_name: str) -> str:
        """开始微调"""
        job_id = f"ft-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        
        try:
            # 获取基础模型
            base_model = model_manager.get_model(base_model_name)
            
            # 检查是否支持微调
            if not hasattr(base_model, 'fine_tune'):
                raise NotImplementedError(f"模型 {base_model_name} 不支持微调")
            
            # 开始微调过程
            self.logger.info(f"开始微调任务: {job_id}")
            
            training_log = {
                'job_id': job_id,
                'base_model': base_model_name,
                'config': self.config.__dict__,
                'start_time': datetime.now().isoformat(),
                'status': 'training'
            }
            
            self.training_history.append(training_log)
            
            # 模拟微调过程（实际实现需要根据具体模型API）
            await self._simulate_training_process(job_id, base_model)
            
            # 更新训练日志
            training_log['status'] = 'completed'
            training_log['end_time'] = datetime.now().isoformat()
            training_log['model_path'] = f"{self.config.output_dir}/{job_id}"
            
            return job_id
            
        except Exception as e:
            self.logger.error(f"微调失败: {e}")
            # 更新失败状态
            if self.training_history:
                self.training_history[-1]['status'] = 'failed'
                self.training_history[-1]['error'] = str(e)
            raise
    
    async def _simulate_training_process(self, job_id: str, base_model):
        """模拟训练过程"""
        total_steps = self.config.num_epochs * 100  # 假设每个epoch 100步
        
        for step in range(1, total_steps + 1):
            # 模拟训练步骤
            await asyncio.sleep(0.01)  # 模拟训练时间
            
            if step % self.config.eval_steps == 0:
                # 模拟评估
                loss = 2.0 * (1 - step / total_steps)  # 模拟损失下降
                accuracy = 0.5 + 0.4 * (step / total_steps)  # 模拟准确率提升
                
                self.logger.info(f"Step {step}/{total_steps} - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}")
            
            if step % self.config.save_steps == 0:
                # 模拟保存检查点
                checkpoint_path = f"{self.config.output_dir}/{job_id}/checkpoint-{step}"
                Path(checkpoint_path).mkdir(parents=True, exist_ok=True)
                self.logger.info(f"保存检查点: {checkpoint_path}")
    
    def get_training_status(self, job_id: str) -> Dict[str, Any]:
        """获取训练状态"""
        for log in self.training_history:
            if log['job_id'] == job_id:
                return log
        
        raise ValueError(f"训练任务不存在: {job_id}")
    
    def list_training_jobs(self) -> List[Dict[str, Any]]:
        """列出所有训练任务"""
        return self.training_history.copy()

# RAG系统实现
class RAGSystem:
    """检索增强生成系统"""
    
    def __init__(self, embedding_model, vector_store, language_model):
        self.embedding_model = embedding_model
        self.vector_store = vector_store
        self.language_model = language_model
        self.logger = logging.getLogger(__name__)
    
    async def add_documents(self, documents: List[Dict[str, Any]]):
        """添加文档到向量数据库"""
        for doc in documents:
            text = doc.get('content', '')
            metadata = doc.get('metadata', {})
            
            # 生成嵌入向量
            embedding = await self.embedding_model.embed(text)
            
            # 存储到向量数据库
            await self.vector_store.add_document(
                text=text,
                embedding=embedding,
                metadata=metadata
            )
        
        self.logger.info(f"已添加 {len(documents)} 个文档")
    
    async def retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """检索相关文档"""
        # 生成查询向量
        query_embedding = await self.embedding_model.embed(query)
        
        # 向量搜索
        similar_docs = await self.vector_store.search(
            query_embedding=query_embedding,
            top_k=top_k
        )
        
        return similar_docs
    
    async def generate_with_context(self, query: str, context_docs: List[Dict[str, Any]]) -> str:
        """基于上下文生成回答"""
        # 构建上下文
        context = ""
        for i, doc in enumerate(context_docs):
            context += f"文档{i+1}: {doc['content']}\n\n"
        
        # 构建提示
        prompt = f"""基于以下上下文信息回答用户问题：

上下文：
{context}

用户问题：{query}

请基于上下文信息给出准确、详细的回答。如果上下文中没有相关信息，请明确说明。
"""
        
        # 生成回答
        response = await self.language_model.generate(prompt)
        return response
    
    async def rag_query(self, query: str, top_k: int = 5) -> Dict[str, Any]:
        """完整的RAG查询流程"""
        # 检索相关文档
        retrieved_docs = await self.retrieve(query, top_k)
        
        # 生成回答
        answer = await self.generate_with_context(query, retrieved_docs)
        
        return {
            'query': query,
            'answer': answer,
            'retrieved_documents': retrieved_docs,
            'timestamp': datetime.now().isoformat()
        }

# 向量数据库接口
class VectorStore(ABC):
    """向量数据库抽象接口"""
    
    @abstractmethod
    async def add_document(self, text: str, embedding: List[float], metadata: Dict[str, Any]):
        """添加文档"""
        pass
    
    @abstractmethod
    async def search(self, query_embedding: List[float], top_k: int) -> List[Dict[str, Any]]:
        """搜索相似文档"""
        pass
    
    @abstractmethod
    async def delete_document(self, doc_id: str):
        """删除文档"""
        pass

# 简单的内存向量存储实现
class InMemoryVectorStore(VectorStore):
    """内存向量存储"""
    
    def __init__(self):
        self.documents: List[Dict[str, Any]] = []
        self.next_id = 1
    
    async def add_document(self, text: str, embedding: List[float], metadata: Dict[str, Any]):
        """添加文档"""
        doc = {
            'id': str(self.next_id),
            'text': text,
            'embedding': embedding,
            'metadata': metadata
        }
        self.documents.append(doc)
        self.next_id += 1
    
    async def search(self, query_embedding: List[float], top_k: int) -> List[Dict[str, Any]]:
        """搜索相似文档"""
        # 计算余弦相似度
        similarities = []
        
        for doc in self.documents:
            doc_embedding = doc['embedding']
            similarity = self._cosine_similarity(query_embedding, doc_embedding)
            similarities.append((similarity, doc))
        
        # 按相似度排序
        similarities.sort(key=lambda x: x[0], reverse=True)
        
        # 返回top_k个结果
        results = []
        for similarity, doc in similarities[:top_k]:
            results.append({
                'content': doc['text'],
                'metadata': doc['metadata'],
                'similarity': similarity
            })
        
        return results
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """计算余弦相似度"""
        import math
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = math.sqrt(sum(a * a for a in vec1))
        magnitude2 = math.sqrt(sum(a * a for a in vec2))
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0
        
        return dot_product / (magnitude1 * magnitude2)
    
    async def delete_document(self, doc_id: str):
        """删除文档"""
        self.documents = [doc for doc in self.documents if doc['id'] != doc_id]

# 使用示例
async def test_fine_tuning_and_rag():
    """测试微调和RAG系统"""
    
    # 1. 微调示例
    fine_tuning_config = FineTuningConfig(
        model_name="custom-claude",
        training_file="",
        num_epochs=3,
        learning_rate=0.0001
    )
    
    fine_tuning_manager = FineTuningManager(fine_tuning_config)
    
    # 准备训练数据
    training_conversations = [
        {
            'id': 'conv1',
            'messages': [
                {'role': 'user', 'content': '什么是机器学习？'},
                {'role': 'assistant', 'content': '机器学习是一种人工智能技术...'}
            ]
        }
    ]
    
    train_file = await fine_tuning_manager.prepare_data(
        training_conversations, 
        'conversation'
    )
    print(f"训练数据准备完成: {train_file}")
    
    # 2. RAG系统示例
    # 创建向量存储
    vector_store = InMemoryVectorStore()
    
    # 模拟嵌入模型（实际使用中替换为真实模型）
    class MockEmbeddingModel:
        async def embed(self, text: str) -> List[float]:
            # 简单的文本哈希作为嵌入向量（实际使用中替换为真实嵌入）
            import hashlib
            hash_obj = hashlib.md5(text.encode())
            hash_hex = hash_obj.hexdigest()
            # 将哈希转换为浮点数向量
            return [float(int(hash_hex[i:i+2], 16)) / 255.0 for i in range(0, 32, 2)]
    
    embedding_model = MockEmbeddingModel()
    
    # 创建RAG系统
    # rag_system = RAGSystem(embedding_model, vector_store, language_model)
    
    # 添加文档
    documents = [
        {
            'content': '机器学习是一种人工智能技术，通过数据训练算法来实现自动化决策。',
            'metadata': {'topic': 'AI', 'source': 'textbook'}
        },
        {
            'content': '深度学习是机器学习的一个子领域，使用神经网络进行学习。',
            'metadata': {'topic': 'AI', 'source': 'research'}
        }
    ]
    
    # await rag_system.add_documents(documents)
    
    # 执行RAG查询
    # result = await rag_system.rag_query("什么是机器学习？")
    # print(f"RAG回答: {result['answer']}")

# 运行测试
# asyncio.run(test_fine_tuning_and_rag())
```

## 性能优化与部署

### 5.1 模型性能监控

```python
# monitoring/model_monitor.py
import time
import asyncio
import logging
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import deque
import json

@dataclass
class ModelMetrics:
    """模型性能指标"""
    model_name: str
    request_count: int = 0
    total_latency: float = 0.0
    error_count: int = 0
    total_tokens: int = 0
    cost: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)

class ModelMonitor:
    """模型性能监控器"""
    
    def __init__(self, window_size: int = 1000):
        self.metrics: Dict[str, ModelMetrics] = {}
        self.request_history: deque = deque(maxlen=window_size)
        self.logger = logging.getLogger(__name__)
    
    async def track_request(self, model_name: str, start_time: float, end_time: float, 
                          tokens: int, cost: float, error: bool = False):
        """跟踪请求性能"""
        latency = end_time - start_time
        
        # 更新模型指标
        if model_name not in self.metrics:
            self.metrics[model_name] = ModelMetrics(model_name=model_name)
        
        metrics = self.metrics[model_name]
        metrics.request_count += 1
        metrics.total_latency += latency
        metrics.total_tokens += tokens
        metrics.cost += cost
        
        if error:
            metrics.error_count += 1
        
        # 记录请求历史
        self.request_history.append({
            'model_name': model_name,
            'timestamp': datetime.now().isoformat(),
            'latency': latency,
            'tokens': tokens,
            'cost': cost,
            'error': error
        })
    
    def get_metrics(self, model_name: str) -> Dict[str, Any]:
        """获取模型指标"""
        if model_name not in self.metrics:
            return {}
        
        metrics = self.metrics[model_name]
        avg_latency = metrics.total_latency / max(metrics.request_count, 1)
        error_rate = metrics.error_count / max(metrics.request_count, 1)
        
        return {
            'model_name': model_name,
            'request_count': metrics.request_count,
            'average_latency': avg_latency,
            'error_rate': error_rate,
            'total_tokens': metrics.total_tokens,
            'total_cost': metrics.cost,
            'requests_per_minute': self._calculate_rpm(model_name)
        }
    
    def _calculate_rpm(self, model_name: str) -> float:
        """计算每分钟请求数"""
        now = datetime.now()
        one_minute_ago = now - timedelta(minutes=1)
        
        recent_requests = [
            req for req in self.request_history
            if (req['model_name'] == model_name and 
                datetime.fromisoformat(req['timestamp']) > one_minute_ago)
        ]
        
        return len(recent_requests)
    
    def get_all_metrics(self) -> Dict[str, Dict[str, Any]]:
        """获取所有模型指标"""
        return {name: self.get_metrics(name) for name in self.metrics.keys()}

# 负载均衡器
class ModelLoadBalancer:
    """模型负载均衡器"""
    
    def __init__(self, model_manager, monitor: ModelMonitor):
        self.model_manager = model_manager
        self.monitor = monitor
        self.logger = logging.getLogger(__name__)
    
    def select_model(self, models: List[str], strategy: str = 'round_robin') -> str:
        """选择模型"""
        if not models:
            raise ValueError("没有可用的模型")
        
        if strategy == 'round_robin':
            return self._round_robin_selection(models)
        elif strategy == 'least_latency':
            return self._least_latency_selection(models)
        elif strategy == 'least_load':
            return self._least_load_selection(models)
        else:
            return models[0]
    
    def _round_robin_selection(self, models: List[str]) -> str:
        """轮询选择"""
        if not hasattr(self, '_round_robin_index'):
            self._round_robin_index = 0
        
        selected = models[self._round_robin_index % len(models)]
        self._round_robin_index += 1
        return selected
    
    def _least_latency_selection(self, models: List[str]) -> str:
        """最低延迟选择"""
        best_model = models[0]
        best_latency = float('inf')
        
        for model in models:
            metrics = self.monitor.get_metrics(model)
            latency = metrics.get('average_latency', float('inf'))
            
            if latency < best_latency:
                best_latency = latency
                best_model = model
        
        return best_model
    
    def _least_load_selection(self, models: List[str]) -> str:
        """最低负载选择"""
        best_model = models[0]
        best_rpm = float('inf')
        
        for model in models:
            metrics = self.monitor.get_metrics(model)
            rpm = metrics.get('requests_per_minute', 0)
            
            if rpm < best_rpm:
                best_rpm = rpm
                best_model = model
        
        return best_model

# 缓存系统
class ModelCache:
    """模型响应缓存系统"""
    
    def __init__(self, max_size: int = 10000, ttl: int = 3600):
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.max_size = max_size
        self.ttl = ttl  # 缓存时间（秒）
        self.access_times: Dict[str, datetime] = {}
    
    def _generate_key(self, model_name: str, prompt: str, **kwargs) -> str:
        """生成缓存键"""
        import hashlib
        
        content = f"{model_name}:{prompt}:{json.dumps(kwargs, sort_keys=True)}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def get(self, model_name: str, prompt: str, **kwargs) -> Optional[str]:
        """获取缓存"""
        key = self._generate_key(model_name, prompt, **kwargs)
        
        if key not in self.cache:
            return None
        
        cache_entry = self.cache[key]
        cached_time = cache_entry['timestamp']
        
        # 检查是否过期
        if datetime.now() - cached_time > timedelta(seconds=self.ttl):
            del self.cache[key]
            del self.access_times[key]
            return None
        
        # 更新访问时间
        self.access_times[key] = datetime.now()
        return cache_entry['response']
    
    def set(self, model_name: str, prompt: str, response: str, **kwargs):
        """设置缓存"""
        key = self._generate_key(model_name, prompt, **kwargs)
        
        # 如果缓存满了，删除最久未访问的条目
        if len(self.cache) >= self.max_size:
            self._evict_lru()
        
        self.cache[key] = {
            'response': response,
            'timestamp': datetime.now()
        }
        self.access_times[key] = datetime.now()
    
    def _evict_lru(self):
        """删除最久未使用的缓存条目"""
        if not self.access_times:
            return
        
        oldest_key = min(self.access_times, key=self.access_times.get)
        del self.cache[oldest_key]
        del self.access_times[oldest_key]

# 集成的模型服务
class OptimizedModelService:
    """优化的模型服务"""
    
    def __init__(self, model_manager):
        self.model_manager = model_manager
        self.monitor = ModelMonitor()
        self.load_balancer = ModelLoadBalancer(model_manager, self.monitor)
        self.cache = ModelCache()
        self.logger = logging.getLogger(__name__)
    
    async def generate(self, prompt: str, models: List[str] = None, use_cache: bool = True, **kwargs) -> str:
        """优化的文本生成"""
        # 检查缓存
        if use_cache and models:
            cached_response = self.cache.get(models[0], prompt, **kwargs)
            if cached_response:
                self.logger.info("使用缓存响应")
                return cached_response
        
        # 选择模型
        available_models = models or self.model_manager.list_models()
        selected_model_name = self.load_balancer.select_model(available_models, 'least_latency')
        model = self.model_manager.get_model(selected_model_name)
        
        # 执行生成
        start_time = time.time()
        error = False
        response = ""
        tokens = 0
        
        try:
            response = await model.generate(prompt, **kwargs)
            tokens = len(response.split())  # 简单的token计数
            
        except Exception as e:
            self.logger.error(f"模型生成失败: {e}")
            error = True
            raise
        
        finally:
            end_time = time.time()
            cost = self._calculate_cost(selected_model_name, tokens)
            
            # 记录性能指标
            await self.monitor.track_request(
                selected_model_name, start_time, end_time, tokens, cost, error
            )
        
        # 缓存响应
        if use_cache and not error and response:
            self.cache.set(selected_model_name, prompt, response, **kwargs)
        
        return response
    
    def _calculate_cost(self, model_name: str, tokens: int) -> float:
        """计算成本"""
        try:
            model = self.model_manager.get_model(model_name)
            cost_per_token = getattr(model.config, 'cost_per_token', 0.0)
            return tokens * cost_per_token
        except:
            return 0.0
    
    def get_performance_report(self) -> Dict[str, Any]:
        """获取性能报告"""
        return {
            'timestamp': datetime.now().isoformat(),
            'metrics': self.monitor.get_all_metrics(),
            'cache_stats': {
                'size': len(self.cache.cache),
                'max_size': self.cache.max_size,
                'hit_rate': self._calculate_cache_hit_rate()
            }
        }
    
    def _calculate_cache_hit_rate(self) -> float:
        """计算缓存命中率"""
        # 这里需要实现缓存命中率统计
        return 0.0  # 简化实现

# 部署配置
@dataclass
class DeploymentConfig:
    """部署配置"""
    service_name: str
    port: int = 8000
    workers: int = 4
    max_requests_per_worker: int = 1000
    timeout: int = 300
    log_level: str = "INFO"
    cors_origins: List[str] = field(default_factory=list)
    rate_limit: Optional[Dict[str, int]] = None
    
# Web API服务
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

class GenerateRequest(BaseModel):
    prompt: str
    models: Optional[List[str]] = None
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None
    use_cache: bool = True

class GenerateResponse(BaseModel):
    response: str
    model_used: str
    tokens: int
    latency: float
    cached: bool

def create_api_app(model_service: OptimizedModelService, config: DeploymentConfig) -> FastAPI:
    """创建API应用"""
    app = FastAPI(
        title="AI Model Integration Service",
        description="高性能AI模型集成服务",
        version="1.0.0"
    )
    
    # 配置CORS
    if config.cors_origins:
        app.add_middleware(
            CORSMiddleware,
            allow_origins=config.cors_origins,
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
    
    @app.post("/generate", response_model=GenerateResponse)
    async def generate_text(request: GenerateRequest):
        """生成文本API"""
        start_time = time.time()
        
        try:
            # 检查缓存
            cached = False
            if request.use_cache and request.models:
                cached_response = model_service.cache.get(
                    request.models[0], 
                    request.prompt,
                    max_tokens=request.max_tokens,
                    temperature=request.temperature
                )
                if cached_response:
                    return GenerateResponse(
                        response=cached_response,
                        model_used=request.models[0],
                        tokens=len(cached_response.split()),
                        latency=time.time() - start_time,
                        cached=True
                    )
            
            # 生成文本
            response = await model_service.generate(
                request.prompt,
                models=request.models,
                use_cache=request.use_cache,
                max_tokens=request.max_tokens,
                temperature=request.temperature
            )
            
            return GenerateResponse(
                response=response,
                model_used="auto-selected",  # 实际应该记录选择的模型
                tokens=len(response.split()),
                latency=time.time() - start_time,
                cached=False
            )
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
    
    @app.get("/metrics")
    async def get_metrics():
        """获取性能指标"""
        return model_service.get_performance_report()
    
    @app.get("/health")
    async def health_check():
        """健康检查"""
        try:
            models_health = await model_service.model_manager.health_check()
            return {
                "status": "healthy",
                "models": models_health,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            raise HTTPException(status_code=503, detail=f"Service unhealthy: {str(e)}")
    
    return app

# 部署脚本
def deploy_service(model_manager, config: DeploymentConfig):
    """部署服务"""
    # 创建优化的模型服务
    model_service = OptimizedModelService(model_manager)
    
    # 创建API应用
    app = create_api_app(model_service, config)
    
    # 启动服务
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=config.port,
        workers=config.workers,
        log_level=config.log_level.lower(),
        timeout_keep_alive=config.timeout
    )

# 使用示例
async def demo_optimized_service():
    """演示优化的服务"""
    # 创建模型管理器
    model_manager = ModelManager()
    
    # 注册模型
    claude_config = create_claude_config()
    claude_model = ModelFactory.create_model(claude_config)
    model_manager.register_model(claude_model)
    
    # 创建优化的服务
    service = OptimizedModelService(model_manager)
    
    # 测试生成
    response = await service.generate(
        "解释什么是人工智能",
        models=["claude-3-sonnet-20240229"],
        use_cache=True
    )
    print(f"生成响应: {response}")
    
    # 获取性能报告
    report = service.get_performance_report()
    print(f"性能报告: {json.dumps(report, indent=2, ensure_ascii=False)}")

# 运行演示
# asyncio.run(demo_optimized_service())
```

## 总结

本文深入探讨了AI模型集成与定制的完整流程，从基础的模型管理到高级的微调技术，再到生产环境的部署优化。通过Claude Code的辅助，我们能够：

1. **统一模型管理**：建立标准化的模型接口，支持多种AI提供商
2. **智能路由选择**：根据请求特性自动选择最合适的模型
3. **性能优化**：通过缓存、负载均衡等技术提升系统性能
4. **监控运维**：实时监控模型性能，确保服务稳定性
5. **微调定制**：根据业务需求对模型进行定制化优化

在实际应用中，建议：

- 根据业务场景选择合适的模型组合
- 重视数据质量，是微调成功的关键
- 建立完善的监控体系，及时发现问题
- 考虑成本效益，合理分配模型资源
- 保持技术更新，跟上AI发展趋势

Claude Code作为强大的AI编程助手，不仅能帮助我们实现复杂的模型集成，更能在开发过程中提供智能建议，大大提升开发效率。掌握这些技术，将为构建下一代智能应用奠定坚实基础。