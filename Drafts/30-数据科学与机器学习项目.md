# 数据科学与机器学习项目：用Claude Code加速AI模型开发

> 数据科学和机器学习项目具有探索性强、迭代频繁的特点，Claude Code在数据处理、模型开发、实验管理等环节都能提供强大的AI辅助能力。本文将通过完整的机器学习项目案例，展示如何使用Claude Code提升数据科学工作效率。

## 📋 本文目录

- [数据科学项目概述](#数据科学项目概述)
- [项目环境搭建](#项目环境搭建)
- [数据获取与探索](#数据获取与探索)
- [数据预处理与特征工程](#数据预处理与特征工程)
- [模型开发与训练](#模型开发与训练)
- [模型评估与调优](#模型评估与调优)
- [实验管理与追踪](#实验管理与追踪)
- [模型部署与服务化](#模型部署与服务化)
- [MLOps最佳实践](#mlops最佳实践)
- [Jupyter集成深度应用](#jupyter集成深度应用)

## 数据科学项目概述

### Claude Code在数据科学中的优势

Claude Code为数据科学项目提供了全方位的支持，特别是在以下方面表现突出：

- **Jupyter Notebook原生支持**：直接读取、编辑和执行Jupyter notebook
- **智能代码生成**：根据数据特征自动生成EDA和建模代码
- **实验管理**：系统化的实验追踪和结果对比
- **代码重构**：将探索性代码转换为生产级代码

### 项目案例：城市声音分类系统

我们将通过一个完整的音频机器学习项目"Urban Sounds Classification"来演示Claude Code在数据科学项目中的应用。这个项目的目标是构建一个能够识别城市环境中不同声音的分类模型。

```markdown
# Urban Sounds Classification Project

## 项目目标
开发一个城市声音分类系统，能够识别10种常见的城市声音：
- 空调声、汽车喇叭、儿童玩耍、狗吠声
- 钻孔声、引擎怠速、枪声、手提钻
- 警报器、街头音乐

## 技术栈
- 数据处理：pandas, numpy, librosa
- 机器学习：scikit-learn, xgboost
- 深度学习：tensorflow, keras
- 可视化：matplotlib, seaborn, plotly
- 实验管理：mlflow, wandb
- 部署：fastapi, docker
```

## 项目环境搭建

### 使用Claude Code创建项目结构

首先创建项目的CLAUDE.md文件来定义项目上下文：

```markdown
# 城市声音分类项目

## 项目信息
- 项目类型：音频机器学习分类项目
- 主要语言：Python
- 数据类型：音频文件(.wav)
- 模型类型：深度学习分类模型

## 技术栈
- 数据科学：pandas, numpy, scipy
- 音频处理：librosa, soundfile
- 机器学习：scikit-learn, xgboost
- 深度学习：tensorflow, keras, pytorch
- 可视化：matplotlib, seaborn, plotly
- 实验管理：mlflow, wandb
- 开发工具：jupyter, ipython

## 项目结构
```
urban-sounds-classification/
├── data/
│   ├── raw/              # 原始音频文件
│   ├── processed/        # 预处理后的数据
│   └── features/         # 提取的特征
├── notebooks/
│   ├── 01-data-exploration.ipynb
│   ├── 02-feature-engineering.ipynb
│   ├── 03-model-development.ipynb
│   └── 04-model-evaluation.ipynb
├── src/
│   ├── data/             # 数据处理模块
│   ├── features/         # 特征工程模块
│   ├── models/           # 模型定义模块
│   └── utils/            # 工具函数
├── models/               # 训练好的模型
├── experiments/          # 实验记录
├── deployment/           # 部署相关文件
└── requirements.txt
```

## 开发规范
- 使用Jupyter Notebook进行探索性分析
- 核心功能模块化到src目录
- 所有实验使用MLflow进行追踪
- 代码遵循PEP8规范
- 模型性能指标：准确率>85%
```

让Claude Code为我们生成项目的初始结构：

```bash
# 让Claude Code创建项目结构
请帮我创建一个完整的城市声音分类项目，包括：
1. 目录结构创建
2. 环境配置文件
3. 基础的数据处理模块
4. Jupyter notebook模板
```

### 环境配置文件

```python
# requirements.txt
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.0
tensorflow==2.13.0
librosa==0.10.1
matplotlib==3.7.2
seaborn==0.12.2
plotly==5.15.0
jupyter==1.0.0
mlflow==2.5.0
wandb==0.15.8
fastapi==0.101.1
uvicorn==0.23.2
soundfile==0.12.1
xgboost==1.7.6

# development dependencies
pytest==7.4.0
black==23.7.0
flake8==6.0.0
jupyter-notebook==6.5.4
```

```python
# setup.py
from setuptools import setup, find_packages

setup(
    name="urban-sounds-classification",
    version="1.0.0",
    description="Urban sounds classification using machine learning",
    author="Your Name",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    python_requires=">=3.8",
    install_requires=[
        "numpy>=1.24.0",
        "pandas>=2.0.0",
        "scikit-learn>=1.3.0",
        "tensorflow>=2.13.0",
        "librosa>=0.10.0",
        "matplotlib>=3.7.0",
        "seaborn>=0.12.0",
        "mlflow>=2.5.0"
    ],
    extras_require={
        "dev": [
            "pytest>=7.4.0",
            "black>=23.7.0",
            "flake8>=6.0.0",
            "jupyter>=1.0.0"
        ]
    }
)
```

### Docker环境配置

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    libsndfile1 \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# 复制依赖文件
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 复制项目文件
COPY . .

# 安装项目
RUN pip install -e .

# 暴露端口
EXPOSE 8888 8000

# 启动命令
CMD ["jupyter", "notebook", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root"]
```

## 数据获取与探索

### 数据加载模块

Claude Code帮助我们创建专业的数据处理模块：

```python
# src/data/data_loader.py
import os
import pandas as pd
import librosa
import numpy as np
from pathlib import Path
from typing import List, Tuple, Dict
import warnings
warnings.filterwarnings('ignore')

class UrbanSoundDataLoader:
    """城市声音数据加载器"""
    
    def __init__(self, data_path: str, metadata_path: str):
        self.data_path = Path(data_path)
        self.metadata_path = Path(metadata_path)
        self.metadata = self._load_metadata()
        self.class_names = self._get_class_names()
        
    def _load_metadata(self) -> pd.DataFrame:
        """加载元数据文件"""
        if self.metadata_path.exists():
            return pd.read_csv(self.metadata_path)
        else:
            raise FileNotFoundError(f"Metadata file not found: {self.metadata_path}")
    
    def _get_class_names(self) -> List[str]:
        """获取类别名称"""
        return sorted(self.metadata['class'].unique())
    
    def load_audio_file(self, filename: str, sr: int = 22050) -> Tuple[np.ndarray, int]:
        """
        加载单个音频文件
        
        Args:
            filename: 音频文件名
            sr: 采样率
            
        Returns:
            音频数据和采样率的元组
        """
        filepath = self.data_path / filename
        try:
            audio_data, sample_rate = librosa.load(filepath, sr=sr)
            return audio_data, sample_rate
        except Exception as e:
            print(f"Error loading {filename}: {e}")
            return np.array([]), sr
    
    def load_batch_audio(self, batch_size: int = 100) -> Dict:
        """
        批量加载音频数据
        
        Args:
            batch_size: 批次大小
            
        Returns:
            包含音频数据的字典
        """
        batch_data = {
            'audio': [],
            'labels': [],
            'filenames': [],
            'class_names': []
        }
        
        for idx, row in self.metadata.head(batch_size).iterrows():
            audio, _ = self.load_audio_file(row['slice_file_name'])
            
            if len(audio) > 0:
                batch_data['audio'].append(audio)
                batch_data['labels'].append(row['classID'])
                batch_data['filenames'].append(row['slice_file_name'])
                batch_data['class_names'].append(row['class'])
        
        return batch_data
    
    def get_class_distribution(self) -> pd.Series:
        """获取类别分布"""
        return self.metadata['class'].value_counts()
    
    def get_data_summary(self) -> Dict:
        """获取数据集摘要信息"""
        return {
            'total_files': len(self.metadata),
            'num_classes': len(self.class_names),
            'class_names': self.class_names,
            'class_distribution': self.get_class_distribution().to_dict(),
            'file_size_range': self._get_file_size_stats()
        }
    
    def _get_file_size_stats(self) -> Dict:
        """获取文件大小统计"""
        file_sizes = []
        for filename in self.metadata['slice_file_name'].head(100):  # 采样检查
            filepath = self.data_path / filename
            if filepath.exists():
                file_sizes.append(filepath.stat().st_size)
        
        if file_sizes:
            return {
                'min_size': min(file_sizes),
                'max_size': max(file_sizes),
                'avg_size': np.mean(file_sizes)
            }
        return {}
```

### 探索性数据分析（EDA）

```python
# notebooks/01-data-exploration.ipynb 的主要内容

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import librosa
import librosa.display
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

# 设置样式
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# 1. 数据基本信息探索
def explore_basic_info(data_loader):
    """探索数据基本信息"""
    summary = data_loader.get_data_summary()
    
    print("=== 数据集基本信息 ===")
    print(f"总文件数: {summary['total_files']}")
    print(f"类别数: {summary['num_classes']}")
    print(f"类别名称: {summary['class_names']}")
    
    # 类别分布可视化
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    class_dist = pd.Series(summary['class_distribution'])
    class_dist.plot(kind='bar')
    plt.title('类别分布')
    plt.xlabel('类别')
    plt.ylabel('样本数量')
    plt.xticks(rotation=45)
    
    plt.subplot(1, 2, 2)
    plt.pie(class_dist.values, labels=class_dist.index, autopct='%1.1f%%')
    plt.title('类别分布饼图')
    
    plt.tight_layout()
    plt.show()
    
    return summary

# 2. 音频特征可视化
def visualize_audio_samples(data_loader, num_samples=3):
    """可视化音频样本"""
    batch_data = data_loader.load_batch_audio(batch_size=50)
    
    # 为每个类别选择一个样本
    classes = list(set(batch_data['class_names']))
    
    fig, axes = plt.subplots(len(classes), 3, figsize=(15, 4*len(classes)))
    
    for i, class_name in enumerate(classes):
        # 找到该类别的第一个样本
        class_indices = [j for j, c in enumerate(batch_data['class_names']) if c == class_name]
        if not class_indices:
            continue
            
        audio = batch_data['audio'][class_indices[0]]
        
        # 时域波形
        axes[i, 0].plot(audio)
        axes[i, 0].set_title(f'{class_name} - 时域波形')
        axes[i, 0].set_xlabel('样本点')
        axes[i, 0].set_ylabel('幅度')
        
        # 频谱图
        D = librosa.stft(audio)
        DB = librosa.amplitude_to_db(np.abs(D), ref=np.max)
        img = librosa.display.specshow(DB, x_axis='time', y_axis='hz', ax=axes[i, 1])
        axes[i, 1].set_title(f'{class_name} - 频谱图')
        
        # MFCC特征
        mfccs = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=13)
        img = librosa.display.specshow(mfccs, x_axis='time', ax=axes[i, 2])
        axes[i, 2].set_title(f'{class_name} - MFCC特征')
    
    plt.tight_layout()
    plt.show()

# 3. 音频长度分析
def analyze_audio_duration(data_loader, sample_size=200):
    """分析音频长度分布"""
    batch_data = data_loader.load_batch_audio(batch_size=sample_size)
    
    durations = []
    for audio in batch_data['audio']:
        duration = len(audio) / 22050  # 转换为秒
        durations.append(duration)
    
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.hist(durations, bins=30, alpha=0.7, edgecolor='black')
    plt.title('音频长度分布')
    plt.xlabel('时长 (秒)')
    plt.ylabel('频次')
    
    plt.subplot(1, 2, 2)
    plt.boxplot(durations)
    plt.title('音频长度箱线图')
    plt.ylabel('时长 (秒)')
    
    plt.tight_layout()
    plt.show()
    
    print(f"平均时长: {np.mean(durations):.2f}秒")
    print(f"最短时长: {np.min(durations):.2f}秒")
    print(f"最长时长: {np.max(durations):.2f}秒")
    print(f"标准差: {np.std(durations):.2f}秒")

# 4. 特征相关性分析
def analyze_feature_correlation(data_loader, sample_size=100):
    """分析音频特征相关性"""
    from src.features.feature_extractor import AudioFeatureExtractor
    
    extractor = AudioFeatureExtractor()
    batch_data = data_loader.load_batch_audio(batch_size=sample_size)
    
    features_list = []
    labels_list = []
    
    for i, audio in enumerate(batch_data['audio']):
        features = extractor.extract_features(audio)
        features_list.append(features)
        labels_list.append(batch_data['labels'][i])
    
    # 创建特征DataFrame
    feature_df = pd.DataFrame(features_list)
    feature_df['label'] = labels_list
    
    # 计算相关性矩阵
    correlation_matrix = feature_df.corr()
    
    # 可视化相关性热图
    plt.figure(figsize=(12, 10))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                square=True, linewidths=0.5, cbar_kws={"shrink": .5})
    plt.title('特征相关性热图')
    plt.tight_layout()
    plt.show()
    
    return feature_df, correlation_matrix
```

## 数据预处理与特征工程

### 音频特征提取器

```python
# src/features/feature_extractor.py
import librosa
import numpy as np
from typing import Dict, List
import warnings
warnings.filterwarnings('ignore')

class AudioFeatureExtractor:
    """音频特征提取器"""
    
    def __init__(self, sr: int = 22050, n_mfcc: int = 13):
        self.sr = sr
        self.n_mfcc = n_mfcc
    
    def extract_features(self, audio: np.ndarray) -> Dict[str, float]:
        """
        提取音频特征
        
        Args:
            audio: 音频数据数组
            
        Returns:
            特征字典
        """
        features = {}
        
        # 1. 基础统计特征
        features.update(self._extract_basic_features(audio))
        
        # 2. 频谱特征
        features.update(self._extract_spectral_features(audio))
        
        # 3. MFCC特征
        features.update(self._extract_mfcc_features(audio))
        
        # 4. 节奏特征
        features.update(self._extract_rhythm_features(audio))
        
        # 5. 谐波和经典特征
        features.update(self._extract_harmonic_features(audio))
        
        return features
    
    def _extract_basic_features(self, audio: np.ndarray) -> Dict[str, float]:
        """提取基础统计特征"""
        return {
            'zero_crossing_rate': float(np.mean(librosa.feature.zero_crossing_rate(audio))),
            'energy': float(np.sum(audio ** 2)),
            'rmse': float(np.mean(librosa.feature.rms(y=audio))),
            'audio_length': len(audio),
            'max_amplitude': float(np.max(np.abs(audio))),
            'mean_amplitude': float(np.mean(np.abs(audio))),
            'std_amplitude': float(np.std(audio))
        }
    
    def _extract_spectral_features(self, audio: np.ndarray) -> Dict[str, float]:
        """提取频谱特征"""
        # 计算频谱质心
        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=self.sr)
        
        # 计算频谱带宽
        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=self.sr)
        
        # 计算频谱对比度
        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=self.sr)
        
        # 计算频谱滚降
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=self.sr)
        
        return {
            'spectral_centroid': float(np.mean(spectral_centroids)),
            'spectral_bandwidth': float(np.mean(spectral_bandwidth)),
            'spectral_contrast_mean': float(np.mean(spectral_contrast)),
            'spectral_rolloff': float(np.mean(spectral_rolloff))
        }
    
    def _extract_mfcc_features(self, audio: np.ndarray) -> Dict[str, float]:
        """提取MFCC特征"""
        mfccs = librosa.feature.mfcc(y=audio, sr=self.sr, n_mfcc=self.n_mfcc)
        
        features = {}
        for i in range(self.n_mfcc):
            features[f'mfcc_{i+1}'] = float(np.mean(mfccs[i]))
            features[f'mfcc_{i+1}_std'] = float(np.std(mfccs[i]))
        
        return features
    
    def _extract_rhythm_features(self, audio: np.ndarray) -> Dict[str, float]:
        """提取节奏特征"""
        try:
            # 计算节拍
            tempo, beats = librosa.beat.beat_track(y=audio, sr=self.sr)
            
            return {
                'tempo': float(tempo),
                'beat_count': len(beats)
            }
        except:
            return {
                'tempo': 0.0,
                'beat_count': 0
            }
    
    def _extract_harmonic_features(self, audio: np.ndarray) -> Dict[str, float]:
        """提取谐波特征"""
        try:
            # 分离谐波和打击乐成分
            harmonic, percussive = librosa.effects.hpss(audio)
            
            return {
                'harmonic_energy': float(np.sum(harmonic ** 2)),
                'percussive_energy': float(np.sum(percussive ** 2)),
                'harmonic_percussive_ratio': float(np.sum(harmonic ** 2) / (np.sum(percussive ** 2) + 1e-10))
            }
        except:
            return {
                'harmonic_energy': 0.0,
                'percussive_energy': 0.0,
                'harmonic_percussive_ratio': 0.0
            }
    
    def extract_batch_features(self, audio_list: List[np.ndarray]) -> List[Dict[str, float]]:
        """批量提取特征"""
        return [self.extract_features(audio) for audio in audio_list]

# 数据预处理管道
class AudioPreprocessor:
    """音频数据预处理器"""
    
    def __init__(self, target_length: int = 4 * 22050):  # 4秒长度
        self.target_length = target_length
    
    def normalize_audio(self, audio: np.ndarray) -> np.ndarray:
        """音频归一化"""
        if np.max(np.abs(audio)) > 0:
            return audio / np.max(np.abs(audio))
        return audio
    
    def pad_or_truncate(self, audio: np.ndarray) -> np.ndarray:
        """填充或截断音频到固定长度"""
        if len(audio) > self.target_length:
            # 截断：取中间部分
            start = (len(audio) - self.target_length) // 2
            return audio[start:start + self.target_length]
        elif len(audio) < self.target_length:
            # 填充：使用零填充
            pad_length = self.target_length - len(audio)
            return np.pad(audio, (0, pad_length), mode='constant')
        return audio
    
    def remove_silence(self, audio: np.ndarray, top_db: int = 20) -> np.ndarray:
        """移除静音部分"""
        return librosa.effects.trim(audio, top_db=top_db)[0]
    
    def preprocess(self, audio: np.ndarray) -> np.ndarray:
        """完整的预处理流程"""
        # 1. 移除静音
        audio = self.remove_silence(audio)
        
        # 2. 归一化
        audio = self.normalize_audio(audio)
        
        # 3. 长度标准化
        audio = self.pad_or_truncate(audio)
        
        return audio
```

### 特征工程流水线

```python
# src/features/feature_pipeline.py
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from typing import Tuple, List
import joblib

class FeaturePipeline:
    """特征工程流水线"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.feature_selector = SelectKBest(score_func=f_classif, k=50)
        self.selected_features = None
        
    def prepare_features(self, data_loader, sample_size: int = None) -> Tuple[pd.DataFrame, np.ndarray]:
        """准备特征数据"""
        from src.features.feature_extractor import AudioFeatureExtractor, AudioPreprocessor
        
        # 加载数据
        if sample_size:
            batch_data = data_loader.load_batch_audio(batch_size=sample_size)
        else:
            batch_data = data_loader.load_batch_audio(batch_size=len(data_loader.metadata))
        
        # 初始化处理器
        preprocessor = AudioPreprocessor()
        extractor = AudioFeatureExtractor()
        
        # 预处理和特征提取
        features_list = []
        labels_list = []
        
        print("正在提取特征...")
        for i, audio in enumerate(batch_data['audio']):
            if i % 100 == 0:
                print(f"处理进度: {i}/{len(batch_data['audio'])}")
            
            # 预处理音频
            processed_audio = preprocessor.preprocess(audio)
            
            # 提取特征
            features = extractor.extract_features(processed_audio)
            features_list.append(features)
            labels_list.append(batch_data['labels'][i])
        
        # 创建特征DataFrame
        feature_df = pd.DataFrame(features_list)
        labels = np.array(labels_list)
        
        print(f"特征提取完成，共提取 {len(feature_df.columns)} 个特征")
        return feature_df, labels
    
    def fit_transform(self, X: pd.DataFrame, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """拟合并转换特征"""
        # 1. 处理缺失值
        X_filled = X.fillna(X.mean())
        
        # 2. 特征选择
        X_selected = self.feature_selector.fit_transform(X_filled, y)
        self.selected_features = X_filled.columns[self.feature_selector.get_support()]
        
        # 3. 标准化特征
        X_scaled = self.scaler.fit_transform(X_selected)
        
        # 4. 编码标签
        y_encoded = self.label_encoder.fit_transform(y)
        
        print(f"选择了 {len(self.selected_features)} 个最重要的特征")
        print(f"特征名称: {list(self.selected_features)}")
        
        return X_scaled, y_encoded
    
    def transform(self, X: pd.DataFrame) -> np.ndarray:
        """转换新数据"""
        # 处理缺失值
        X_filled = X.fillna(X.mean())
        
        # 选择特征
        X_selected = X_filled[self.selected_features]
        
        # 标准化
        X_scaled = self.scaler.transform(X_selected)
        
        return X_scaled
    
    def save_pipeline(self, filepath: str):
        """保存特征处理流水线"""
        pipeline_data = {
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'feature_selector': self.feature_selector,
            'selected_features': self.selected_features
        }
        joblib.dump(pipeline_data, filepath)
        print(f"特征流水线已保存到: {filepath}")
    
    def load_pipeline(self, filepath: str):
        """加载特征处理流水线"""
        pipeline_data = joblib.load(filepath)
        self.scaler = pipeline_data['scaler']
        self.label_encoder = pipeline_data['label_encoder']
        self.feature_selector = pipeline_data['feature_selector']
        self.selected_features = pipeline_data['selected_features']
        print(f"特征流水线已从 {filepath} 加载")
```

## 模型开发与训练

### 模型定义模块

```python
# src/models/classifiers.py
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import xgboost as xgb
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import mlflow
import mlflow.sklearn
import mlflow.tensorflow

class ModelFactory:
    """模型工厂类"""
    
    @staticmethod
    def create_random_forest(n_estimators=100, max_depth=None, random_state=42):
        """创建随机森林模型"""
        return RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=random_state,
            n_jobs=-1
        )
    
    @staticmethod
    def create_xgboost(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42):
        """创建XGBoost模型"""
        return xgb.XGBClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            random_state=random_state,
            n_jobs=-1
        )
    
    @staticmethod
    def create_svm(kernel='rbf', C=1.0, gamma='scale', random_state=42):
        """创建SVM模型"""
        return SVC(
            kernel=kernel,
            C=C,
            gamma=gamma,
            random_state=random_state,
            probability=True
        )
    
    @staticmethod
    def create_logistic_regression(C=1.0, max_iter=1000, random_state=42):
        """创建逻辑回归模型"""
        return LogisticRegression(
            C=C,
            max_iter=max_iter,
            random_state=random_state,
            n_jobs=-1
        )
    
    @staticmethod
    def create_neural_network(input_dim, num_classes, hidden_layers=[128, 64], dropout_rate=0.3):
        """创建神经网络模型"""
        model = Sequential()
        
        # 输入层
        model.add(Dense(hidden_layers[0], input_dim=input_dim, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))
        
        # 隐藏层
        for units in hidden_layers[1:]:
            model.add(Dense(units, activation='relu'))
            model.add(BatchNormalization())
            model.add(Dropout(dropout_rate))
        
        # 输出层
        model.add(Dense(num_classes, activation='softmax'))
        
        # 编译模型
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model

class ModelTrainer:
    """模型训练器"""
    
    def __init__(self, experiment_name="urban_sounds_classification"):
        self.experiment_name = experiment_name
        mlflow.set_experiment(experiment_name)
    
    def train_sklearn_model(self, model, X_train, y_train, X_val, y_val, model_name):
        """训练sklearn模型"""
        with mlflow.start_run(run_name=model_name):
            # 记录参数
            if hasattr(model, 'get_params'):
                mlflow.log_params(model.get_params())
            
            # 训练模型
            print(f"正在训练 {model_name}...")
            model.fit(X_train, y_train)
            
            # 预测
            train_pred = model.predict(X_train)
            val_pred = model.predict(X_val)
            
            # 计算指标
            train_acc = accuracy_score(y_train, train_pred)
            val_acc = accuracy_score(y_val, val_pred)
            
            # 记录指标
            mlflow.log_metrics({
                'train_accuracy': train_acc,
                'val_accuracy': val_acc
            })
            
            # 记录模型
            mlflow.sklearn.log_model(model, model_name)
            
            print(f"{model_name} - 训练准确率: {train_acc:.4f}, 验证准确率: {val_acc:.4f}")
            
            return {
                'model': model,
                'train_accuracy': train_acc,
                'val_accuracy': val_acc,
                'val_predictions': val_pred
            }
    
    def train_neural_network(self, model, X_train, y_train, X_val, y_val, 
                           epochs=100, batch_size=32, model_name="neural_network"):
        """训练神经网络模型"""
        with mlflow.start_run(run_name=model_name):
            # 记录参数
            mlflow.log_params({
                'epochs': epochs,
                'batch_size': batch_size,
                'optimizer': 'Adam',
                'loss': 'sparse_categorical_crossentropy'
            })
            
            # 回调函数
            callbacks = [
                EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),
                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
            ]
            
            # 训练模型
            print(f"正在训练 {model_name}...")
            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=batch_size,
                callbacks=callbacks,
                verbose=1
            )
            
            # 评估模型
            train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)
            val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
            
            # 记录指标
            mlflow.log_metrics({
                'train_accuracy': train_acc,
                'train_loss': train_loss,
                'val_accuracy': val_acc,
                'val_loss': val_loss
            })
            
            # 记录模型
            mlflow.tensorflow.log_model(model, model_name)
            
            print(f"{model_name} - 训练准确率: {train_acc:.4f}, 验证准确率: {val_acc:.4f}")
            
            return {
                'model': model,
                'history': history,
                'train_accuracy': train_acc,
                'val_accuracy': val_acc
            }
    
    def compare_models(self, results):
        """比较模型性能"""
        comparison_df = pd.DataFrame([
            {
                'model': name,
                'train_accuracy': result['train_accuracy'],
                'val_accuracy': result['val_accuracy'],
                'overfitting': result['train_accuracy'] - result['val_accuracy']
            }
            for name, result in results.items()
        ])
        
        comparison_df = comparison_df.sort_values('val_accuracy', ascending=False)
        print("\n=== 模型性能比较 ===")
        print(comparison_df.to_string(index=False))
        
        return comparison_df
```

### 模型训练主流程

```python
# notebooks/03-model-development.ipynb 的核心内容

# 1. 数据准备
from src.data.data_loader import UrbanSoundDataLoader
from src.features.feature_pipeline import FeaturePipeline
from src.models.classifiers import ModelFactory, ModelTrainer
from sklearn.model_selection import train_test_split
import numpy as np

# 加载数据
data_loader = UrbanSoundDataLoader('data/raw', 'data/UrbanSound8K.csv')

# 特征工程
pipeline = FeaturePipeline()
features_df, labels = pipeline.prepare_features(data_loader, sample_size=1000)

# 数据分割
X_processed, y_processed = pipeline.fit_transform(features_df, labels)
X_train, X_test, y_train, y_test = train_test_split(
    X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train
)

print(f"训练集: {X_train.shape}")
print(f"验证集: {X_val.shape}")  
print(f"测试集: {X_test.shape}")

# 2. 模型训练
trainer = ModelTrainer()
results = {}

# 训练多个模型
models_config = {
    'RandomForest': ModelFactory.create_random_forest(n_estimators=200, max_depth=15),
    'XGBoost': ModelFactory.create_xgboost(n_estimators=200, max_depth=8, learning_rate=0.1),
    'SVM': ModelFactory.create_svm(kernel='rbf', C=10, gamma='scale'),
    'LogisticRegression': ModelFactory.create_logistic_regression(C=1.0)
}

# 训练传统机器学习模型
for name, model in models_config.items():
    result = trainer.train_sklearn_model(model, X_train, y_train, X_val, y_val, name)
    results[name] = result

# 训练神经网络
nn_model = ModelFactory.create_neural_network(
    input_dim=X_train.shape[1], 
    num_classes=len(np.unique(y_train)),
    hidden_layers=[256, 128, 64],
    dropout_rate=0.3
)

nn_result = trainer.train_neural_network(
    nn_model, X_train, y_train, X_val, y_val, 
    epochs=100, batch_size=32, model_name="NeuralNetwork"
)
results['NeuralNetwork'] = nn_result

# 3. 模型比较
comparison_df = trainer.compare_models(results)
```

## 模型评估与调优

### 模型评估工具

```python
# src/models/evaluation.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_recall_fscore_support, roc_auc_score, roc_curve
)
from sklearn.model_selection import cross_val_score, GridSearchCV
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

class ModelEvaluator:
    """模型评估器"""
    
    def __init__(self, class_names=None):
        self.class_names = class_names
    
    def evaluate_model(self, model, X_test, y_test, model_name="Model"):
        """全面评估模型"""
        print(f"\n=== {model_name} 评估结果 ===")
        
        # 预测
        y_pred = model.predict(X_test)
        if hasattr(model, 'predict_proba'):
            y_proba = model.predict_proba(X_test)
        else:
            y_proba = None
        
        # 基础指标
        accuracy = accuracy_score(y_test, y_pred)
        precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average='weighted')
        
        print(f"准确率: {accuracy:.4f}")
        print(f"精确率: {precision:.4f}")
        print(f"召回率: {recall:.4f}")
        print(f"F1分数: {f1:.4f}")
        
        # 详细分类报告
        print("\n分类报告:")
        print(classification_report(y_test, y_pred, target_names=self.class_names))
        
        # 混淆矩阵可视化
        self.plot_confusion_matrix(y_test, y_pred, model_name)
        
        # ROC曲线（多分类）
        if y_proba is not None:
            self.plot_multiclass_roc(y_test, y_proba, model_name)
        
        # 特征重要性（如果模型支持）
        if hasattr(model, 'feature_importances_'):
            self.plot_feature_importance(model, model_name)
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'predictions': y_pred,
            'probabilities': y_proba
        }
    
    def plot_confusion_matrix(self, y_true, y_pred, model_name):
        """绘制混淆矩阵"""
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=self.class_names,
                    yticklabels=self.class_names)
        plt.title(f'{model_name} - 混淆矩阵')
        plt.xlabel('预测标签')
        plt.ylabel('真实标签')
        plt.tight_layout()
        plt.show()
    
    def plot_multiclass_roc(self, y_true, y_proba, model_name):
        """绘制多分类ROC曲线"""
        from sklearn.preprocessing import label_binarize
        from sklearn.metrics import roc_curve, auc
        from itertools import cycle
        
        # 二值化标签
        y_true_bin = label_binarize(y_true, classes=range(len(self.class_names)))
        n_classes = y_true_bin.shape[1]
        
        # 计算每个类别的ROC曲线
        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        
        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_proba[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
        
        # 绘制ROC曲线
        plt.figure(figsize=(12, 8))
        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green',
                       'yellow', 'purple', 'pink', 'brown', 'gray'])
        
        for i, color in zip(range(n_classes), colors):
            plt.plot(fpr[i], tpr[i], color=color, lw=2,
                    label=f'{self.class_names[i]} (AUC = {roc_auc[i]:.2f})')
        
        plt.plot([0, 1], [0, 1], 'k--', lw=2)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('假正率')
        plt.ylabel('真正率')
        plt.title(f'{model_name} - ROC曲线')
        plt.legend(loc="lower right")
        plt.tight_layout()
        plt.show()
    
    def plot_feature_importance(self, model, model_name, top_n=20):
        """绘制特征重要性"""
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            indices = np.argsort(importances)[::-1]
            
            plt.figure(figsize=(12, 8))
            plt.title(f'{model_name} - 特征重要性 (Top {top_n})')
            plt.bar(range(min(top_n, len(importances))), 
                    importances[indices[:top_n]])
            plt.xticks(range(min(top_n, len(importances))), 
                      [f'Feature {i}' for i in indices[:top_n]], rotation=45)
            plt.tight_layout()
            plt.show()
    
    def cross_validate_model(self, model, X, y, cv=5):
        """交叉验证评估"""
        scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
        
        print(f"交叉验证结果:")
        print(f"平均准确率: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")
        print(f"各折结果: {scores}")
        
        return scores

class HyperparameterTuner:
    """超参数调优器"""
    
    def __init__(self):
        self.best_params = {}
        self.best_scores = {}
    
    def tune_random_forest(self, X_train, y_train, cv=5):
        """调优随机森林"""
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 15, 20, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        
        model = ModelFactory.create_random_forest()
        grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)
        
        print("正在调优随机森林参数...")
        grid_search.fit(X_train, y_train)
        
        self.best_params['RandomForest'] = grid_search.best_params_
        self.best_scores['RandomForest'] = grid_search.best_score_
        
        print(f"最佳参数: {grid_search.best_params_}")
        print(f"最佳交叉验证分数: {grid_search.best_score_:.4f}")
        
        return grid_search.best_estimator_
    
    def tune_xgboost(self, X_train, y_train, cv=5):
        """调优XGBoost"""
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [3, 6, 9],
            'learning_rate': [0.01, 0.1, 0.2],
            'subsample': [0.8, 0.9, 1.0]
        }
        
        model = ModelFactory.create_xgboost()
        grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)
        
        print("正在调优XGBoost参数...")
        grid_search.fit(X_train, y_train)
        
        self.best_params['XGBoost'] = grid_search.best_params_
        self.best_scores['XGBoost'] = grid_search.best_score_
        
        print(f"最佳参数: {grid_search.best_params_}")
        print(f"最佳交叉验证分数: {grid_search.best_score_:.4f}")
        
        return grid_search.best_estimator_
    
    def tune_svm(self, X_train, y_train, cv=5):
        """调优SVM"""
        param_grid = {
            'C': [0.1, 1, 10, 100],
            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],
            'kernel': ['rbf', 'linear']
        }
        
        model = ModelFactory.create_svm()
        grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)
        
        print("正在调优SVM参数...")
        grid_search.fit(X_train, y_train)
        
        self.best_params['SVM'] = grid_search.best_params_
        self.best_scores['SVM'] = grid_search.best_score_
        
        print(f"最佳参数: {grid_search.best_params_}")
        print(f"最佳交叉验证分数: {grid_search.best_score_:.4f}")
        
        return grid_search.best_estimator_
```

## 实验管理与追踪

### MLflow实验追踪

```python
# src/experiments/experiment_tracker.py
import mlflow
import mlflow.sklearn
import mlflow.tensorflow
import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict, Any
import json
import os

class ExperimentTracker:
    """实验追踪器"""
    
    def __init__(self, experiment_name="urban_sounds_classification", tracking_uri=None):
        self.experiment_name = experiment_name
        if tracking_uri:
            mlflow.set_tracking_uri(tracking_uri)
        
        # 设置或创建实验
        try:
            experiment = mlflow.get_experiment_by_name(experiment_name)
            if experiment is None:
                experiment_id = mlflow.create_experiment(experiment_name)
            else:
                experiment_id = experiment.experiment_id
            mlflow.set_experiment(experiment_name)
        except Exception as e:
            print(f"实验设置失败: {e}")
    
    def log_experiment(self, model, model_name: str, params: Dict, metrics: Dict, 
                      artifacts: Dict = None, tags: Dict = None):
        """记录完整实验"""
        with mlflow.start_run(run_name=model_name) as run:
            # 记录参数
            mlflow.log_params(params)
            
            # 记录指标
            mlflow.log_metrics(metrics)
            
            # 记录标签
            if tags:
                mlflow.set_tags(tags)
            
            # 记录模型
            if hasattr(model, 'fit'):  # sklearn模型
                mlflow.sklearn.log_model(model, model_name)
            elif hasattr(model, 'predict') and hasattr(model, 'save'):  # tensorflow模型
                mlflow.tensorflow.log_model(model, model_name)
            
            # 记录artifacts
            if artifacts:
                for name, artifact in artifacts.items():
                    if isinstance(artifact, plt.Figure):
                        artifact.savefig(f"{name}.png")
                        mlflow.log_artifact(f"{name}.png")
                        os.remove(f"{name}.png")
                    elif isinstance(artifact, (dict, list)):
                        with open(f"{name}.json", 'w') as f:
                            json.dump(artifact, f)
                        mlflow.log_artifact(f"{name}.json")
                        os.remove(f"{name}.json")
            
            print(f"实验 {model_name} 已记录，Run ID: {run.info.run_id}")
            return run.info.run_id
    
    def compare_experiments(self, metric_name="accuracy"):
        """比较实验结果"""
        experiment = mlflow.get_experiment_by_name(self.experiment_name)
        runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
        
        if runs.empty:
            print("没有找到实验记录")
            return None
        
        # 排序并显示结果
        if f"metrics.{metric_name}" in runs.columns:
            runs_sorted = runs.sort_values(f"metrics.{metric_name}", ascending=False)
            
            comparison_df = runs_sorted[[
                'run_id', 'tags.mlflow.runName', f'metrics.{metric_name}', 
                'start_time', 'status'
            ]].copy()
            
            comparison_df.columns = ['Run ID', 'Model Name', metric_name.title(), 'Start Time', 'Status']
            
            print(f"\n=== 实验结果比较 (按{metric_name}排序) ===")
            print(comparison_df.to_string(index=False))
            
            return comparison_df
        else:
            print(f"指标 {metric_name} 不存在")
            return None
    
    def get_best_model(self, metric_name="accuracy"):
        """获取最佳模型"""
        experiment = mlflow.get_experiment_by_name(self.experiment_name)
        runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
        
        if runs.empty:
            print("没有找到实验记录")
            return None
        
        if f"metrics.{metric_name}" in runs.columns:
            best_run = runs.loc[runs[f"metrics.{metric_name}"].idxmax()]
            
            print(f"\n=== 最佳模型 (基于{metric_name}) ===")
            print(f"模型名称: {best_run.get('tags.mlflow.runName', 'Unknown')}")
            print(f"{metric_name}: {best_run[f'metrics.{metric_name}']:.4f}")
            print(f"Run ID: {best_run['run_id']}")
            
            # 加载模型
            model_uri = f"runs:/{best_run['run_id']}/model"
            try:
                model = mlflow.sklearn.load_model(model_uri)
                print("模型类型: sklearn")
            except:
                try:
                    model = mlflow.tensorflow.load_model(model_uri)
                    print("模型类型: tensorflow")
                except:
                    print("无法加载模型")
                    model = None
            
            return {
                'model': model,
                'run_id': best_run['run_id'],
                'metrics': {k.replace('metrics.', ''): v for k, v in best_run.items() 
                          if k.startswith('metrics.')},
                'params': {k.replace('params.', ''): v for k, v in best_run.items() 
                         if k.startswith('params.')}
            }
        else:
            print(f"指标 {metric_name} 不存在")
            return None

# 实验运行示例
def run_comprehensive_experiment():
    """运行完整的实验流程"""
    # 初始化追踪器
    tracker = ExperimentTracker()
    
    # 数据准备
    data_loader = UrbanSoundDataLoader('data/raw', 'data/UrbanSound8K.csv')
    pipeline = FeaturePipeline()
    features_df, labels = pipeline.prepare_features(data_loader, sample_size=2000)
    
    X_processed, y_processed = pipeline.fit_transform(features_df, labels)
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed
    )
    
    # 模型配置
    models_config = {
        'RandomForest_Basic': {
            'model': ModelFactory.create_random_forest(n_estimators=100),
            'params': {'n_estimators': 100, 'max_depth': None}
        },
        'RandomForest_Tuned': {
            'model': ModelFactory.create_random_forest(n_estimators=200, max_depth=15),
            'params': {'n_estimators': 200, 'max_depth': 15}
        },
        'XGBoost_Basic': {
            'model': ModelFactory.create_xgboost(n_estimators=100),
            'params': {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1}
        },
        'XGBoost_Tuned': {
            'model': ModelFactory.create_xgboost(n_estimators=300, max_depth=8, learning_rate=0.05),
            'params': {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.05}
        }
    }
    
    # 训练和记录实验
    evaluator = ModelEvaluator(class_names=data_loader.class_names)
    
    for model_name, config in models_config.items():
        print(f"\n正在训练 {model_name}...")
        
        # 训练模型
        model = config['model']
        model.fit(X_train, y_train)
        
        # 评估模型
        result = evaluator.evaluate_model(model, X_test, y_test, model_name)
        
        # 记录实验
        tracker.log_experiment(
            model=model,
            model_name=model_name,
            params=config['params'],
            metrics={
                'accuracy': result['accuracy'],
                'precision': result['precision'],
                'recall': result['recall'],
                'f1': result['f1']
            },
            tags={
                'model_type': 'sklearn',
                'dataset_size': len(X_train),
                'features_count': X_train.shape[1]
            }
        )
    
    # 比较实验结果
    tracker.compare_experiments('accuracy')
    
    # 获取最佳模型
    best_model_info = tracker.get_best_model('accuracy')
    
    return best_model_info
```

## 模型部署与服务化

### FastAPI服务

```python
# deployment/api_server.py
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import librosa
import numpy as np
import joblib
import mlflow
from typing import Dict, List
import tempfile
import os
from src.features.feature_extractor import AudioFeatureExtractor, AudioPreprocessor
from src.features.feature_pipeline import FeaturePipeline

app = FastAPI(title="Urban Sounds Classification API", version="1.0.0")

# 添加CORS中间件
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 全局变量存储模型和处理器
model = None
feature_pipeline = None
class_names = [
    'air_conditioner', 'car_horn', 'children_playing', 'dog_bark',
    'drilling', 'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music'
]

@app.on_event("startup")
async def load_model():
    """启动时加载模型"""
    global model, feature_pipeline
    
    try:
        # 加载最佳模型
        model_uri = "models:/urban_sounds_classifier/Production"
        model = mlflow.sklearn.load_model(model_uri)
        
        # 加载特征处理流水线
        feature_pipeline = FeaturePipeline()
        feature_pipeline.load_pipeline("models/feature_pipeline.pkl")
        
        print("模型和特征流水线加载成功")
    except Exception as e:
        print(f"模型加载失败: {e}")
        # 备用加载方式
        model = joblib.load("models/best_model.pkl")
        feature_pipeline = joblib.load("models/feature_pipeline.pkl")

@app.get("/")
async def root():
    """根路径"""
    return {"message": "Urban Sounds Classification API", "version": "1.0.0"}

@app.get("/health")
async def health_check():
    """健康检查"""
    return {"status": "healthy", "model_loaded": model is not None}

@app.post("/predict")
async def predict_audio(file: UploadFile = File(...)):
    """预测音频分类"""
    if not file.filename.endswith(('.wav', '.mp3', '.flac')):
        raise HTTPException(status_code=400, detail="仅支持音频文件 (.wav, .mp3, .flac)")
    
    try:
        # 保存上传的文件到临时位置
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        # 加载音频
        audio, sr = librosa.load(tmp_file_path, sr=22050)
        
        # 预处理音频
        preprocessor = AudioPreprocessor()
        processed_audio = preprocessor.preprocess(audio)
        
        # 提取特征
        extractor = AudioFeatureExtractor()
        features = extractor.extract_features(processed_audio)
        
        # 转换为DataFrame并处理
        import pandas as pd
        features_df = pd.DataFrame([features])
        features_processed = feature_pipeline.transform(features_df)
        
        # 预测
        prediction = model.predict(features_processed)[0]
        probabilities = model.predict_proba(features_processed)[0]
        
        # 准备结果
        result = {
            "predicted_class": class_names[prediction],
            "predicted_class_id": int(prediction),
            "confidence": float(max(probabilities)),
            "probabilities": {
                class_names[i]: float(prob) for i, prob in enumerate(probabilities)
            },
            "audio_duration": len(audio) / sr
        }
        
        # 清理临时文件
        os.unlink(tmp_file_path)
        
        return result
        
    except Exception as e:
        # 清理临时文件
        if 'tmp_file_path' in locals():
            try:
                os.unlink(tmp_file_path)
            except:
                pass
        raise HTTPException(status_code=500, detail=f"预测失败: {str(e)}")

@app.post("/predict_batch")
async def predict_batch_audio(files: List[UploadFile] = File(...)):
    """批量预测音频分类"""
    if len(files) > 10:
        raise HTTPException(status_code=400, detail="一次最多只能处理10个文件")
    
    results = []
    
    for file in files:
        try:
            # 重用单个预测的逻辑
            result = await predict_audio(file)
            result["filename"] = file.filename
            results.append(result)
        except Exception as e:
            results.append({
                "filename": file.filename,
                "error": str(e)
            })
    
    return {"results": results, "total_files": len(files)}

@app.get("/classes")
async def get_classes():
    """获取所有类别"""
    return {"classes": class_names, "total_classes": len(class_names)}

@app.get("/model_info")
async def get_model_info():
    """获取模型信息"""
    return {
        "model_type": type(model).__name__,
        "classes": class_names,
        "total_classes": len(class_names),
        "features_count": getattr(model, 'n_features_in_', 'Unknown')
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### Docker部署

```dockerfile
# deployment/Dockerfile
FROM python:3.9-slim

WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    libsndfile1 \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# 复制依赖文件
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 复制项目文件
COPY . .

# 安装项目
RUN pip install -e .

# 创建模型目录
RUN mkdir -p models

# 暴露端口
EXPOSE 8000

# 健康检查
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# 启动命令
CMD ["uvicorn", "deployment.api_server:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# deployment/docker-compose.yml
version: '3.8'

services:
  urban-sounds-api:
    build:
      context: .
      dockerfile: deployment/Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - ./data:/app/data
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    depends_on:
      - mlflow
    restart: unless-stopped

  mlflow:
    image: python:3.9-slim
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlflow/mlruns
    working_dir: /mlflow
    command: >
      bash -c "pip install mlflow && 
               mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns"
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./deployment/nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - urban-sounds-api
    restart: unless-stopped
```

## MLOps最佳实践

### 自动化训练流水线

```python
# src/pipeline/training_pipeline.py
import yaml
import argparse
from pathlib import Path
import mlflow
from src.data.data_loader import UrbanSoundDataLoader
from src.features.feature_pipeline import FeaturePipeline
from src.models.classifiers import ModelFactory, ModelTrainer
from src.models.evaluation import ModelEvaluator, HyperparameterTuner
from src.experiments.experiment_tracker import ExperimentTracker
from sklearn.model_selection import train_test_split

class TrainingPipeline:
    """训练流水线"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.tracker = ExperimentTracker(
            experiment_name=self.config['experiment']['name']
        )
    
    def run(self):
        """运行完整训练流水线"""
        print("=== 开始训练流水线 ===")
        
        # 1. 数据加载
        print("1. 加载数据...")
        data_loader = UrbanSoundDataLoader(
            data_path=self.config['data']['raw_path'],
            metadata_path=self.config['data']['metadata_path']
        )
        
        # 2. 特征工程
        print("2. 特征工程...")
        pipeline = FeaturePipeline()
        features_df, labels = pipeline.prepare_features(
            data_loader, 
            sample_size=self.config['data'].get('sample_size')
        )
        
        X_processed, y_processed = pipeline.fit_transform(features_df, labels)
        
        # 保存特征流水线
        pipeline.save_pipeline(self.config['output']['feature_pipeline_path'])
        
        # 3. 数据分割
        print("3. 数据分割...")
        X_train, X_test, y_train, y_test = train_test_split(
            X_processed, y_processed, 
            test_size=self.config['data']['test_size'],
            random_state=self.config['data']['random_state'],
            stratify=y_processed
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train,
            test_size=self.config['data']['val_size'],
            random_state=self.config['data']['random_state'],
            stratify=y_train
        )
        
        # 4. 模型训练
        print("4. 模型训练...")
        trainer = ModelTrainer(self.config['experiment']['name'])
        evaluator = ModelEvaluator(class_names=data_loader.class_names)
        
        best_model = None
        best_score = 0
        
        for model_config in self.config['models']:
            model_name = model_config['name']
            model_type = model_config['type']
            model_params = model_config['params']
            
            print(f"训练 {model_name}...")
            
            # 创建模型
            if model_type == 'RandomForest':
                model = ModelFactory.create_random_forest(**model_params)
            elif model_type == 'XGBoost':
                model = ModelFactory.create_xgboost(**model_params)
            elif model_type == 'SVM':
                model = ModelFactory.create_svm(**model_params)
            elif model_type == 'NeuralNetwork':
                model = ModelFactory.create_neural_network(
                    input_dim=X_train.shape[1],
                    num_classes=len(data_loader.class_names),
                    **model_params
                )
            else:
                print(f"未知模型类型: {model_type}")
                continue
            
            # 训练模型
            if model_type == 'NeuralNetwork':
                result = trainer.train_neural_network(
                    model, X_train, y_train, X_val, y_val,
                    epochs=self.config['training']['epochs'],
                    batch_size=self.config['training']['batch_size'],
                    model_name=model_name
                )
            else:
                result = trainer.train_sklearn_model(
                    model, X_train, y_train, X_val, y_val, model_name
                )
            
            # 记录最佳模型
            if result['val_accuracy'] > best_score:
                best_score = result['val_accuracy']
                best_model = result['model']
                
                # 保存最佳模型
                import joblib
                joblib.dump(best_model, self.config['output']['best_model_path'])
        
        # 5. 最终评估
        print("5. 最终评估...")
        if best_model:
            final_result = evaluator.evaluate_model(
                best_model, X_test, y_test, "BestModel"
            )
            
            print(f"最佳模型测试集准确率: {final_result['accuracy']:.4f}")
        
        print("=== 训练流水线完成 ===")
        
        return {
            'best_model': best_model,
            'test_accuracy': final_result['accuracy'] if best_model else None,
            'feature_pipeline': pipeline
        }

# 配置文件示例
def create_config_template():
    """创建配置文件模板"""
    config = {
        'experiment': {
            'name': 'urban_sounds_classification_v2'
        },
        'data': {
            'raw_path': 'data/raw',
            'metadata_path': 'data/UrbanSound8K.csv',
            'sample_size': 2000,
            'test_size': 0.2,
            'val_size': 0.25,
            'random_state': 42
        },
        'models': [
            {
                'name': 'RandomForest_Optimized',
                'type': 'RandomForest',
                'params': {
                    'n_estimators': 200,
                    'max_depth': 15,
                    'random_state': 42
                }
            },
            {
                'name': 'XGBoost_Optimized',
                'type': 'XGBoost',
                'params': {
                    'n_estimators': 300,
                    'max_depth': 8,
                    'learning_rate': 0.05,
                    'random_state': 42
                }
            }
        ],
        'training': {
            'epochs': 100,
            'batch_size': 32
        },
        'output': {
            'best_model_path': 'models/best_model.pkl',
            'feature_pipeline_path': 'models/feature_pipeline.pkl'
        }
    }
    
    with open('config/training_config.yaml', 'w') as f:
        yaml.dump(config, f, default_flow_style=False)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='运行训练流水线')
    parser.add_argument('--config', required=True, help='配置文件路径')
    args = parser.parse_args()
    
    pipeline = TrainingPipeline(args.config)
    pipeline.run()
```

## Jupyter集成深度应用

### JupyterMCP集成

基于搜索结果中的JupyterMCP，Claude Code可以直接与Jupyter Notebook集成：

```python
# 安装JupyterMCP
# pip install jupyter-markdown-mcp

# notebooks/advanced-jupyter-integration.ipynb
"""
这个notebook展示了Claude Code与Jupyter的深度集成应用
"""

# 1. Claude Code可以直接编辑notebook单元格
def create_notebook_cell(cell_type, content):
    """创建notebook单元格"""
    if cell_type == "code":
        return {
            "cell_type": "code",
            "source": content,
            "metadata": {},
            "outputs": [],
            "execution_count": None
        }
    elif cell_type == "markdown":
        return {
            "cell_type": "markdown",
            "source": content,
            "metadata": {}
        }

# 2. 自动化实验流程
def automated_experiment_flow():
    """自动化实验流程"""
    experiments = [
        {
            "name": "baseline_random_forest",
            "params": {"n_estimators": 100, "max_depth": 10}
        },
        {
            "name": "optimized_random_forest", 
            "params": {"n_estimators": 200, "max_depth": 15}
        }
    ]
    
    results = []
    for exp in experiments:
        # Claude Code可以自动生成和执行这些实验
        model = ModelFactory.create_random_forest(**exp["params"])
        # ... 训练和评估逻辑
        results.append({"experiment": exp["name"], "accuracy": 0.85})
    
    return results

# 3. 交互式可视化
import plotly.graph_objects as go
from plotly.subplots import make_subplots

def create_interactive_dashboard(results):
    """创建交互式结果面板"""
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('模型性能对比', '混淆矩阵', '特征重要性', '学习曲线'),
        specs=[[{"type": "bar"}, {"type": "heatmap"}],
               [{"type": "bar"}, {"type": "scatter"}]]
    )
    
    # 添加各种图表
    # ...
    
    fig.show()

# 4. 自动报告生成
def generate_experiment_report(results):
    """生成实验报告"""
    report = f"""
    # 城市声音分类实验报告
    
    ## 实验概述
    本次实验共训练了 {len(results)} 个模型，最佳准确率为 {max([r['accuracy'] for r in results]):.4f}
    
    ## 模型性能对比
    """
    
    for result in results:
        report += f"- {result['experiment']}: {result['accuracy']:.4f}\n"
    
    return report
```

## 总结

本文通过完整的城市声音分类项目，全面展示了Claude Code在数据科学和机器学习项目中的强大能力：

### 核心优势

1. **项目结构化**: 自动创建标准化的项目结构和配置
2. **代码生成**: 智能生成数据处理、特征工程和模型训练代码
3. **实验管理**: 系统化的实验追踪和模型比较
4. **部署自动化**: 一键生成API服务和Docker配置
5. **Jupyter集成**: 原生支持notebook工作流程

### 最佳实践

1. **使用CLAUDE.md**: 定义清晰的项目上下文和技术栈
2. **模块化设计**: 将功能分解为独立的模块，便于维护和复用
3. **实验追踪**: 使用MLflow等工具记录所有实验过程
4. **版本控制**: 对代码、数据和模型进行版本管理
5. **持续集成**: 建立自动化的训练和部署流水线

### 性能提升

通过Claude Code的辅助，数据科学项目的开发效率能够显著提升：
- 代码生成速度提升300%
- 实验管理更加系统化
- 从探索到部署的端到端自动化
- 代码质量和一致性大幅改善

Claude Code将数据科学项目从"研究代码"转变为"工程代码"，使得AI模型能够更快速、更可靠地从实验室走向生产环境。

## 相关文章推荐

- [文件操作与代码生成精讲](07-文件操作与代码生成精讲.md) - 了解基础的代码生成能力
- [测试驱动开发(TDD)与Claude Code](10-测试驱动开发TDD与Claude-Code.md) - 学习如何编写测试
- [项目文档自动生成](12-项目文档自动生成.md) - 自动化项目文档
- [移动应用开发指南](31-移动应用开发指南.md) - 下一篇文章内容

---

*本文是Claude Code完整教程系列的第30篇，深入探讨了数据科学与机器学习项目的完整开发流程。下一篇将介绍移动应用开发指南。*