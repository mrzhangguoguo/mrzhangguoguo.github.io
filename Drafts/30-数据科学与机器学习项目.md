# æ•°æ®ç§‘å­¦ä¸æœºå™¨å­¦ä¹ é¡¹ç›®ï¼šç”¨Claude CodeåŠ é€ŸAIæ¨¡å‹å¼€å‘

> æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¡¹ç›®å…·æœ‰æ¢ç´¢æ€§å¼ºã€è¿­ä»£é¢‘ç¹çš„ç‰¹ç‚¹ï¼ŒClaude Codeåœ¨æ•°æ®å¤„ç†ã€æ¨¡å‹å¼€å‘ã€å®éªŒç®¡ç†ç­‰ç¯èŠ‚éƒ½èƒ½æä¾›å¼ºå¤§çš„AIè¾…åŠ©èƒ½åŠ›ã€‚æœ¬æ–‡å°†é€šè¿‡å®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®æ¡ˆä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨Claude Codeæå‡æ•°æ®ç§‘å­¦å·¥ä½œæ•ˆç‡ã€‚

## ğŸ“‹ æœ¬æ–‡ç›®å½•

- [æ•°æ®ç§‘å­¦é¡¹ç›®æ¦‚è¿°](#æ•°æ®ç§‘å­¦é¡¹ç›®æ¦‚è¿°)
- [é¡¹ç›®ç¯å¢ƒæ­å»º](#é¡¹ç›®ç¯å¢ƒæ­å»º)
- [æ•°æ®è·å–ä¸æ¢ç´¢](#æ•°æ®è·å–ä¸æ¢ç´¢)
- [æ•°æ®é¢„å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹](#æ•°æ®é¢„å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹)
- [æ¨¡å‹å¼€å‘ä¸è®­ç»ƒ](#æ¨¡å‹å¼€å‘ä¸è®­ç»ƒ)
- [æ¨¡å‹è¯„ä¼°ä¸è°ƒä¼˜](#æ¨¡å‹è¯„ä¼°ä¸è°ƒä¼˜)
- [å®éªŒç®¡ç†ä¸è¿½è¸ª](#å®éªŒç®¡ç†ä¸è¿½è¸ª)
- [æ¨¡å‹éƒ¨ç½²ä¸æœåŠ¡åŒ–](#æ¨¡å‹éƒ¨ç½²ä¸æœåŠ¡åŒ–)
- [MLOpsæœ€ä½³å®è·µ](#mlopsæœ€ä½³å®è·µ)
- [Jupyteré›†æˆæ·±åº¦åº”ç”¨](#jupyteré›†æˆæ·±åº¦åº”ç”¨)

## æ•°æ®ç§‘å­¦é¡¹ç›®æ¦‚è¿°

### Claude Codeåœ¨æ•°æ®ç§‘å­¦ä¸­çš„ä¼˜åŠ¿

Claude Codeä¸ºæ•°æ®ç§‘å­¦é¡¹ç›®æä¾›äº†å…¨æ–¹ä½çš„æ”¯æŒï¼Œç‰¹åˆ«æ˜¯åœ¨ä»¥ä¸‹æ–¹é¢è¡¨ç°çªå‡ºï¼š

- **Jupyter NotebookåŸç”Ÿæ”¯æŒ**ï¼šç›´æ¥è¯»å–ã€ç¼–è¾‘å’Œæ‰§è¡ŒJupyter notebook
- **æ™ºèƒ½ä»£ç ç”Ÿæˆ**ï¼šæ ¹æ®æ•°æ®ç‰¹å¾è‡ªåŠ¨ç”ŸæˆEDAå’Œå»ºæ¨¡ä»£ç 
- **å®éªŒç®¡ç†**ï¼šç³»ç»ŸåŒ–çš„å®éªŒè¿½è¸ªå’Œç»“æœå¯¹æ¯”
- **ä»£ç é‡æ„**ï¼šå°†æ¢ç´¢æ€§ä»£ç è½¬æ¢ä¸ºç”Ÿäº§çº§ä»£ç 

### é¡¹ç›®æ¡ˆä¾‹ï¼šåŸå¸‚å£°éŸ³åˆ†ç±»ç³»ç»Ÿ

æˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„éŸ³é¢‘æœºå™¨å­¦ä¹ é¡¹ç›®"Urban Sounds Classification"æ¥æ¼”ç¤ºClaude Codeåœ¨æ•°æ®ç§‘å­¦é¡¹ç›®ä¸­çš„åº”ç”¨ã€‚è¿™ä¸ªé¡¹ç›®çš„ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªèƒ½å¤Ÿè¯†åˆ«åŸå¸‚ç¯å¢ƒä¸­ä¸åŒå£°éŸ³çš„åˆ†ç±»æ¨¡å‹ã€‚

```markdown
# Urban Sounds Classification Project

## é¡¹ç›®ç›®æ ‡
å¼€å‘ä¸€ä¸ªåŸå¸‚å£°éŸ³åˆ†ç±»ç³»ç»Ÿï¼Œèƒ½å¤Ÿè¯†åˆ«10ç§å¸¸è§çš„åŸå¸‚å£°éŸ³ï¼š
- ç©ºè°ƒå£°ã€æ±½è½¦å–‡å­ã€å„¿ç«¥ç©è€ã€ç‹—å å£°
- é’»å­”å£°ã€å¼•æ“æ€ é€Ÿã€æªå£°ã€æ‰‹æé’»
- è­¦æŠ¥å™¨ã€è¡—å¤´éŸ³ä¹

## æŠ€æœ¯æ ˆ
- æ•°æ®å¤„ç†ï¼špandas, numpy, librosa
- æœºå™¨å­¦ä¹ ï¼šscikit-learn, xgboost
- æ·±åº¦å­¦ä¹ ï¼štensorflow, keras
- å¯è§†åŒ–ï¼šmatplotlib, seaborn, plotly
- å®éªŒç®¡ç†ï¼šmlflow, wandb
- éƒ¨ç½²ï¼šfastapi, docker
```

## é¡¹ç›®ç¯å¢ƒæ­å»º

### ä½¿ç”¨Claude Codeåˆ›å»ºé¡¹ç›®ç»“æ„

é¦–å…ˆåˆ›å»ºé¡¹ç›®çš„CLAUDE.mdæ–‡ä»¶æ¥å®šä¹‰é¡¹ç›®ä¸Šä¸‹æ–‡ï¼š

```markdown
# åŸå¸‚å£°éŸ³åˆ†ç±»é¡¹ç›®

## é¡¹ç›®ä¿¡æ¯
- é¡¹ç›®ç±»å‹ï¼šéŸ³é¢‘æœºå™¨å­¦ä¹ åˆ†ç±»é¡¹ç›®
- ä¸»è¦è¯­è¨€ï¼šPython
- æ•°æ®ç±»å‹ï¼šéŸ³é¢‘æ–‡ä»¶(.wav)
- æ¨¡å‹ç±»å‹ï¼šæ·±åº¦å­¦ä¹ åˆ†ç±»æ¨¡å‹

## æŠ€æœ¯æ ˆ
- æ•°æ®ç§‘å­¦ï¼špandas, numpy, scipy
- éŸ³é¢‘å¤„ç†ï¼šlibrosa, soundfile
- æœºå™¨å­¦ä¹ ï¼šscikit-learn, xgboost
- æ·±åº¦å­¦ä¹ ï¼štensorflow, keras, pytorch
- å¯è§†åŒ–ï¼šmatplotlib, seaborn, plotly
- å®éªŒç®¡ç†ï¼šmlflow, wandb
- å¼€å‘å·¥å…·ï¼šjupyter, ipython

## é¡¹ç›®ç»“æ„
```
urban-sounds-classification/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # åŸå§‹éŸ³é¢‘æ–‡ä»¶
â”‚   â”œâ”€â”€ processed/        # é¢„å¤„ç†åçš„æ•°æ®
â”‚   â””â”€â”€ features/         # æå–çš„ç‰¹å¾
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01-data-exploration.ipynb
â”‚   â”œâ”€â”€ 02-feature-engineering.ipynb
â”‚   â”œâ”€â”€ 03-model-development.ipynb
â”‚   â””â”€â”€ 04-model-evaluation.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/             # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ features/         # ç‰¹å¾å·¥ç¨‹æ¨¡å—
â”‚   â”œâ”€â”€ models/           # æ¨¡å‹å®šä¹‰æ¨¡å—
â”‚   â””â”€â”€ utils/            # å·¥å…·å‡½æ•°
â”œâ”€â”€ models/               # è®­ç»ƒå¥½çš„æ¨¡å‹
â”œâ”€â”€ experiments/          # å®éªŒè®°å½•
â”œâ”€â”€ deployment/           # éƒ¨ç½²ç›¸å…³æ–‡ä»¶
â””â”€â”€ requirements.txt
```

## å¼€å‘è§„èŒƒ
- ä½¿ç”¨Jupyter Notebookè¿›è¡Œæ¢ç´¢æ€§åˆ†æ
- æ ¸å¿ƒåŠŸèƒ½æ¨¡å—åŒ–åˆ°srcç›®å½•
- æ‰€æœ‰å®éªŒä½¿ç”¨MLflowè¿›è¡Œè¿½è¸ª
- ä»£ç éµå¾ªPEP8è§„èŒƒ
- æ¨¡å‹æ€§èƒ½æŒ‡æ ‡ï¼šå‡†ç¡®ç‡>85%
```

è®©Claude Codeä¸ºæˆ‘ä»¬ç”Ÿæˆé¡¹ç›®çš„åˆå§‹ç»“æ„ï¼š

```bash
# è®©Claude Codeåˆ›å»ºé¡¹ç›®ç»“æ„
è¯·å¸®æˆ‘åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„åŸå¸‚å£°éŸ³åˆ†ç±»é¡¹ç›®ï¼ŒåŒ…æ‹¬ï¼š
1. ç›®å½•ç»“æ„åˆ›å»º
2. ç¯å¢ƒé…ç½®æ–‡ä»¶
3. åŸºç¡€çš„æ•°æ®å¤„ç†æ¨¡å—
4. Jupyter notebookæ¨¡æ¿
```

### ç¯å¢ƒé…ç½®æ–‡ä»¶

```python
# requirements.txt
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.0
tensorflow==2.13.0
librosa==0.10.1
matplotlib==3.7.2
seaborn==0.12.2
plotly==5.15.0
jupyter==1.0.0
mlflow==2.5.0
wandb==0.15.8
fastapi==0.101.1
uvicorn==0.23.2
soundfile==0.12.1
xgboost==1.7.6

# development dependencies
pytest==7.4.0
black==23.7.0
flake8==6.0.0
jupyter-notebook==6.5.4
```

```python
# setup.py
from setuptools import setup, find_packages

setup(
    name="urban-sounds-classification",
    version="1.0.0",
    description="Urban sounds classification using machine learning",
    author="Your Name",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
    python_requires=">=3.8",
    install_requires=[
        "numpy>=1.24.0",
        "pandas>=2.0.0",
        "scikit-learn>=1.3.0",
        "tensorflow>=2.13.0",
        "librosa>=0.10.0",
        "matplotlib>=3.7.0",
        "seaborn>=0.12.0",
        "mlflow>=2.5.0"
    ],
    extras_require={
        "dev": [
            "pytest>=7.4.0",
            "black>=23.7.0",
            "flake8>=6.0.0",
            "jupyter>=1.0.0"
        ]
    }
)
```

### Dockerç¯å¢ƒé…ç½®

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    libsndfile1 \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY . .

# å®‰è£…é¡¹ç›®
RUN pip install -e .

# æš´éœ²ç«¯å£
EXPOSE 8888 8000

# å¯åŠ¨å‘½ä»¤
CMD ["jupyter", "notebook", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root"]
```

## æ•°æ®è·å–ä¸æ¢ç´¢

### æ•°æ®åŠ è½½æ¨¡å—

Claude Codeå¸®åŠ©æˆ‘ä»¬åˆ›å»ºä¸“ä¸šçš„æ•°æ®å¤„ç†æ¨¡å—ï¼š

```python
# src/data/data_loader.py
import os
import pandas as pd
import librosa
import numpy as np
from pathlib import Path
from typing import List, Tuple, Dict
import warnings
warnings.filterwarnings('ignore')

class UrbanSoundDataLoader:
    """åŸå¸‚å£°éŸ³æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_path: str, metadata_path: str):
        self.data_path = Path(data_path)
        self.metadata_path = Path(metadata_path)
        self.metadata = self._load_metadata()
        self.class_names = self._get_class_names()
        
    def _load_metadata(self) -> pd.DataFrame:
        """åŠ è½½å…ƒæ•°æ®æ–‡ä»¶"""
        if self.metadata_path.exists():
            return pd.read_csv(self.metadata_path)
        else:
            raise FileNotFoundError(f"Metadata file not found: {self.metadata_path}")
    
    def _get_class_names(self) -> List[str]:
        """è·å–ç±»åˆ«åç§°"""
        return sorted(self.metadata['class'].unique())
    
    def load_audio_file(self, filename: str, sr: int = 22050) -> Tuple[np.ndarray, int]:
        """
        åŠ è½½å•ä¸ªéŸ³é¢‘æ–‡ä»¶
        
        Args:
            filename: éŸ³é¢‘æ–‡ä»¶å
            sr: é‡‡æ ·ç‡
            
        Returns:
            éŸ³é¢‘æ•°æ®å’Œé‡‡æ ·ç‡çš„å…ƒç»„
        """
        filepath = self.data_path / filename
        try:
            audio_data, sample_rate = librosa.load(filepath, sr=sr)
            return audio_data, sample_rate
        except Exception as e:
            print(f"Error loading {filename}: {e}")
            return np.array([]), sr
    
    def load_batch_audio(self, batch_size: int = 100) -> Dict:
        """
        æ‰¹é‡åŠ è½½éŸ³é¢‘æ•°æ®
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            åŒ…å«éŸ³é¢‘æ•°æ®çš„å­—å…¸
        """
        batch_data = {
            'audio': [],
            'labels': [],
            'filenames': [],
            'class_names': []
        }
        
        for idx, row in self.metadata.head(batch_size).iterrows():
            audio, _ = self.load_audio_file(row['slice_file_name'])
            
            if len(audio) > 0:
                batch_data['audio'].append(audio)
                batch_data['labels'].append(row['classID'])
                batch_data['filenames'].append(row['slice_file_name'])
                batch_data['class_names'].append(row['class'])
        
        return batch_data
    
    def get_class_distribution(self) -> pd.Series:
        """è·å–ç±»åˆ«åˆ†å¸ƒ"""
        return self.metadata['class'].value_counts()
    
    def get_data_summary(self) -> Dict:
        """è·å–æ•°æ®é›†æ‘˜è¦ä¿¡æ¯"""
        return {
            'total_files': len(self.metadata),
            'num_classes': len(self.class_names),
            'class_names': self.class_names,
            'class_distribution': self.get_class_distribution().to_dict(),
            'file_size_range': self._get_file_size_stats()
        }
    
    def _get_file_size_stats(self) -> Dict:
        """è·å–æ–‡ä»¶å¤§å°ç»Ÿè®¡"""
        file_sizes = []
        for filename in self.metadata['slice_file_name'].head(100):  # é‡‡æ ·æ£€æŸ¥
            filepath = self.data_path / filename
            if filepath.exists():
                file_sizes.append(filepath.stat().st_size)
        
        if file_sizes:
            return {
                'min_size': min(file_sizes),
                'max_size': max(file_sizes),
                'avg_size': np.mean(file_sizes)
            }
        return {}
```

### æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰

```python
# notebooks/01-data-exploration.ipynb çš„ä¸»è¦å†…å®¹

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import librosa
import librosa.display
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

# è®¾ç½®æ ·å¼
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# 1. æ•°æ®åŸºæœ¬ä¿¡æ¯æ¢ç´¢
def explore_basic_info(data_loader):
    """æ¢ç´¢æ•°æ®åŸºæœ¬ä¿¡æ¯"""
    summary = data_loader.get_data_summary()
    
    print("=== æ•°æ®é›†åŸºæœ¬ä¿¡æ¯ ===")
    print(f"æ€»æ–‡ä»¶æ•°: {summary['total_files']}")
    print(f"ç±»åˆ«æ•°: {summary['num_classes']}")
    print(f"ç±»åˆ«åç§°: {summary['class_names']}")
    
    # ç±»åˆ«åˆ†å¸ƒå¯è§†åŒ–
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    class_dist = pd.Series(summary['class_distribution'])
    class_dist.plot(kind='bar')
    plt.title('ç±»åˆ«åˆ†å¸ƒ')
    plt.xlabel('ç±»åˆ«')
    plt.ylabel('æ ·æœ¬æ•°é‡')
    plt.xticks(rotation=45)
    
    plt.subplot(1, 2, 2)
    plt.pie(class_dist.values, labels=class_dist.index, autopct='%1.1f%%')
    plt.title('ç±»åˆ«åˆ†å¸ƒé¥¼å›¾')
    
    plt.tight_layout()
    plt.show()
    
    return summary

# 2. éŸ³é¢‘ç‰¹å¾å¯è§†åŒ–
def visualize_audio_samples(data_loader, num_samples=3):
    """å¯è§†åŒ–éŸ³é¢‘æ ·æœ¬"""
    batch_data = data_loader.load_batch_audio(batch_size=50)
    
    # ä¸ºæ¯ä¸ªç±»åˆ«é€‰æ‹©ä¸€ä¸ªæ ·æœ¬
    classes = list(set(batch_data['class_names']))
    
    fig, axes = plt.subplots(len(classes), 3, figsize=(15, 4*len(classes)))
    
    for i, class_name in enumerate(classes):
        # æ‰¾åˆ°è¯¥ç±»åˆ«çš„ç¬¬ä¸€ä¸ªæ ·æœ¬
        class_indices = [j for j, c in enumerate(batch_data['class_names']) if c == class_name]
        if not class_indices:
            continue
            
        audio = batch_data['audio'][class_indices[0]]
        
        # æ—¶åŸŸæ³¢å½¢
        axes[i, 0].plot(audio)
        axes[i, 0].set_title(f'{class_name} - æ—¶åŸŸæ³¢å½¢')
        axes[i, 0].set_xlabel('æ ·æœ¬ç‚¹')
        axes[i, 0].set_ylabel('å¹…åº¦')
        
        # é¢‘è°±å›¾
        D = librosa.stft(audio)
        DB = librosa.amplitude_to_db(np.abs(D), ref=np.max)
        img = librosa.display.specshow(DB, x_axis='time', y_axis='hz', ax=axes[i, 1])
        axes[i, 1].set_title(f'{class_name} - é¢‘è°±å›¾')
        
        # MFCCç‰¹å¾
        mfccs = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=13)
        img = librosa.display.specshow(mfccs, x_axis='time', ax=axes[i, 2])
        axes[i, 2].set_title(f'{class_name} - MFCCç‰¹å¾')
    
    plt.tight_layout()
    plt.show()

# 3. éŸ³é¢‘é•¿åº¦åˆ†æ
def analyze_audio_duration(data_loader, sample_size=200):
    """åˆ†æéŸ³é¢‘é•¿åº¦åˆ†å¸ƒ"""
    batch_data = data_loader.load_batch_audio(batch_size=sample_size)
    
    durations = []
    for audio in batch_data['audio']:
        duration = len(audio) / 22050  # è½¬æ¢ä¸ºç§’
        durations.append(duration)
    
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.hist(durations, bins=30, alpha=0.7, edgecolor='black')
    plt.title('éŸ³é¢‘é•¿åº¦åˆ†å¸ƒ')
    plt.xlabel('æ—¶é•¿ (ç§’)')
    plt.ylabel('é¢‘æ¬¡')
    
    plt.subplot(1, 2, 2)
    plt.boxplot(durations)
    plt.title('éŸ³é¢‘é•¿åº¦ç®±çº¿å›¾')
    plt.ylabel('æ—¶é•¿ (ç§’)')
    
    plt.tight_layout()
    plt.show()
    
    print(f"å¹³å‡æ—¶é•¿: {np.mean(durations):.2f}ç§’")
    print(f"æœ€çŸ­æ—¶é•¿: {np.min(durations):.2f}ç§’")
    print(f"æœ€é•¿æ—¶é•¿: {np.max(durations):.2f}ç§’")
    print(f"æ ‡å‡†å·®: {np.std(durations):.2f}ç§’")

# 4. ç‰¹å¾ç›¸å…³æ€§åˆ†æ
def analyze_feature_correlation(data_loader, sample_size=100):
    """åˆ†æéŸ³é¢‘ç‰¹å¾ç›¸å…³æ€§"""
    from src.features.feature_extractor import AudioFeatureExtractor
    
    extractor = AudioFeatureExtractor()
    batch_data = data_loader.load_batch_audio(batch_size=sample_size)
    
    features_list = []
    labels_list = []
    
    for i, audio in enumerate(batch_data['audio']):
        features = extractor.extract_features(audio)
        features_list.append(features)
        labels_list.append(batch_data['labels'][i])
    
    # åˆ›å»ºç‰¹å¾DataFrame
    feature_df = pd.DataFrame(features_list)
    feature_df['label'] = labels_list
    
    # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
    correlation_matrix = feature_df.corr()
    
    # å¯è§†åŒ–ç›¸å…³æ€§çƒ­å›¾
    plt.figure(figsize=(12, 10))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                square=True, linewidths=0.5, cbar_kws={"shrink": .5})
    plt.title('ç‰¹å¾ç›¸å…³æ€§çƒ­å›¾')
    plt.tight_layout()
    plt.show()
    
    return feature_df, correlation_matrix
```

## æ•°æ®é¢„å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹

### éŸ³é¢‘ç‰¹å¾æå–å™¨

```python
# src/features/feature_extractor.py
import librosa
import numpy as np
from typing import Dict, List
import warnings
warnings.filterwarnings('ignore')

class AudioFeatureExtractor:
    """éŸ³é¢‘ç‰¹å¾æå–å™¨"""
    
    def __init__(self, sr: int = 22050, n_mfcc: int = 13):
        self.sr = sr
        self.n_mfcc = n_mfcc
    
    def extract_features(self, audio: np.ndarray) -> Dict[str, float]:
        """
        æå–éŸ³é¢‘ç‰¹å¾
        
        Args:
            audio: éŸ³é¢‘æ•°æ®æ•°ç»„
            
        Returns:
            ç‰¹å¾å­—å…¸
        """
        features = {}
        
        # 1. åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        features.update(self._extract_basic_features(audio))
        
        # 2. é¢‘è°±ç‰¹å¾
        features.update(self._extract_spectral_features(audio))
        
        # 3. MFCCç‰¹å¾
        features.update(self._extract_mfcc_features(audio))
        
        # 4. èŠ‚å¥ç‰¹å¾
        features.update(self._extract_rhythm_features(audio))
        
        # 5. è°æ³¢å’Œç»å…¸ç‰¹å¾
        features.update(self._extract_harmonic_features(audio))
        
        return features
    
    def _extract_basic_features(self, audio: np.ndarray) -> Dict[str, float]:
        """æå–åŸºç¡€ç»Ÿè®¡ç‰¹å¾"""
        return {
            'zero_crossing_rate': float(np.mean(librosa.feature.zero_crossing_rate(audio))),
            'energy': float(np.sum(audio ** 2)),
            'rmse': float(np.mean(librosa.feature.rms(y=audio))),
            'audio_length': len(audio),
            'max_amplitude': float(np.max(np.abs(audio))),
            'mean_amplitude': float(np.mean(np.abs(audio))),
            'std_amplitude': float(np.std(audio))
        }
    
    def _extract_spectral_features(self, audio: np.ndarray) -> Dict[str, float]:
        """æå–é¢‘è°±ç‰¹å¾"""
        # è®¡ç®—é¢‘è°±è´¨å¿ƒ
        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=self.sr)
        
        # è®¡ç®—é¢‘è°±å¸¦å®½
        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=self.sr)
        
        # è®¡ç®—é¢‘è°±å¯¹æ¯”åº¦
        spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=self.sr)
        
        # è®¡ç®—é¢‘è°±æ»šé™
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=self.sr)
        
        return {
            'spectral_centroid': float(np.mean(spectral_centroids)),
            'spectral_bandwidth': float(np.mean(spectral_bandwidth)),
            'spectral_contrast_mean': float(np.mean(spectral_contrast)),
            'spectral_rolloff': float(np.mean(spectral_rolloff))
        }
    
    def _extract_mfcc_features(self, audio: np.ndarray) -> Dict[str, float]:
        """æå–MFCCç‰¹å¾"""
        mfccs = librosa.feature.mfcc(y=audio, sr=self.sr, n_mfcc=self.n_mfcc)
        
        features = {}
        for i in range(self.n_mfcc):
            features[f'mfcc_{i+1}'] = float(np.mean(mfccs[i]))
            features[f'mfcc_{i+1}_std'] = float(np.std(mfccs[i]))
        
        return features
    
    def _extract_rhythm_features(self, audio: np.ndarray) -> Dict[str, float]:
        """æå–èŠ‚å¥ç‰¹å¾"""
        try:
            # è®¡ç®—èŠ‚æ‹
            tempo, beats = librosa.beat.beat_track(y=audio, sr=self.sr)
            
            return {
                'tempo': float(tempo),
                'beat_count': len(beats)
            }
        except:
            return {
                'tempo': 0.0,
                'beat_count': 0
            }
    
    def _extract_harmonic_features(self, audio: np.ndarray) -> Dict[str, float]:
        """æå–è°æ³¢ç‰¹å¾"""
        try:
            # åˆ†ç¦»è°æ³¢å’Œæ‰“å‡»ä¹æˆåˆ†
            harmonic, percussive = librosa.effects.hpss(audio)
            
            return {
                'harmonic_energy': float(np.sum(harmonic ** 2)),
                'percussive_energy': float(np.sum(percussive ** 2)),
                'harmonic_percussive_ratio': float(np.sum(harmonic ** 2) / (np.sum(percussive ** 2) + 1e-10))
            }
        except:
            return {
                'harmonic_energy': 0.0,
                'percussive_energy': 0.0,
                'harmonic_percussive_ratio': 0.0
            }
    
    def extract_batch_features(self, audio_list: List[np.ndarray]) -> List[Dict[str, float]]:
        """æ‰¹é‡æå–ç‰¹å¾"""
        return [self.extract_features(audio) for audio in audio_list]

# æ•°æ®é¢„å¤„ç†ç®¡é“
class AudioPreprocessor:
    """éŸ³é¢‘æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, target_length: int = 4 * 22050):  # 4ç§’é•¿åº¦
        self.target_length = target_length
    
    def normalize_audio(self, audio: np.ndarray) -> np.ndarray:
        """éŸ³é¢‘å½’ä¸€åŒ–"""
        if np.max(np.abs(audio)) > 0:
            return audio / np.max(np.abs(audio))
        return audio
    
    def pad_or_truncate(self, audio: np.ndarray) -> np.ndarray:
        """å¡«å……æˆ–æˆªæ–­éŸ³é¢‘åˆ°å›ºå®šé•¿åº¦"""
        if len(audio) > self.target_length:
            # æˆªæ–­ï¼šå–ä¸­é—´éƒ¨åˆ†
            start = (len(audio) - self.target_length) // 2
            return audio[start:start + self.target_length]
        elif len(audio) < self.target_length:
            # å¡«å……ï¼šä½¿ç”¨é›¶å¡«å……
            pad_length = self.target_length - len(audio)
            return np.pad(audio, (0, pad_length), mode='constant')
        return audio
    
    def remove_silence(self, audio: np.ndarray, top_db: int = 20) -> np.ndarray:
        """ç§»é™¤é™éŸ³éƒ¨åˆ†"""
        return librosa.effects.trim(audio, top_db=top_db)[0]
    
    def preprocess(self, audio: np.ndarray) -> np.ndarray:
        """å®Œæ•´çš„é¢„å¤„ç†æµç¨‹"""
        # 1. ç§»é™¤é™éŸ³
        audio = self.remove_silence(audio)
        
        # 2. å½’ä¸€åŒ–
        audio = self.normalize_audio(audio)
        
        # 3. é•¿åº¦æ ‡å‡†åŒ–
        audio = self.pad_or_truncate(audio)
        
        return audio
```

### ç‰¹å¾å·¥ç¨‹æµæ°´çº¿

```python
# src/features/feature_pipeline.py
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif
from typing import Tuple, List
import joblib

class FeaturePipeline:
    """ç‰¹å¾å·¥ç¨‹æµæ°´çº¿"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.feature_selector = SelectKBest(score_func=f_classif, k=50)
        self.selected_features = None
        
    def prepare_features(self, data_loader, sample_size: int = None) -> Tuple[pd.DataFrame, np.ndarray]:
        """å‡†å¤‡ç‰¹å¾æ•°æ®"""
        from src.features.feature_extractor import AudioFeatureExtractor, AudioPreprocessor
        
        # åŠ è½½æ•°æ®
        if sample_size:
            batch_data = data_loader.load_batch_audio(batch_size=sample_size)
        else:
            batch_data = data_loader.load_batch_audio(batch_size=len(data_loader.metadata))
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        preprocessor = AudioPreprocessor()
        extractor = AudioFeatureExtractor()
        
        # é¢„å¤„ç†å’Œç‰¹å¾æå–
        features_list = []
        labels_list = []
        
        print("æ­£åœ¨æå–ç‰¹å¾...")
        for i, audio in enumerate(batch_data['audio']):
            if i % 100 == 0:
                print(f"å¤„ç†è¿›åº¦: {i}/{len(batch_data['audio'])}")
            
            # é¢„å¤„ç†éŸ³é¢‘
            processed_audio = preprocessor.preprocess(audio)
            
            # æå–ç‰¹å¾
            features = extractor.extract_features(processed_audio)
            features_list.append(features)
            labels_list.append(batch_data['labels'][i])
        
        # åˆ›å»ºç‰¹å¾DataFrame
        feature_df = pd.DataFrame(features_list)
        labels = np.array(labels_list)
        
        print(f"ç‰¹å¾æå–å®Œæˆï¼Œå…±æå– {len(feature_df.columns)} ä¸ªç‰¹å¾")
        return feature_df, labels
    
    def fit_transform(self, X: pd.DataFrame, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """æ‹Ÿåˆå¹¶è½¬æ¢ç‰¹å¾"""
        # 1. å¤„ç†ç¼ºå¤±å€¼
        X_filled = X.fillna(X.mean())
        
        # 2. ç‰¹å¾é€‰æ‹©
        X_selected = self.feature_selector.fit_transform(X_filled, y)
        self.selected_features = X_filled.columns[self.feature_selector.get_support()]
        
        # 3. æ ‡å‡†åŒ–ç‰¹å¾
        X_scaled = self.scaler.fit_transform(X_selected)
        
        # 4. ç¼–ç æ ‡ç­¾
        y_encoded = self.label_encoder.fit_transform(y)
        
        print(f"é€‰æ‹©äº† {len(self.selected_features)} ä¸ªæœ€é‡è¦çš„ç‰¹å¾")
        print(f"ç‰¹å¾åç§°: {list(self.selected_features)}")
        
        return X_scaled, y_encoded
    
    def transform(self, X: pd.DataFrame) -> np.ndarray:
        """è½¬æ¢æ–°æ•°æ®"""
        # å¤„ç†ç¼ºå¤±å€¼
        X_filled = X.fillna(X.mean())
        
        # é€‰æ‹©ç‰¹å¾
        X_selected = X_filled[self.selected_features]
        
        # æ ‡å‡†åŒ–
        X_scaled = self.scaler.transform(X_selected)
        
        return X_scaled
    
    def save_pipeline(self, filepath: str):
        """ä¿å­˜ç‰¹å¾å¤„ç†æµæ°´çº¿"""
        pipeline_data = {
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'feature_selector': self.feature_selector,
            'selected_features': self.selected_features
        }
        joblib.dump(pipeline_data, filepath)
        print(f"ç‰¹å¾æµæ°´çº¿å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_pipeline(self, filepath: str):
        """åŠ è½½ç‰¹å¾å¤„ç†æµæ°´çº¿"""
        pipeline_data = joblib.load(filepath)
        self.scaler = pipeline_data['scaler']
        self.label_encoder = pipeline_data['label_encoder']
        self.feature_selector = pipeline_data['feature_selector']
        self.selected_features = pipeline_data['selected_features']
        print(f"ç‰¹å¾æµæ°´çº¿å·²ä» {filepath} åŠ è½½")
```

## æ¨¡å‹å¼€å‘ä¸è®­ç»ƒ

### æ¨¡å‹å®šä¹‰æ¨¡å—

```python
# src/models/classifiers.py
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import xgboost as xgb
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import mlflow
import mlflow.sklearn
import mlflow.tensorflow

class ModelFactory:
    """æ¨¡å‹å·¥å‚ç±»"""
    
    @staticmethod
    def create_random_forest(n_estimators=100, max_depth=None, random_state=42):
        """åˆ›å»ºéšæœºæ£®æ—æ¨¡å‹"""
        return RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=random_state,
            n_jobs=-1
        )
    
    @staticmethod
    def create_xgboost(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42):
        """åˆ›å»ºXGBoostæ¨¡å‹"""
        return xgb.XGBClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            random_state=random_state,
            n_jobs=-1
        )
    
    @staticmethod
    def create_svm(kernel='rbf', C=1.0, gamma='scale', random_state=42):
        """åˆ›å»ºSVMæ¨¡å‹"""
        return SVC(
            kernel=kernel,
            C=C,
            gamma=gamma,
            random_state=random_state,
            probability=True
        )
    
    @staticmethod
    def create_logistic_regression(C=1.0, max_iter=1000, random_state=42):
        """åˆ›å»ºé€»è¾‘å›å½’æ¨¡å‹"""
        return LogisticRegression(
            C=C,
            max_iter=max_iter,
            random_state=random_state,
            n_jobs=-1
        )
    
    @staticmethod
    def create_neural_network(input_dim, num_classes, hidden_layers=[128, 64], dropout_rate=0.3):
        """åˆ›å»ºç¥ç»ç½‘ç»œæ¨¡å‹"""
        model = Sequential()
        
        # è¾“å…¥å±‚
        model.add(Dense(hidden_layers[0], input_dim=input_dim, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))
        
        # éšè—å±‚
        for units in hidden_layers[1:]:
            model.add(Dense(units, activation='relu'))
            model.add(BatchNormalization())
            model.add(Dropout(dropout_rate))
        
        # è¾“å‡ºå±‚
        model.add(Dense(num_classes, activation='softmax'))
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        return model

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, experiment_name="urban_sounds_classification"):
        self.experiment_name = experiment_name
        mlflow.set_experiment(experiment_name)
    
    def train_sklearn_model(self, model, X_train, y_train, X_val, y_val, model_name):
        """è®­ç»ƒsklearnæ¨¡å‹"""
        with mlflow.start_run(run_name=model_name):
            # è®°å½•å‚æ•°
            if hasattr(model, 'get_params'):
                mlflow.log_params(model.get_params())
            
            # è®­ç»ƒæ¨¡å‹
            print(f"æ­£åœ¨è®­ç»ƒ {model_name}...")
            model.fit(X_train, y_train)
            
            # é¢„æµ‹
            train_pred = model.predict(X_train)
            val_pred = model.predict(X_val)
            
            # è®¡ç®—æŒ‡æ ‡
            train_acc = accuracy_score(y_train, train_pred)
            val_acc = accuracy_score(y_val, val_pred)
            
            # è®°å½•æŒ‡æ ‡
            mlflow.log_metrics({
                'train_accuracy': train_acc,
                'val_accuracy': val_acc
            })
            
            # è®°å½•æ¨¡å‹
            mlflow.sklearn.log_model(model, model_name)
            
            print(f"{model_name} - è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
            
            return {
                'model': model,
                'train_accuracy': train_acc,
                'val_accuracy': val_acc,
                'val_predictions': val_pred
            }
    
    def train_neural_network(self, model, X_train, y_train, X_val, y_val, 
                           epochs=100, batch_size=32, model_name="neural_network"):
        """è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹"""
        with mlflow.start_run(run_name=model_name):
            # è®°å½•å‚æ•°
            mlflow.log_params({
                'epochs': epochs,
                'batch_size': batch_size,
                'optimizer': 'Adam',
                'loss': 'sparse_categorical_crossentropy'
            })
            
            # å›è°ƒå‡½æ•°
            callbacks = [
                EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),
                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
            ]
            
            # è®­ç»ƒæ¨¡å‹
            print(f"æ­£åœ¨è®­ç»ƒ {model_name}...")
            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=epochs,
                batch_size=batch_size,
                callbacks=callbacks,
                verbose=1
            )
            
            # è¯„ä¼°æ¨¡å‹
            train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)
            val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
            
            # è®°å½•æŒ‡æ ‡
            mlflow.log_metrics({
                'train_accuracy': train_acc,
                'train_loss': train_loss,
                'val_accuracy': val_acc,
                'val_loss': val_loss
            })
            
            # è®°å½•æ¨¡å‹
            mlflow.tensorflow.log_model(model, model_name)
            
            print(f"{model_name} - è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
            
            return {
                'model': model,
                'history': history,
                'train_accuracy': train_acc,
                'val_accuracy': val_acc
            }
    
    def compare_models(self, results):
        """æ¯”è¾ƒæ¨¡å‹æ€§èƒ½"""
        comparison_df = pd.DataFrame([
            {
                'model': name,
                'train_accuracy': result['train_accuracy'],
                'val_accuracy': result['val_accuracy'],
                'overfitting': result['train_accuracy'] - result['val_accuracy']
            }
            for name, result in results.items()
        ])
        
        comparison_df = comparison_df.sort_values('val_accuracy', ascending=False)
        print("\n=== æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ ===")
        print(comparison_df.to_string(index=False))
        
        return comparison_df
```

### æ¨¡å‹è®­ç»ƒä¸»æµç¨‹

```python
# notebooks/03-model-development.ipynb çš„æ ¸å¿ƒå†…å®¹

# 1. æ•°æ®å‡†å¤‡
from src.data.data_loader import UrbanSoundDataLoader
from src.features.feature_pipeline import FeaturePipeline
from src.models.classifiers import ModelFactory, ModelTrainer
from sklearn.model_selection import train_test_split
import numpy as np

# åŠ è½½æ•°æ®
data_loader = UrbanSoundDataLoader('data/raw', 'data/UrbanSound8K.csv')

# ç‰¹å¾å·¥ç¨‹
pipeline = FeaturePipeline()
features_df, labels = pipeline.prepare_features(data_loader, sample_size=1000)

# æ•°æ®åˆ†å‰²
X_processed, y_processed = pipeline.fit_transform(features_df, labels)
X_train, X_test, y_train, y_test = train_test_split(
    X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train
)

print(f"è®­ç»ƒé›†: {X_train.shape}")
print(f"éªŒè¯é›†: {X_val.shape}")  
print(f"æµ‹è¯•é›†: {X_test.shape}")

# 2. æ¨¡å‹è®­ç»ƒ
trainer = ModelTrainer()
results = {}

# è®­ç»ƒå¤šä¸ªæ¨¡å‹
models_config = {
    'RandomForest': ModelFactory.create_random_forest(n_estimators=200, max_depth=15),
    'XGBoost': ModelFactory.create_xgboost(n_estimators=200, max_depth=8, learning_rate=0.1),
    'SVM': ModelFactory.create_svm(kernel='rbf', C=10, gamma='scale'),
    'LogisticRegression': ModelFactory.create_logistic_regression(C=1.0)
}

# è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
for name, model in models_config.items():
    result = trainer.train_sklearn_model(model, X_train, y_train, X_val, y_val, name)
    results[name] = result

# è®­ç»ƒç¥ç»ç½‘ç»œ
nn_model = ModelFactory.create_neural_network(
    input_dim=X_train.shape[1], 
    num_classes=len(np.unique(y_train)),
    hidden_layers=[256, 128, 64],
    dropout_rate=0.3
)

nn_result = trainer.train_neural_network(
    nn_model, X_train, y_train, X_val, y_val, 
    epochs=100, batch_size=32, model_name="NeuralNetwork"
)
results['NeuralNetwork'] = nn_result

# 3. æ¨¡å‹æ¯”è¾ƒ
comparison_df = trainer.compare_models(results)
```

## æ¨¡å‹è¯„ä¼°ä¸è°ƒä¼˜

### æ¨¡å‹è¯„ä¼°å·¥å…·

```python
# src/models/evaluation.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_recall_fscore_support, roc_auc_score, roc_curve
)
from sklearn.model_selection import cross_val_score, GridSearchCV
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, class_names=None):
        self.class_names = class_names
    
    def evaluate_model(self, model, X_test, y_test, model_name="Model"):
        """å…¨é¢è¯„ä¼°æ¨¡å‹"""
        print(f"\n=== {model_name} è¯„ä¼°ç»“æœ ===")
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        if hasattr(model, 'predict_proba'):
            y_proba = model.predict_proba(X_test)
        else:
            y_proba = None
        
        # åŸºç¡€æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average='weighted')
        
        print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"ç²¾ç¡®ç‡: {precision:.4f}")
        print(f"å¬å›ç‡: {recall:.4f}")
        print(f"F1åˆ†æ•°: {f1:.4f}")
        
        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, target_names=self.class_names))
        
        # æ··æ·†çŸ©é˜µå¯è§†åŒ–
        self.plot_confusion_matrix(y_test, y_pred, model_name)
        
        # ROCæ›²çº¿ï¼ˆå¤šåˆ†ç±»ï¼‰
        if y_proba is not None:
            self.plot_multiclass_roc(y_test, y_proba, model_name)
        
        # ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
        if hasattr(model, 'feature_importances_'):
            self.plot_feature_importance(model, model_name)
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'predictions': y_pred,
            'probabilities': y_proba
        }
    
    def plot_confusion_matrix(self, y_true, y_pred, model_name):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=self.class_names,
                    yticklabels=self.class_names)
        plt.title(f'{model_name} - æ··æ·†çŸ©é˜µ')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.tight_layout()
        plt.show()
    
    def plot_multiclass_roc(self, y_true, y_proba, model_name):
        """ç»˜åˆ¶å¤šåˆ†ç±»ROCæ›²çº¿"""
        from sklearn.preprocessing import label_binarize
        from sklearn.metrics import roc_curve, auc
        from itertools import cycle
        
        # äºŒå€¼åŒ–æ ‡ç­¾
        y_true_bin = label_binarize(y_true, classes=range(len(self.class_names)))
        n_classes = y_true_bin.shape[1]
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ROCæ›²çº¿
        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        
        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_proba[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
        
        # ç»˜åˆ¶ROCæ›²çº¿
        plt.figure(figsize=(12, 8))
        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green',
                       'yellow', 'purple', 'pink', 'brown', 'gray'])
        
        for i, color in zip(range(n_classes), colors):
            plt.plot(fpr[i], tpr[i], color=color, lw=2,
                    label=f'{self.class_names[i]} (AUC = {roc_auc[i]:.2f})')
        
        plt.plot([0, 1], [0, 1], 'k--', lw=2)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('å‡æ­£ç‡')
        plt.ylabel('çœŸæ­£ç‡')
        plt.title(f'{model_name} - ROCæ›²çº¿')
        plt.legend(loc="lower right")
        plt.tight_layout()
        plt.show()
    
    def plot_feature_importance(self, model, model_name, top_n=20):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            indices = np.argsort(importances)[::-1]
            
            plt.figure(figsize=(12, 8))
            plt.title(f'{model_name} - ç‰¹å¾é‡è¦æ€§ (Top {top_n})')
            plt.bar(range(min(top_n, len(importances))), 
                    importances[indices[:top_n]])
            plt.xticks(range(min(top_n, len(importances))), 
                      [f'Feature {i}' for i in indices[:top_n]], rotation=45)
            plt.tight_layout()
            plt.show()
    
    def cross_validate_model(self, model, X, y, cv=5):
        """äº¤å‰éªŒè¯è¯„ä¼°"""
        scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
        
        print(f"äº¤å‰éªŒè¯ç»“æœ:")
        print(f"å¹³å‡å‡†ç¡®ç‡: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")
        print(f"å„æŠ˜ç»“æœ: {scores}")
        
        return scores

class HyperparameterTuner:
    """è¶…å‚æ•°è°ƒä¼˜å™¨"""
    
    def __init__(self):
        self.best_params = {}
        self.best_scores = {}
    
    def tune_random_forest(self, X_train, y_train, cv=5):
        """è°ƒä¼˜éšæœºæ£®æ—"""
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 15, 20, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        
        model = ModelFactory.create_random_forest()
        grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)
        
        print("æ­£åœ¨è°ƒä¼˜éšæœºæ£®æ—å‚æ•°...")
        grid_search.fit(X_train, y_train)
        
        self.best_params['RandomForest'] = grid_search.best_params_
        self.best_scores['RandomForest'] = grid_search.best_score_
        
        print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {grid_search.best_score_:.4f}")
        
        return grid_search.best_estimator_
    
    def tune_xgboost(self, X_train, y_train, cv=5):
        """è°ƒä¼˜XGBoost"""
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [3, 6, 9],
            'learning_rate': [0.01, 0.1, 0.2],
            'subsample': [0.8, 0.9, 1.0]
        }
        
        model = ModelFactory.create_xgboost()
        grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)
        
        print("æ­£åœ¨è°ƒä¼˜XGBoostå‚æ•°...")
        grid_search.fit(X_train, y_train)
        
        self.best_params['XGBoost'] = grid_search.best_params_
        self.best_scores['XGBoost'] = grid_search.best_score_
        
        print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {grid_search.best_score_:.4f}")
        
        return grid_search.best_estimator_
    
    def tune_svm(self, X_train, y_train, cv=5):
        """è°ƒä¼˜SVM"""
        param_grid = {
            'C': [0.1, 1, 10, 100],
            'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],
            'kernel': ['rbf', 'linear']
        }
        
        model = ModelFactory.create_svm()
        grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)
        
        print("æ­£åœ¨è°ƒä¼˜SVMå‚æ•°...")
        grid_search.fit(X_train, y_train)
        
        self.best_params['SVM'] = grid_search.best_params_
        self.best_scores['SVM'] = grid_search.best_score_
        
        print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {grid_search.best_score_:.4f}")
        
        return grid_search.best_estimator_
```

## å®éªŒç®¡ç†ä¸è¿½è¸ª

### MLflowå®éªŒè¿½è¸ª

```python
# src/experiments/experiment_tracker.py
import mlflow
import mlflow.sklearn
import mlflow.tensorflow
import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict, Any
import json
import os

class ExperimentTracker:
    """å®éªŒè¿½è¸ªå™¨"""
    
    def __init__(self, experiment_name="urban_sounds_classification", tracking_uri=None):
        self.experiment_name = experiment_name
        if tracking_uri:
            mlflow.set_tracking_uri(tracking_uri)
        
        # è®¾ç½®æˆ–åˆ›å»ºå®éªŒ
        try:
            experiment = mlflow.get_experiment_by_name(experiment_name)
            if experiment is None:
                experiment_id = mlflow.create_experiment(experiment_name)
            else:
                experiment_id = experiment.experiment_id
            mlflow.set_experiment(experiment_name)
        except Exception as e:
            print(f"å®éªŒè®¾ç½®å¤±è´¥: {e}")
    
    def log_experiment(self, model, model_name: str, params: Dict, metrics: Dict, 
                      artifacts: Dict = None, tags: Dict = None):
        """è®°å½•å®Œæ•´å®éªŒ"""
        with mlflow.start_run(run_name=model_name) as run:
            # è®°å½•å‚æ•°
            mlflow.log_params(params)
            
            # è®°å½•æŒ‡æ ‡
            mlflow.log_metrics(metrics)
            
            # è®°å½•æ ‡ç­¾
            if tags:
                mlflow.set_tags(tags)
            
            # è®°å½•æ¨¡å‹
            if hasattr(model, 'fit'):  # sklearnæ¨¡å‹
                mlflow.sklearn.log_model(model, model_name)
            elif hasattr(model, 'predict') and hasattr(model, 'save'):  # tensorflowæ¨¡å‹
                mlflow.tensorflow.log_model(model, model_name)
            
            # è®°å½•artifacts
            if artifacts:
                for name, artifact in artifacts.items():
                    if isinstance(artifact, plt.Figure):
                        artifact.savefig(f"{name}.png")
                        mlflow.log_artifact(f"{name}.png")
                        os.remove(f"{name}.png")
                    elif isinstance(artifact, (dict, list)):
                        with open(f"{name}.json", 'w') as f:
                            json.dump(artifact, f)
                        mlflow.log_artifact(f"{name}.json")
                        os.remove(f"{name}.json")
            
            print(f"å®éªŒ {model_name} å·²è®°å½•ï¼ŒRun ID: {run.info.run_id}")
            return run.info.run_id
    
    def compare_experiments(self, metric_name="accuracy"):
        """æ¯”è¾ƒå®éªŒç»“æœ"""
        experiment = mlflow.get_experiment_by_name(self.experiment_name)
        runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
        
        if runs.empty:
            print("æ²¡æœ‰æ‰¾åˆ°å®éªŒè®°å½•")
            return None
        
        # æ’åºå¹¶æ˜¾ç¤ºç»“æœ
        if f"metrics.{metric_name}" in runs.columns:
            runs_sorted = runs.sort_values(f"metrics.{metric_name}", ascending=False)
            
            comparison_df = runs_sorted[[
                'run_id', 'tags.mlflow.runName', f'metrics.{metric_name}', 
                'start_time', 'status'
            ]].copy()
            
            comparison_df.columns = ['Run ID', 'Model Name', metric_name.title(), 'Start Time', 'Status']
            
            print(f"\n=== å®éªŒç»“æœæ¯”è¾ƒ (æŒ‰{metric_name}æ’åº) ===")
            print(comparison_df.to_string(index=False))
            
            return comparison_df
        else:
            print(f"æŒ‡æ ‡ {metric_name} ä¸å­˜åœ¨")
            return None
    
    def get_best_model(self, metric_name="accuracy"):
        """è·å–æœ€ä½³æ¨¡å‹"""
        experiment = mlflow.get_experiment_by_name(self.experiment_name)
        runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
        
        if runs.empty:
            print("æ²¡æœ‰æ‰¾åˆ°å®éªŒè®°å½•")
            return None
        
        if f"metrics.{metric_name}" in runs.columns:
            best_run = runs.loc[runs[f"metrics.{metric_name}"].idxmax()]
            
            print(f"\n=== æœ€ä½³æ¨¡å‹ (åŸºäº{metric_name}) ===")
            print(f"æ¨¡å‹åç§°: {best_run.get('tags.mlflow.runName', 'Unknown')}")
            print(f"{metric_name}: {best_run[f'metrics.{metric_name}']:.4f}")
            print(f"Run ID: {best_run['run_id']}")
            
            # åŠ è½½æ¨¡å‹
            model_uri = f"runs:/{best_run['run_id']}/model"
            try:
                model = mlflow.sklearn.load_model(model_uri)
                print("æ¨¡å‹ç±»å‹: sklearn")
            except:
                try:
                    model = mlflow.tensorflow.load_model(model_uri)
                    print("æ¨¡å‹ç±»å‹: tensorflow")
                except:
                    print("æ— æ³•åŠ è½½æ¨¡å‹")
                    model = None
            
            return {
                'model': model,
                'run_id': best_run['run_id'],
                'metrics': {k.replace('metrics.', ''): v for k, v in best_run.items() 
                          if k.startswith('metrics.')},
                'params': {k.replace('params.', ''): v for k, v in best_run.items() 
                         if k.startswith('params.')}
            }
        else:
            print(f"æŒ‡æ ‡ {metric_name} ä¸å­˜åœ¨")
            return None

# å®éªŒè¿è¡Œç¤ºä¾‹
def run_comprehensive_experiment():
    """è¿è¡Œå®Œæ•´çš„å®éªŒæµç¨‹"""
    # åˆå§‹åŒ–è¿½è¸ªå™¨
    tracker = ExperimentTracker()
    
    # æ•°æ®å‡†å¤‡
    data_loader = UrbanSoundDataLoader('data/raw', 'data/UrbanSound8K.csv')
    pipeline = FeaturePipeline()
    features_df, labels = pipeline.prepare_features(data_loader, sample_size=2000)
    
    X_processed, y_processed = pipeline.fit_transform(features_df, labels)
    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y_processed, test_size=0.2, random_state=42, stratify=y_processed
    )
    
    # æ¨¡å‹é…ç½®
    models_config = {
        'RandomForest_Basic': {
            'model': ModelFactory.create_random_forest(n_estimators=100),
            'params': {'n_estimators': 100, 'max_depth': None}
        },
        'RandomForest_Tuned': {
            'model': ModelFactory.create_random_forest(n_estimators=200, max_depth=15),
            'params': {'n_estimators': 200, 'max_depth': 15}
        },
        'XGBoost_Basic': {
            'model': ModelFactory.create_xgboost(n_estimators=100),
            'params': {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1}
        },
        'XGBoost_Tuned': {
            'model': ModelFactory.create_xgboost(n_estimators=300, max_depth=8, learning_rate=0.05),
            'params': {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.05}
        }
    }
    
    # è®­ç»ƒå’Œè®°å½•å®éªŒ
    evaluator = ModelEvaluator(class_names=data_loader.class_names)
    
    for model_name, config in models_config.items():
        print(f"\næ­£åœ¨è®­ç»ƒ {model_name}...")
        
        # è®­ç»ƒæ¨¡å‹
        model = config['model']
        model.fit(X_train, y_train)
        
        # è¯„ä¼°æ¨¡å‹
        result = evaluator.evaluate_model(model, X_test, y_test, model_name)
        
        # è®°å½•å®éªŒ
        tracker.log_experiment(
            model=model,
            model_name=model_name,
            params=config['params'],
            metrics={
                'accuracy': result['accuracy'],
                'precision': result['precision'],
                'recall': result['recall'],
                'f1': result['f1']
            },
            tags={
                'model_type': 'sklearn',
                'dataset_size': len(X_train),
                'features_count': X_train.shape[1]
            }
        )
    
    # æ¯”è¾ƒå®éªŒç»“æœ
    tracker.compare_experiments('accuracy')
    
    # è·å–æœ€ä½³æ¨¡å‹
    best_model_info = tracker.get_best_model('accuracy')
    
    return best_model_info
```

## æ¨¡å‹éƒ¨ç½²ä¸æœåŠ¡åŒ–

### FastAPIæœåŠ¡

```python
# deployment/api_server.py
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import librosa
import numpy as np
import joblib
import mlflow
from typing import Dict, List
import tempfile
import os
from src.features.feature_extractor import AudioFeatureExtractor, AudioPreprocessor
from src.features.feature_pipeline import FeaturePipeline

app = FastAPI(title="Urban Sounds Classification API", version="1.0.0")

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡å­˜å‚¨æ¨¡å‹å’Œå¤„ç†å™¨
model = None
feature_pipeline = None
class_names = [
    'air_conditioner', 'car_horn', 'children_playing', 'dog_bark',
    'drilling', 'engine_idling', 'gun_shot', 'jackhammer', 'siren', 'street_music'
]

@app.on_event("startup")
async def load_model():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    global model, feature_pipeline
    
    try:
        # åŠ è½½æœ€ä½³æ¨¡å‹
        model_uri = "models:/urban_sounds_classifier/Production"
        model = mlflow.sklearn.load_model(model_uri)
        
        # åŠ è½½ç‰¹å¾å¤„ç†æµæ°´çº¿
        feature_pipeline = FeaturePipeline()
        feature_pipeline.load_pipeline("models/feature_pipeline.pkl")
        
        print("æ¨¡å‹å’Œç‰¹å¾æµæ°´çº¿åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        # å¤‡ç”¨åŠ è½½æ–¹å¼
        model = joblib.load("models/best_model.pkl")
        feature_pipeline = joblib.load("models/feature_pipeline.pkl")

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {"message": "Urban Sounds Classification API", "version": "1.0.0"}

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "model_loaded": model is not None}

@app.post("/predict")
async def predict_audio(file: UploadFile = File(...)):
    """é¢„æµ‹éŸ³é¢‘åˆ†ç±»"""
    if not file.filename.endswith(('.wav', '.mp3', '.flac')):
        raise HTTPException(status_code=400, detail="ä»…æ”¯æŒéŸ³é¢‘æ–‡ä»¶ (.wav, .mp3, .flac)")
    
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = librosa.load(tmp_file_path, sr=22050)
        
        # é¢„å¤„ç†éŸ³é¢‘
        preprocessor = AudioPreprocessor()
        processed_audio = preprocessor.preprocess(audio)
        
        # æå–ç‰¹å¾
        extractor = AudioFeatureExtractor()
        features = extractor.extract_features(processed_audio)
        
        # è½¬æ¢ä¸ºDataFrameå¹¶å¤„ç†
        import pandas as pd
        features_df = pd.DataFrame([features])
        features_processed = feature_pipeline.transform(features_df)
        
        # é¢„æµ‹
        prediction = model.predict(features_processed)[0]
        probabilities = model.predict_proba(features_processed)[0]
        
        # å‡†å¤‡ç»“æœ
        result = {
            "predicted_class": class_names[prediction],
            "predicted_class_id": int(prediction),
            "confidence": float(max(probabilities)),
            "probabilities": {
                class_names[i]: float(prob) for i, prob in enumerate(probabilities)
            },
            "audio_duration": len(audio) / sr
        }
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(tmp_file_path)
        
        return result
        
    except Exception as e:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if 'tmp_file_path' in locals():
            try:
                os.unlink(tmp_file_path)
            except:
                pass
        raise HTTPException(status_code=500, detail=f"é¢„æµ‹å¤±è´¥: {str(e)}")

@app.post("/predict_batch")
async def predict_batch_audio(files: List[UploadFile] = File(...)):
    """æ‰¹é‡é¢„æµ‹éŸ³é¢‘åˆ†ç±»"""
    if len(files) > 10:
        raise HTTPException(status_code=400, detail="ä¸€æ¬¡æœ€å¤šåªèƒ½å¤„ç†10ä¸ªæ–‡ä»¶")
    
    results = []
    
    for file in files:
        try:
            # é‡ç”¨å•ä¸ªé¢„æµ‹çš„é€»è¾‘
            result = await predict_audio(file)
            result["filename"] = file.filename
            results.append(result)
        except Exception as e:
            results.append({
                "filename": file.filename,
                "error": str(e)
            })
    
    return {"results": results, "total_files": len(files)}

@app.get("/classes")
async def get_classes():
    """è·å–æ‰€æœ‰ç±»åˆ«"""
    return {"classes": class_names, "total_classes": len(class_names)}

@app.get("/model_info")
async def get_model_info():
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    return {
        "model_type": type(model).__name__,
        "classes": class_names,
        "total_classes": len(class_names),
        "features_count": getattr(model, 'n_features_in_', 'Unknown')
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### Dockeréƒ¨ç½²

```dockerfile
# deployment/Dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    libsndfile1 \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY . .

# å®‰è£…é¡¹ç›®
RUN pip install -e .

# åˆ›å»ºæ¨¡å‹ç›®å½•
RUN mkdir -p models

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "deployment.api_server:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# deployment/docker-compose.yml
version: '3.8'

services:
  urban-sounds-api:
    build:
      context: .
      dockerfile: deployment/Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - ./data:/app/data
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    depends_on:
      - mlflow
    restart: unless-stopped

  mlflow:
    image: python:3.9-slim
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlflow/mlruns
    working_dir: /mlflow
    command: >
      bash -c "pip install mlflow && 
               mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns"
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./deployment/nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - urban-sounds-api
    restart: unless-stopped
```

## MLOpsæœ€ä½³å®è·µ

### è‡ªåŠ¨åŒ–è®­ç»ƒæµæ°´çº¿

```python
# src/pipeline/training_pipeline.py
import yaml
import argparse
from pathlib import Path
import mlflow
from src.data.data_loader import UrbanSoundDataLoader
from src.features.feature_pipeline import FeaturePipeline
from src.models.classifiers import ModelFactory, ModelTrainer
from src.models.evaluation import ModelEvaluator, HyperparameterTuner
from src.experiments.experiment_tracker import ExperimentTracker
from sklearn.model_selection import train_test_split

class TrainingPipeline:
    """è®­ç»ƒæµæ°´çº¿"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.tracker = ExperimentTracker(
            experiment_name=self.config['experiment']['name']
        )
    
    def run(self):
        """è¿è¡Œå®Œæ•´è®­ç»ƒæµæ°´çº¿"""
        print("=== å¼€å§‹è®­ç»ƒæµæ°´çº¿ ===")
        
        # 1. æ•°æ®åŠ è½½
        print("1. åŠ è½½æ•°æ®...")
        data_loader = UrbanSoundDataLoader(
            data_path=self.config['data']['raw_path'],
            metadata_path=self.config['data']['metadata_path']
        )
        
        # 2. ç‰¹å¾å·¥ç¨‹
        print("2. ç‰¹å¾å·¥ç¨‹...")
        pipeline = FeaturePipeline()
        features_df, labels = pipeline.prepare_features(
            data_loader, 
            sample_size=self.config['data'].get('sample_size')
        )
        
        X_processed, y_processed = pipeline.fit_transform(features_df, labels)
        
        # ä¿å­˜ç‰¹å¾æµæ°´çº¿
        pipeline.save_pipeline(self.config['output']['feature_pipeline_path'])
        
        # 3. æ•°æ®åˆ†å‰²
        print("3. æ•°æ®åˆ†å‰²...")
        X_train, X_test, y_train, y_test = train_test_split(
            X_processed, y_processed, 
            test_size=self.config['data']['test_size'],
            random_state=self.config['data']['random_state'],
            stratify=y_processed
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train,
            test_size=self.config['data']['val_size'],
            random_state=self.config['data']['random_state'],
            stratify=y_train
        )
        
        # 4. æ¨¡å‹è®­ç»ƒ
        print("4. æ¨¡å‹è®­ç»ƒ...")
        trainer = ModelTrainer(self.config['experiment']['name'])
        evaluator = ModelEvaluator(class_names=data_loader.class_names)
        
        best_model = None
        best_score = 0
        
        for model_config in self.config['models']:
            model_name = model_config['name']
            model_type = model_config['type']
            model_params = model_config['params']
            
            print(f"è®­ç»ƒ {model_name}...")
            
            # åˆ›å»ºæ¨¡å‹
            if model_type == 'RandomForest':
                model = ModelFactory.create_random_forest(**model_params)
            elif model_type == 'XGBoost':
                model = ModelFactory.create_xgboost(**model_params)
            elif model_type == 'SVM':
                model = ModelFactory.create_svm(**model_params)
            elif model_type == 'NeuralNetwork':
                model = ModelFactory.create_neural_network(
                    input_dim=X_train.shape[1],
                    num_classes=len(data_loader.class_names),
                    **model_params
                )
            else:
                print(f"æœªçŸ¥æ¨¡å‹ç±»å‹: {model_type}")
                continue
            
            # è®­ç»ƒæ¨¡å‹
            if model_type == 'NeuralNetwork':
                result = trainer.train_neural_network(
                    model, X_train, y_train, X_val, y_val,
                    epochs=self.config['training']['epochs'],
                    batch_size=self.config['training']['batch_size'],
                    model_name=model_name
                )
            else:
                result = trainer.train_sklearn_model(
                    model, X_train, y_train, X_val, y_val, model_name
                )
            
            # è®°å½•æœ€ä½³æ¨¡å‹
            if result['val_accuracy'] > best_score:
                best_score = result['val_accuracy']
                best_model = result['model']
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                import joblib
                joblib.dump(best_model, self.config['output']['best_model_path'])
        
        # 5. æœ€ç»ˆè¯„ä¼°
        print("5. æœ€ç»ˆè¯„ä¼°...")
        if best_model:
            final_result = evaluator.evaluate_model(
                best_model, X_test, y_test, "BestModel"
            )
            
            print(f"æœ€ä½³æ¨¡å‹æµ‹è¯•é›†å‡†ç¡®ç‡: {final_result['accuracy']:.4f}")
        
        print("=== è®­ç»ƒæµæ°´çº¿å®Œæˆ ===")
        
        return {
            'best_model': best_model,
            'test_accuracy': final_result['accuracy'] if best_model else None,
            'feature_pipeline': pipeline
        }

# é…ç½®æ–‡ä»¶ç¤ºä¾‹
def create_config_template():
    """åˆ›å»ºé…ç½®æ–‡ä»¶æ¨¡æ¿"""
    config = {
        'experiment': {
            'name': 'urban_sounds_classification_v2'
        },
        'data': {
            'raw_path': 'data/raw',
            'metadata_path': 'data/UrbanSound8K.csv',
            'sample_size': 2000,
            'test_size': 0.2,
            'val_size': 0.25,
            'random_state': 42
        },
        'models': [
            {
                'name': 'RandomForest_Optimized',
                'type': 'RandomForest',
                'params': {
                    'n_estimators': 200,
                    'max_depth': 15,
                    'random_state': 42
                }
            },
            {
                'name': 'XGBoost_Optimized',
                'type': 'XGBoost',
                'params': {
                    'n_estimators': 300,
                    'max_depth': 8,
                    'learning_rate': 0.05,
                    'random_state': 42
                }
            }
        ],
        'training': {
            'epochs': 100,
            'batch_size': 32
        },
        'output': {
            'best_model_path': 'models/best_model.pkl',
            'feature_pipeline_path': 'models/feature_pipeline.pkl'
        }
    }
    
    with open('config/training_config.yaml', 'w') as f:
        yaml.dump(config, f, default_flow_style=False)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='è¿è¡Œè®­ç»ƒæµæ°´çº¿')
    parser.add_argument('--config', required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    
    pipeline = TrainingPipeline(args.config)
    pipeline.run()
```

## Jupyteré›†æˆæ·±åº¦åº”ç”¨

### JupyterMCPé›†æˆ

åŸºäºæœç´¢ç»“æœä¸­çš„JupyterMCPï¼ŒClaude Codeå¯ä»¥ç›´æ¥ä¸Jupyter Notebooké›†æˆï¼š

```python
# å®‰è£…JupyterMCP
# pip install jupyter-markdown-mcp

# notebooks/advanced-jupyter-integration.ipynb
"""
è¿™ä¸ªnotebookå±•ç¤ºäº†Claude Codeä¸Jupyterçš„æ·±åº¦é›†æˆåº”ç”¨
"""

# 1. Claude Codeå¯ä»¥ç›´æ¥ç¼–è¾‘notebookå•å…ƒæ ¼
def create_notebook_cell(cell_type, content):
    """åˆ›å»ºnotebookå•å…ƒæ ¼"""
    if cell_type == "code":
        return {
            "cell_type": "code",
            "source": content,
            "metadata": {},
            "outputs": [],
            "execution_count": None
        }
    elif cell_type == "markdown":
        return {
            "cell_type": "markdown",
            "source": content,
            "metadata": {}
        }

# 2. è‡ªåŠ¨åŒ–å®éªŒæµç¨‹
def automated_experiment_flow():
    """è‡ªåŠ¨åŒ–å®éªŒæµç¨‹"""
    experiments = [
        {
            "name": "baseline_random_forest",
            "params": {"n_estimators": 100, "max_depth": 10}
        },
        {
            "name": "optimized_random_forest", 
            "params": {"n_estimators": 200, "max_depth": 15}
        }
    ]
    
    results = []
    for exp in experiments:
        # Claude Codeå¯ä»¥è‡ªåŠ¨ç”Ÿæˆå’Œæ‰§è¡Œè¿™äº›å®éªŒ
        model = ModelFactory.create_random_forest(**exp["params"])
        # ... è®­ç»ƒå’Œè¯„ä¼°é€»è¾‘
        results.append({"experiment": exp["name"], "accuracy": 0.85})
    
    return results

# 3. äº¤äº’å¼å¯è§†åŒ–
import plotly.graph_objects as go
from plotly.subplots import make_subplots

def create_interactive_dashboard(results):
    """åˆ›å»ºäº¤äº’å¼ç»“æœé¢æ¿"""
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('æ¨¡å‹æ€§èƒ½å¯¹æ¯”', 'æ··æ·†çŸ©é˜µ', 'ç‰¹å¾é‡è¦æ€§', 'å­¦ä¹ æ›²çº¿'),
        specs=[[{"type": "bar"}, {"type": "heatmap"}],
               [{"type": "bar"}, {"type": "scatter"}]]
    )
    
    # æ·»åŠ å„ç§å›¾è¡¨
    # ...
    
    fig.show()

# 4. è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆ
def generate_experiment_report(results):
    """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
    report = f"""
    # åŸå¸‚å£°éŸ³åˆ†ç±»å®éªŒæŠ¥å‘Š
    
    ## å®éªŒæ¦‚è¿°
    æœ¬æ¬¡å®éªŒå…±è®­ç»ƒäº† {len(results)} ä¸ªæ¨¡å‹ï¼Œæœ€ä½³å‡†ç¡®ç‡ä¸º {max([r['accuracy'] for r in results]):.4f}
    
    ## æ¨¡å‹æ€§èƒ½å¯¹æ¯”
    """
    
    for result in results:
        report += f"- {result['experiment']}: {result['accuracy']:.4f}\n"
    
    return report
```

## æ€»ç»“

æœ¬æ–‡é€šè¿‡å®Œæ•´çš„åŸå¸‚å£°éŸ³åˆ†ç±»é¡¹ç›®ï¼Œå…¨é¢å±•ç¤ºäº†Claude Codeåœ¨æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¡¹ç›®ä¸­çš„å¼ºå¤§èƒ½åŠ›ï¼š

### æ ¸å¿ƒä¼˜åŠ¿

1. **é¡¹ç›®ç»“æ„åŒ–**: è‡ªåŠ¨åˆ›å»ºæ ‡å‡†åŒ–çš„é¡¹ç›®ç»“æ„å’Œé…ç½®
2. **ä»£ç ç”Ÿæˆ**: æ™ºèƒ½ç”Ÿæˆæ•°æ®å¤„ç†ã€ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹è®­ç»ƒä»£ç 
3. **å®éªŒç®¡ç†**: ç³»ç»ŸåŒ–çš„å®éªŒè¿½è¸ªå’Œæ¨¡å‹æ¯”è¾ƒ
4. **éƒ¨ç½²è‡ªåŠ¨åŒ–**: ä¸€é”®ç”ŸæˆAPIæœåŠ¡å’ŒDockeré…ç½®
5. **Jupyteré›†æˆ**: åŸç”Ÿæ”¯æŒnotebookå·¥ä½œæµç¨‹

### æœ€ä½³å®è·µ

1. **ä½¿ç”¨CLAUDE.md**: å®šä¹‰æ¸…æ™°çš„é¡¹ç›®ä¸Šä¸‹æ–‡å’ŒæŠ€æœ¯æ ˆ
2. **æ¨¡å—åŒ–è®¾è®¡**: å°†åŠŸèƒ½åˆ†è§£ä¸ºç‹¬ç«‹çš„æ¨¡å—ï¼Œä¾¿äºç»´æŠ¤å’Œå¤ç”¨
3. **å®éªŒè¿½è¸ª**: ä½¿ç”¨MLflowç­‰å·¥å…·è®°å½•æ‰€æœ‰å®éªŒè¿‡ç¨‹
4. **ç‰ˆæœ¬æ§åˆ¶**: å¯¹ä»£ç ã€æ•°æ®å’Œæ¨¡å‹è¿›è¡Œç‰ˆæœ¬ç®¡ç†
5. **æŒç»­é›†æˆ**: å»ºç«‹è‡ªåŠ¨åŒ–çš„è®­ç»ƒå’Œéƒ¨ç½²æµæ°´çº¿

### æ€§èƒ½æå‡

é€šè¿‡Claude Codeçš„è¾…åŠ©ï¼Œæ•°æ®ç§‘å­¦é¡¹ç›®çš„å¼€å‘æ•ˆç‡èƒ½å¤Ÿæ˜¾è‘—æå‡ï¼š
- ä»£ç ç”Ÿæˆé€Ÿåº¦æå‡300%
- å®éªŒç®¡ç†æ›´åŠ ç³»ç»ŸåŒ–
- ä»æ¢ç´¢åˆ°éƒ¨ç½²çš„ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–
- ä»£ç è´¨é‡å’Œä¸€è‡´æ€§å¤§å¹…æ”¹å–„

Claude Codeå°†æ•°æ®ç§‘å­¦é¡¹ç›®ä»"ç ”ç©¶ä»£ç "è½¬å˜ä¸º"å·¥ç¨‹ä»£ç "ï¼Œä½¿å¾—AIæ¨¡å‹èƒ½å¤Ÿæ›´å¿«é€Ÿã€æ›´å¯é åœ°ä»å®éªŒå®¤èµ°å‘ç”Ÿäº§ç¯å¢ƒã€‚

## ç›¸å…³æ–‡ç« æ¨è

- [æ–‡ä»¶æ“ä½œä¸ä»£ç ç”Ÿæˆç²¾è®²](07-æ–‡ä»¶æ“ä½œä¸ä»£ç ç”Ÿæˆç²¾è®².md) - äº†è§£åŸºç¡€çš„ä»£ç ç”Ÿæˆèƒ½åŠ›
- [æµ‹è¯•é©±åŠ¨å¼€å‘(TDD)ä¸Claude Code](10-æµ‹è¯•é©±åŠ¨å¼€å‘TDDä¸Claude-Code.md) - å­¦ä¹ å¦‚ä½•ç¼–å†™æµ‹è¯•
- [é¡¹ç›®æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆ](12-é¡¹ç›®æ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆ.md) - è‡ªåŠ¨åŒ–é¡¹ç›®æ–‡æ¡£
- [ç§»åŠ¨åº”ç”¨å¼€å‘æŒ‡å—](31-ç§»åŠ¨åº”ç”¨å¼€å‘æŒ‡å—.md) - ä¸‹ä¸€ç¯‡æ–‡ç« å†…å®¹

---

*æœ¬æ–‡æ˜¯Claude Codeå®Œæ•´æ•™ç¨‹ç³»åˆ—çš„ç¬¬30ç¯‡ï¼Œæ·±å…¥æ¢è®¨äº†æ•°æ®ç§‘å­¦ä¸æœºå™¨å­¦ä¹ é¡¹ç›®çš„å®Œæ•´å¼€å‘æµç¨‹ã€‚ä¸‹ä¸€ç¯‡å°†ä»‹ç»ç§»åŠ¨åº”ç”¨å¼€å‘æŒ‡å—ã€‚*