# Claude Code 教程系列 36：性能优化与调试技巧

## 引言

在软件开发过程中，性能优化和调试是确保应用程序高效运行的关键环节。Claude Code 不仅能够帮助我们快速开发功能，更重要的是能够协助我们识别性能瓶颈、优化代码效率，并提供强大的调试支持。本文将深入探讨如何利用 Claude Code 进行系统化的性能优化和高效调试。

## 目录
1. [性能优化基础理论](#性能优化基础理论)
2. [Claude Code性能最佳实践](#claude-code性能最佳实践)
3. [代码性能分析与优化](#代码性能分析与优化)
4. [内存管理与资源优化](#内存管理与资源优化)
5. [调试技巧与工具集成](#调试技巧与工具集成)
6. [性能监控与测试](#性能监控与测试)
7. [生产环境优化策略](#生产环境优化策略)
8. [实战案例分析](#实战案例分析)

## 性能优化基础理论

### 1.1 性能指标体系

让我们首先建立一个完整的性能监控框架，使用 Claude Code 来设计和实现：

```python
# performance/metrics_collector.py
import time
import psutil
import asyncio
import logging
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import deque
import threading
from contextlib import contextmanager
import json

@dataclass
class PerformanceMetric:
    """性能指标数据类"""
    timestamp: datetime
    cpu_percent: float
    memory_mb: float
    memory_percent: float
    disk_io_read: int
    disk_io_write: int
    network_sent: int
    network_recv: int
    response_time: float = 0.0
    throughput: float = 0.0
    error_count: int = 0
    active_connections: int = 0

@dataclass
class FunctionMetrics:
    """函数性能指标"""
    function_name: str
    call_count: int = 0
    total_time: float = 0.0
    avg_time: float = 0.0
    min_time: float = float('inf')
    max_time: float = 0.0
    error_count: int = 0
    last_called: Optional[datetime] = None

class PerformanceCollector:
    """性能数据收集器"""
    
    def __init__(self, collection_interval: float = 1.0, max_history: int = 1000):
        self.collection_interval = collection_interval
        self.max_history = max_history
        self.metrics_history: deque = deque(maxlen=max_history)
        self.function_metrics: Dict[str, FunctionMetrics] = {}
        self.is_collecting = False
        self.collection_thread: Optional[threading.Thread] = None
        self.logger = logging.getLogger(__name__)
        
        # 初始化系统信息
        self.process = psutil.Process()
        self.initial_net_io = psutil.net_io_counters()
        self.initial_disk_io = psutil.disk_io_counters()
    
    def start_collection(self):
        """开始收集性能数据"""
        if self.is_collecting:
            return
        
        self.is_collecting = True
        self.collection_thread = threading.Thread(target=self._collect_loop, daemon=True)
        self.collection_thread.start()
        self.logger.info("性能数据收集已启动")
    
    def stop_collection(self):
        """停止收集性能数据"""
        self.is_collecting = False
        if self.collection_thread:
            self.collection_thread.join()
        self.logger.info("性能数据收集已停止")
    
    def _collect_loop(self):
        """收集数据的主循环"""
        while self.is_collecting:
            try:
                metric = self._collect_current_metrics()
                self.metrics_history.append(metric)
                time.sleep(self.collection_interval)
            except Exception as e:
                self.logger.error(f"收集性能数据时出错: {e}")
    
    def _collect_current_metrics(self) -> PerformanceMetric:
        """收集当前性能指标"""
        try:
            # CPU和内存
            cpu_percent = self.process.cpu_percent()
            memory_info = self.process.memory_info()
            memory_mb = memory_info.rss / 1024 / 1024
            memory_percent = self.process.memory_percent()
            
            # 磁盘I/O
            current_disk_io = psutil.disk_io_counters()
            disk_read = current_disk_io.read_bytes - self.initial_disk_io.read_bytes
            disk_write = current_disk_io.write_bytes - self.initial_disk_io.write_bytes
            
            # 网络I/O
            current_net_io = psutil.net_io_counters()
            net_sent = current_net_io.bytes_sent - self.initial_net_io.bytes_sent
            net_recv = current_net_io.bytes_recv - self.initial_net_io.bytes_recv
            
            return PerformanceMetric(
                timestamp=datetime.now(),
                cpu_percent=cpu_percent,
                memory_mb=memory_mb,
                memory_percent=memory_percent,
                disk_io_read=disk_read,
                disk_io_write=disk_write,
                network_sent=net_sent,
                network_recv=net_recv
            )
        
        except Exception as e:
            self.logger.error(f"收集系统指标时出错: {e}")
            return PerformanceMetric(
                timestamp=datetime.now(),
                cpu_percent=0, memory_mb=0, memory_percent=0,
                disk_io_read=0, disk_io_write=0,
                network_sent=0, network_recv=0
            )
    
    @contextmanager
    def measure_function(self, function_name: str):
        """函数性能测量上下文管理器"""
        start_time = time.time()
        error_occurred = False
        
        try:
            yield
        except Exception as e:
            error_occurred = True
            raise
        finally:
            end_time = time.time()
            execution_time = end_time - start_time
            self._record_function_metrics(function_name, execution_time, error_occurred)
    
    def _record_function_metrics(self, function_name: str, execution_time: float, error: bool):
        """记录函数性能指标"""
        if function_name not in self.function_metrics:
            self.function_metrics[function_name] = FunctionMetrics(function_name=function_name)
        
        metrics = self.function_metrics[function_name]
        metrics.call_count += 1
        metrics.total_time += execution_time
        metrics.avg_time = metrics.total_time / metrics.call_count
        metrics.min_time = min(metrics.min_time, execution_time)
        metrics.max_time = max(metrics.max_time, execution_time)
        metrics.last_called = datetime.now()
        
        if error:
            metrics.error_count += 1
    
    def get_current_metrics(self) -> Optional[PerformanceMetric]:
        """获取最新的性能指标"""
        return self.metrics_history[-1] if self.metrics_history else None
    
    def get_metrics_history(self, minutes: int = 10) -> List[PerformanceMetric]:
        """获取指定时间范围内的性能历史"""
        cutoff_time = datetime.now() - timedelta(minutes=minutes)
        return [m for m in self.metrics_history if m.timestamp > cutoff_time]
    
    def get_function_metrics(self) -> Dict[str, FunctionMetrics]:
        """获取函数性能统计"""
        return self.function_metrics.copy()
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """获取性能摘要"""
        if not self.metrics_history:
            return {"error": "没有可用的性能数据"}
        
        recent_metrics = list(self.metrics_history)[-100:]  # 最近100个数据点
        
        return {
            "timestamp": datetime.now().isoformat(),
            "system_metrics": {
                "avg_cpu_percent": sum(m.cpu_percent for m in recent_metrics) / len(recent_metrics),
                "avg_memory_mb": sum(m.memory_mb for m in recent_metrics) / len(recent_metrics),
                "max_memory_mb": max(m.memory_mb for m in recent_metrics),
                "avg_memory_percent": sum(m.memory_percent for m in recent_metrics) / len(recent_metrics)
            },
            "function_metrics": {
                name: {
                    "call_count": metrics.call_count,
                    "avg_time": metrics.avg_time,
                    "min_time": metrics.min_time,
                    "max_time": metrics.max_time,
                    "error_rate": metrics.error_count / max(metrics.call_count, 1),
                    "last_called": metrics.last_called.isoformat() if metrics.last_called else None
                }
                for name, metrics in self.function_metrics.items()
            },
            "data_points": len(recent_metrics)
        }

# 性能分析器装饰器
def performance_monitor(collector: PerformanceCollector):
    """性能监控装饰器"""
    def decorator(func: Callable):
        def wrapper(*args, **kwargs):
            with collector.measure_function(func.__name__):
                return func(*args, **kwargs)
        return wrapper
    return decorator

async def async_performance_monitor(collector: PerformanceCollector):
    """异步性能监控装饰器"""
    def decorator(func: Callable):
        async def wrapper(*args, **kwargs):
            with collector.measure_function(func.__name__):
                return await func(*args, **kwargs)
        return wrapper
    return decorator

# 使用示例
perf_collector = PerformanceCollector()

@performance_monitor(perf_collector)
def cpu_intensive_task(n: int) -> int:
    """CPU密集型任务示例"""
    total = 0
    for i in range(n):
        total += i ** 2
    return total

@performance_monitor(perf_collector)
def memory_intensive_task(size: int) -> List[int]:
    """内存密集型任务示例"""
    return [i for i in range(size)]

# 启动性能监控
def start_performance_monitoring():
    """启动性能监控"""
    perf_collector.start_collection()
    
    # 运行示例任务
    cpu_intensive_task(1000000)
    memory_intensive_task(100000)
    
    # 获取性能报告
    summary = perf_collector.get_performance_summary()
    print(json.dumps(summary, indent=2, ensure_ascii=False, default=str))
    
    return perf_collector

# 性能瓶颈分析器
class BottleneckAnalyzer:
    """性能瓶颈分析器"""
    
    def __init__(self, collector: PerformanceCollector):
        self.collector = collector
        self.logger = logging.getLogger(__name__)
    
    def analyze_cpu_usage(self, threshold: float = 80.0) -> Dict[str, Any]:
        """分析CPU使用率"""
        metrics = self.collector.get_metrics_history()
        if not metrics:
            return {"error": "没有可用的CPU数据"}
        
        high_cpu_periods = [m for m in metrics if m.cpu_percent > threshold]
        avg_cpu = sum(m.cpu_percent for m in metrics) / len(metrics)
        
        return {
            "average_cpu": avg_cpu,
            "high_cpu_periods": len(high_cpu_periods),
            "high_cpu_percentage": len(high_cpu_periods) / len(metrics) * 100,
            "recommendation": self._get_cpu_recommendation(avg_cpu, len(high_cpu_periods))
        }
    
    def analyze_memory_usage(self, threshold: float = 80.0) -> Dict[str, Any]:
        """分析内存使用情况"""
        metrics = self.collector.get_metrics_history()
        if not metrics:
            return {"error": "没有可用的内存数据"}
        
        high_memory_periods = [m for m in metrics if m.memory_percent > threshold]
        avg_memory = sum(m.memory_percent for m in metrics) / len(metrics)
        peak_memory = max(m.memory_mb for m in metrics)
        
        return {
            "average_memory_percent": avg_memory,
            "peak_memory_mb": peak_memory,
            "high_memory_periods": len(high_memory_periods),
            "memory_growth_trend": self._analyze_memory_trend(metrics),
            "recommendation": self._get_memory_recommendation(avg_memory, peak_memory)
        }
    
    def analyze_function_performance(self, min_calls: int = 10) -> Dict[str, Any]:
        """分析函数性能"""
        function_metrics = self.collector.get_function_metrics()
        
        # 过滤调用次数少的函数
        significant_functions = {
            name: metrics for name, metrics in function_metrics.items()
            if metrics.call_count >= min_calls
        }
        
        if not significant_functions:
            return {"error": "没有足够的函数调用数据"}
        
        # 识别慢函数
        slow_functions = sorted(
            significant_functions.items(),
            key=lambda x: x[1].avg_time,
            reverse=True
        )[:5]
        
        # 识别高频函数
        frequent_functions = sorted(
            significant_functions.items(),
            key=lambda x: x[1].call_count,
            reverse=True
        )[:5]
        
        return {
            "total_functions": len(significant_functions),
            "slowest_functions": [
                {
                    "name": name,
                    "avg_time": metrics.avg_time,
                    "call_count": metrics.call_count,
                    "total_time": metrics.total_time
                }
                for name, metrics in slow_functions
            ],
            "most_frequent_functions": [
                {
                    "name": name,
                    "call_count": metrics.call_count,
                    "avg_time": metrics.avg_time,
                    "total_time": metrics.total_time
                }
                for name, metrics in frequent_functions
            ],
            "recommendations": self._get_function_recommendations(slow_functions, frequent_functions)
        }
    
    def _analyze_memory_trend(self, metrics: List[PerformanceMetric]) -> str:
        """分析内存增长趋势"""
        if len(metrics) < 10:
            return "数据不足"
        
        first_half = metrics[:len(metrics)//2]
        second_half = metrics[len(metrics)//2:]
        
        avg_first = sum(m.memory_mb for m in first_half) / len(first_half)
        avg_second = sum(m.memory_mb for m in second_half) / len(second_half)
        
        growth_rate = (avg_second - avg_first) / avg_first * 100
        
        if growth_rate > 10:
            return "内存持续增长，可能存在内存泄漏"
        elif growth_rate > 5:
            return "内存轻微增长"
        elif growth_rate < -5:
            return "内存使用下降"
        else:
            return "内存使用稳定"
    
    def _get_cpu_recommendation(self, avg_cpu: float, high_periods: int) -> str:
        """获取CPU优化建议"""
        if avg_cpu > 80:
            return "CPU使用率过高，建议优化算法或增加计算资源"
        elif avg_cpu > 60:
            return "CPU使用率较高，考虑代码优化或负载分散"
        elif high_periods > 0:
            return "存在CPU峰值，检查是否有CPU密集型操作"
        else:
            return "CPU使用情况正常"
    
    def _get_memory_recommendation(self, avg_memory: float, peak_memory: float) -> str:
        """获取内存优化建议"""
        if peak_memory > 1000:  # 1GB
            return "内存使用量较大，考虑优化数据结构或实现分页"
        elif avg_memory > 80:
            return "内存使用率过高，检查是否存在内存泄漏"
        elif avg_memory > 60:
            return "内存使用率较高，考虑优化内存使用"
        else:
            return "内存使用情况正常"
    
    def _get_function_recommendations(self, slow_functions: List, frequent_functions: List) -> List[str]:
        """获取函数优化建议"""
        recommendations = []
        
        if slow_functions:
            slowest_name = slow_functions[0][0]
            slowest_time = slow_functions[0][1].avg_time
            recommendations.append(f"函数 {slowest_name} 执行时间较长({slowest_time:.4f}s)，需要优化")
        
        if frequent_functions:
            most_frequent = frequent_functions[0]
            if most_frequent[1].total_time > 1.0:  # 总时间超过1秒
                recommendations.append(f"函数 {most_frequent[0]} 调用频率高且总耗时长，建议缓存或优化")
        
        return recommendations or ["函数性能表现良好"]

# 使用示例
def demo_performance_analysis():
    """演示性能分析"""
    collector = start_performance_monitoring()
    time.sleep(2)  # 等待收集一些数据
    
    analyzer = BottleneckAnalyzer(collector)
    
    # 分析CPU
    cpu_analysis = analyzer.analyze_cpu_usage()
    print("CPU分析:", json.dumps(cpu_analysis, indent=2, ensure_ascii=False))
    
    # 分析内存
    memory_analysis = analyzer.analyze_memory_usage()
    print("内存分析:", json.dumps(memory_analysis, indent=2, ensure_ascii=False))
    
    # 分析函数性能
    function_analysis = analyzer.analyze_function_performance(min_calls=1)
    print("函数分析:", json.dumps(function_analysis, indent=2, ensure_ascii=False))
    
    collector.stop_collection()

# 运行演示
# demo_performance_analysis()
```

## Claude Code性能最佳实践

### 2.1 上下文管理优化

Claude Code 的性能很大程度上取决于有效的上下文管理。让我们实现一个智能的上下文管理系统：

```python
# claude_optimization/context_manager.py
import os
import time
import hashlib
import pickle
from typing import Dict, List, Optional, Set, Any
from pathlib import Path
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import logging
import json

@dataclass
class ContextItem:
    """上下文项目"""
    content: str
    file_path: Optional[str] = None
    importance: float = 1.0
    last_accessed: datetime = field(default_factory=datetime.now)
    access_count: int = 0
    token_count: int = 0
    category: str = "general"

@dataclass
class ContextSession:
    """上下文会话"""
    session_id: str
    items: List[ContextItem] = field(default_factory=list)
    max_tokens: int = 100000
    current_tokens: int = 0
    created_at: datetime = field(default_factory=datetime.now)
    last_optimized: datetime = field(default_factory=datetime.now)

class ClaudeContextOptimizer:
    """Claude上下文优化器"""
    
    def __init__(self, workspace_path: str, max_sessions: int = 10):
        self.workspace_path = Path(workspace_path)
        self.max_sessions = max_sessions
        self.sessions: Dict[str, ContextSession] = {}
        self.file_cache: Dict[str, str] = {}
        self.index_cache: Dict[str, Set[str]] = {}
        self.logger = logging.getLogger(__name__)
        
        # 创建缓存目录
        self.cache_dir = self.workspace_path / '.claude_cache'
        self.cache_dir.mkdir(exist_ok=True)
    
    def create_session(self, session_id: Optional[str] = None) -> str:
        """创建新的上下文会话"""
        if session_id is None:
            session_id = self._generate_session_id()
        
        self.sessions[session_id] = ContextSession(session_id=session_id)
        self.logger.info(f"创建上下文会话: {session_id}")
        
        # 清理旧会话
        self._cleanup_old_sessions()
        
        return session_id
    
    def add_file_to_context(self, session_id: str, file_path: str, 
                           importance: float = 1.0, category: str = "code") -> bool:
        """添加文件到上下文"""
        if session_id not in self.sessions:
            self.logger.error(f"会话不存在: {session_id}")
            return False
        
        file_path_obj = Path(file_path)
        if not file_path_obj.exists():
            self.logger.error(f"文件不存在: {file_path}")
            return False
        
        # 检查文件是否已缓存
        content = self._get_file_content(file_path)
        if content is None:
            return False
        
        # 估算token数量
        token_count = self._estimate_tokens(content)
        
        # 检查是否会超出token限制
        session = self.sessions[session_id]
        if session.current_tokens + token_count > session.max_tokens:
            self.logger.warning(f"添加文件 {file_path} 将超出token限制，尝试优化上下文")
            self._optimize_context(session_id)
            
            # 再次检查
            session = self.sessions[session_id]
            if session.current_tokens + token_count > session.max_tokens:
                self.logger.error(f"优化后仍无法添加文件: {file_path}")
                return False
        
        # 创建上下文项目
        item = ContextItem(
            content=content,
            file_path=file_path,
            importance=importance,
            token_count=token_count,
            category=category
        )
        
        session.items.append(item)
        session.current_tokens += token_count
        
        self.logger.info(f"添加文件到上下文: {file_path} (tokens: {token_count})")
        return True
    
    def add_text_to_context(self, session_id: str, text: str, 
                           importance: float = 1.0, category: str = "text") -> bool:
        """添加文本到上下文"""
        if session_id not in self.sessions:
            return False
        
        token_count = self._estimate_tokens(text)
        session = self.sessions[session_id]
        
        if session.current_tokens + token_count > session.max_tokens:
            self._optimize_context(session_id)
            session = self.sessions[session_id]
            if session.current_tokens + token_count > session.max_tokens:
                self.logger.error("优化后仍无法添加文本")
                return False
        
        item = ContextItem(
            content=text,
            importance=importance,
            token_count=token_count,
            category=category
        )
        
        session.items.append(item)
        session.current_tokens += token_count
        
        return True
    
    def _optimize_context(self, session_id: str) -> None:
        """优化上下文，移除不重要的项目"""
        if session_id not in self.sessions:
            return
        
        session = self.sessions[session_id]
        original_token_count = session.current_tokens
        
        # 按重要性和访问时间排序
        items_by_priority = sorted(
            session.items,
            key=lambda x: (x.importance, x.access_count, x.last_accessed),
            reverse=True
        )
        
        # 保留重要项目，移除不重要的
        target_tokens = int(session.max_tokens * 0.7)  # 保留70%的空间
        current_tokens = 0
        kept_items = []
        
        for item in items_by_priority:
            if current_tokens + item.token_count <= target_tokens:
                kept_items.append(item)
                current_tokens += item.token_count
            else:
                self.logger.debug(f"移除上下文项目: {item.file_path or 'text'}")
        
        session.items = kept_items
        session.current_tokens = current_tokens
        session.last_optimized = datetime.now()
        
        removed_tokens = original_token_count - current_tokens
        self.logger.info(f"上下文优化完成: 移除 {removed_tokens} tokens")
    
    def get_context_summary(self, session_id: str) -> Dict[str, Any]:
        """获取上下文摘要"""
        if session_id not in self.sessions:
            return {}
        
        session = self.sessions[session_id]
        
        # 按类别统计
        category_stats = {}
        for item in session.items:
            if item.category not in category_stats:
                category_stats[item.category] = {
                    'count': 0,
                    'tokens': 0,
                    'avg_importance': 0
                }
            
            stats = category_stats[item.category]
            stats['count'] += 1
            stats['tokens'] += item.token_count
            stats['avg_importance'] += item.importance
        
        # 计算平均重要性
        for stats in category_stats.values():
            stats['avg_importance'] /= stats['count']
        
        return {
            'session_id': session_id,
            'total_items': len(session.items),
            'total_tokens': session.current_tokens,
            'max_tokens': session.max_tokens,
            'utilization': session.current_tokens / session.max_tokens * 100,
            'categories': category_stats,
            'last_optimized': session.last_optimized.isoformat(),
            'needs_optimization': session.current_tokens > session.max_tokens * 0.9
        }
    
    def search_context(self, session_id: str, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """在上下文中搜索相关内容"""
        if session_id not in self.sessions:
            return []
        
        session = self.sessions[session_id]
        query_lower = query.lower()
        
        # 计算相关性得分
        results = []
        for i, item in enumerate(session.items):
            content_lower = item.content.lower()
            
            # 简单的相关性计算
            score = 0
            query_words = query_lower.split()
            for word in query_words:
                if word in content_lower:
                    score += content_lower.count(word)
            
            if score > 0:
                # 更新访问统计
                item.access_count += 1
                item.last_accessed = datetime.now()
                
                results.append({
                    'index': i,
                    'file_path': item.file_path,
                    'category': item.category,
                    'importance': item.importance,
                    'relevance_score': score,
                    'preview': item.content[:200] + "..." if len(item.content) > 200 else item.content
                })
        
        # 按相关性排序
        results.sort(key=lambda x: x['relevance_score'], reverse=True)
        return results[:max_results]
    
    def _get_file_content(self, file_path: str) -> Optional[str]:
        """获取文件内容（带缓存）"""
        file_path_obj = Path(file_path)
        
        # 检查文件修改时间
        mtime = file_path_obj.stat().st_mtime
        cache_key = f"{file_path}:{mtime}"
        
        if cache_key in self.file_cache:
            return self.file_cache[cache_key]
        
        try:
            # 读取文件
            with open(file_path_obj, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # 缓存内容
            self.file_cache[cache_key] = content
            
            # 清理旧缓存
            if len(self.file_cache) > 100:
                self._cleanup_file_cache()
            
            return content
        
        except Exception as e:
            self.logger.error(f"读取文件失败 {file_path}: {e}")
            return None
    
    def _estimate_tokens(self, text: str) -> int:
        """估算文本的token数量"""
        # 简单估算：英文约4字符=1token，中文约2字符=1token
        chinese_chars = sum(1 for c in text if ord(c) > 127)
        english_chars = len(text) - chinese_chars
        
        estimated_tokens = (english_chars // 4) + (chinese_chars // 2)
        return max(estimated_tokens, len(text.split()) // 3)  # 取较大值作为保守估计
    
    def _generate_session_id(self) -> str:
        """生成会话ID"""
        timestamp = datetime.now().isoformat()
        return hashlib.md5(timestamp.encode()).hexdigest()[:12]
    
    def _cleanup_old_sessions(self) -> None:
        """清理旧会话"""
        if len(self.sessions) <= self.max_sessions:
            return
        
        # 按创建时间排序，保留最新的
        sorted_sessions = sorted(
            self.sessions.items(),
            key=lambda x: x[1].created_at,
            reverse=True
        )
        
        # 保留最新的会话
        sessions_to_keep = dict(sorted_sessions[:self.max_sessions])
        self.sessions = sessions_to_keep
        
        self.logger.info(f"清理旧会话，保留 {len(sessions_to_keep)} 个会话")
    
    def _cleanup_file_cache(self) -> None:
        """清理文件缓存"""
        # 保留最近使用的50个文件
        self.file_cache = dict(list(self.file_cache.items())[-50:])
    
    def export_context(self, session_id: str, output_path: str) -> bool:
        """导出上下文到文件"""
        if session_id not in self.sessions:
            return False
        
        session = self.sessions[session_id]
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(f"# Claude Context Export - Session: {session_id}\n")
                f.write(f"# Generated at: {datetime.now().isoformat()}\n")
                f.write(f"# Total tokens: {session.current_tokens}\n\n")
                
                for i, item in enumerate(session.items):
                    f.write(f"## Item {i+1}: {item.category}\n")
                    if item.file_path:
                        f.write(f"**File:** {item.file_path}\n")
                    f.write(f"**Importance:** {item.importance}\n")
                    f.write(f"**Tokens:** {item.token_count}\n\n")
                    f.write("```\n")
                    f.write(item.content)
                    f.write("\n```\n\n")
            
            self.logger.info(f"上下文已导出到: {output_path}")
            return True
        
        except Exception as e:
            self.logger.error(f"导出上下文失败: {e}")
            return False

# 智能文件索引器
class ProjectIndexer:
    """项目文件智能索引器"""
    
    def __init__(self, project_path: str):
        self.project_path = Path(project_path)
        self.index: Dict[str, Dict[str, Any]] = {}
        self.ignore_patterns = {
            '.git', '__pycache__', 'node_modules', '.vscode',
            '*.pyc', '*.log', '*.tmp', '.DS_Store'
        }
        self.logger = logging.getLogger(__name__)
    
    def build_index(self) -> None:
        """构建项目索引"""
        self.logger.info(f"开始构建项目索引: {self.project_path}")
        
        for file_path in self._get_project_files():
            try:
                self._index_file(file_path)
            except Exception as e:
                self.logger.error(f"索引文件失败 {file_path}: {e}")
        
        self.logger.info(f"项目索引构建完成，共索引 {len(self.index)} 个文件")
    
    def _get_project_files(self):
        """获取项目中的所有文件"""
        for file_path in self.project_path.rglob('*'):
            if file_path.is_file() and not self._should_ignore(file_path):
                yield file_path
    
    def _should_ignore(self, file_path: Path) -> bool:
        """检查文件是否应该被忽略"""
        path_str = str(file_path.relative_to(self.project_path))
        
        for pattern in self.ignore_patterns:
            if pattern in path_str or file_path.name == pattern:
                return True
            
            # 简单通配符匹配
            if pattern.startswith('*.') and file_path.suffix == pattern[1:]:
                return True
        
        return False
    
    def _index_file(self, file_path: Path) -> None:
        """索引单个文件"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # 提取关键信息
            relative_path = str(file_path.relative_to(self.project_path))
            
            self.index[relative_path] = {
                'full_path': str(file_path),
                'size': file_path.stat().st_size,
                'modified': file_path.stat().st_mtime,
                'extension': file_path.suffix,
                'lines': len(content.splitlines()),
                'functions': self._extract_functions(content, file_path.suffix),
                'classes': self._extract_classes(content, file_path.suffix),
                'imports': self._extract_imports(content, file_path.suffix),
                'keywords': self._extract_keywords(content)
            }
        
        except Exception as e:
            self.logger.error(f"索引文件失败 {file_path}: {e}")
    
    def _extract_functions(self, content: str, extension: str) -> List[str]:
        """提取函数名"""
        functions = []
        lines = content.splitlines()
        
        if extension == '.py':
            for line in lines:
                stripped = line.strip()
                if stripped.startswith('def ') and '(' in stripped:
                    func_name = stripped[4:stripped.index('(')].strip()
                    functions.append(func_name)
        
        elif extension in ['.js', '.ts']:
            for line in lines:
                stripped = line.strip()
                if 'function ' in stripped and '(' in stripped:
                    # 简单提取，实际情况更复杂
                    parts = stripped.split('function ')
                    if len(parts) > 1:
                        func_part = parts[1]
                        if '(' in func_part:
                            func_name = func_part[:func_part.index('(')].strip()
                            if func_name:
                                functions.append(func_name)
        
        return functions
    
    def _extract_classes(self, content: str, extension: str) -> List[str]:
        """提取类名"""
        classes = []
        lines = content.splitlines()
        
        if extension == '.py':
            for line in lines:
                stripped = line.strip()
                if stripped.startswith('class ') and ':' in stripped:
                    class_part = stripped[6:stripped.index(':')].strip()
                    # 去除继承部分
                    if '(' in class_part:
                        class_part = class_part[:class_part.index('(')]
                    classes.append(class_part.strip())
        
        elif extension in ['.js', '.ts']:
            for line in lines:
                stripped = line.strip()
                if stripped.startswith('class ') and '{' in content:
                    class_name = stripped[6:].split()[0]
                    classes.append(class_name)
        
        return classes
    
    def _extract_imports(self, content: str, extension: str) -> List[str]:
        """提取导入的模块"""
        imports = []
        lines = content.splitlines()
        
        if extension == '.py':
            for line in lines:
                stripped = line.strip()
                if stripped.startswith('import '):
                    module = stripped[7:].split()[0].split('.')[0]
                    imports.append(module)
                elif stripped.startswith('from '):
                    if ' import ' in stripped:
                        module = stripped[5:stripped.index(' import ')].split('.')[0]
                        imports.append(module)
        
        elif extension in ['.js', '.ts']:
            for line in lines:
                stripped = line.strip()
                if 'require(' in stripped or 'import ' in stripped:
                    # 简化处理
                    if 'require(' in stripped:
                        start = stripped.find("require('") + 9
                        if start > 8:
                            end = stripped.find("'", start)
                            if end > start:
                                imports.append(stripped[start:end])
        
        return list(set(imports))  # 去重
    
    def _extract_keywords(self, content: str) -> List[str]:
        """提取关键词"""
        # 简单的关键词提取
        words = content.lower().split()
        
        # 过滤常见词汇
        common_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have',
            'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'
        }
        
        keywords = []
        for word in words:
            # 清理单词
            clean_word = ''.join(c for c in word if c.isalnum())
            if len(clean_word) > 3 and clean_word not in common_words:
                keywords.append(clean_word)
        
        # 返回最常见的关键词
        from collections import Counter
        word_counts = Counter(keywords)
        return [word for word, count in word_counts.most_common(20)]
    
    def search_files(self, query: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """搜索文件"""
        query_lower = query.lower()
        results = []
        
        for file_path, info in self.index.items():
            score = 0
            
            # 文件名匹配
            if query_lower in file_path.lower():
                score += 5
            
            # 函数名匹配
            for func in info.get('functions', []):
                if query_lower in func.lower():
                    score += 3
            
            # 类名匹配
            for cls in info.get('classes', []):
                if query_lower in cls.lower():
                    score += 3
            
            # 关键词匹配
            for keyword in info.get('keywords', []):
                if query_lower in keyword.lower():
                    score += 1
            
            if score > 0:
                results.append({
                    'file_path': file_path,
                    'score': score,
                    'info': info
                })
        
        # 按分数排序
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:max_results]
    
    def get_file_recommendations(self, current_file: str, max_results: int = 5) -> List[str]:
        """基于当前文件推荐相关文件"""
        if current_file not in self.index:
            return []
        
        current_info = self.index[current_file]
        current_imports = set(current_info.get('imports', []))
        current_keywords = set(current_info.get('keywords', []))
        
        recommendations = []
        
        for file_path, info in self.index.items():
            if file_path == current_file:
                continue
            
            score = 0
            
            # 共同导入
            common_imports = current_imports.intersection(set(info.get('imports', [])))
            score += len(common_imports) * 3
            
            # 共同关键词
            common_keywords = current_keywords.intersection(set(info.get('keywords', [])))
            score += len(common_keywords)
            
            # 相同扩展名
            if Path(current_file).suffix == Path(file_path).suffix:
                score += 1
            
            if score > 0:
                recommendations.append({
                    'file_path': file_path,
                    'score': score,
                    'common_imports': list(common_imports),
                    'common_keywords': list(common_keywords)[:5]
                })
        
        recommendations.sort(key=lambda x: x['score'], reverse=True)
        return [r['file_path'] for r in recommendations[:max_results]]

# 使用示例
def demo_context_optimization():
    """演示上下文优化"""
    # 创建优化器
    optimizer = ClaudeContextOptimizer('.')
    
    # 创建会话
    session_id = optimizer.create_session()
    
    # 添加文件到上下文
    optimizer.add_file_to_context(session_id, __file__, importance=2.0, category='main')
    
    # 添加文本
    optimizer.add_text_to_context(
        session_id,
        "这是一个示例文本，用于演示上下文管理功能",
        importance=1.5,
        category='example'
    )
    
    # 获取摘要
    summary = optimizer.get_context_summary(session_id)
    print("上下文摘要:", json.dumps(summary, indent=2, ensure_ascii=False))
    
    # 搜索上下文
    results = optimizer.search_context(session_id, "上下文管理")
    print("搜索结果:", json.dumps(results, indent=2, ensure_ascii=False))
    
    # 构建项目索引
    indexer = ProjectIndexer('.')
    indexer.build_index()
    
    # 搜索文件
    search_results = indexer.search_files("performance")
    print("文件搜索结果:", json.dumps(search_results[:3], indent=2, ensure_ascii=False))

# 运行演示
# demo_context_optimization()
```

### 2.2 语义搜索与结构化索引

实现智能的代码搜索和结构化索引系统：

```python
# claude_optimization/semantic_search.py
import ast
import re
import json
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
from dataclasses import dataclass
from collections import defaultdict
import logging

@dataclass
class CodeElement:
    """代码元素"""
    name: str
    element_type: str  # function, class, variable, etc.
    file_path: str
    line_number: int
    scope: str
    signature: Optional[str] = None
    docstring: Optional[str] = None
    complexity: int = 0
    dependencies: List[str] = None

class PythonCodeAnalyzer:
    """Python代码分析器"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def analyze_file(self, file_path: Path) -> List[CodeElement]:
        """分析Python文件"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            analyzer = CodeVisitor(str(file_path))
            analyzer.visit(tree)
            
            return analyzer.elements
        
        except Exception as e:
            self.logger.error(f"分析文件失败 {file_path}: {e}")
            return []

class CodeVisitor(ast.NodeVisitor):
    """AST访问器，提取代码元素"""
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.elements: List[CodeElement] = []
        self.scope_stack: List[str] = []
        self.current_class: Optional[str] = None
    
    def visit_FunctionDef(self, node: ast.FunctionDef):
        """访问函数定义"""
        scope = '.'.join(self.scope_stack) if self.scope_stack else 'global'
        
        # 构建函数签名
        args = []
        for arg in node.args.args:
            args.append(arg.arg)
        signature = f"{node.name}({', '.join(args)})"
        
        # 提取文档字符串
        docstring = ast.get_docstring(node)
        
        # 计算圈复杂度（简化版）
        complexity = self._calculate_complexity(node)
        
        element = CodeElement(
            name=node.name,
            element_type='function',
            file_path=self.file_path,
            line_number=node.lineno,
            scope=scope,
            signature=signature,
            docstring=docstring,
            complexity=complexity,
            dependencies=[]
        )
        
        self.elements.append(element)
        
        # 进入函数作用域
        self.scope_stack.append(node.name)
        self.generic_visit(node)
        self.scope_stack.pop()
    
    def visit_ClassDef(self, node: ast.ClassDef):
        """访问类定义"""
        scope = '.'.join(self.scope_stack) if self.scope_stack else 'global'
        
        # 提取基类
        bases = [self._get_name(base) for base in node.bases]
        signature = f"class {node.name}({', '.join(bases)})" if bases else f"class {node.name}"
        
        docstring = ast.get_docstring(node)
        
        element = CodeElement(
            name=node.name,
            element_type='class',
            file_path=self.file_path,
            line_number=node.lineno,
            scope=scope,
            signature=signature,
            docstring=docstring,
            dependencies=bases
        )
        
        self.elements.append(element)
        
        # 进入类作用域
        old_class = self.current_class
        self.current_class = node.name
        self.scope_stack.append(node.name)
        self.generic_visit(node)
        self.scope_stack.pop()
        self.current_class = old_class
    
    def visit_Assign(self, node: ast.Assign):
        """访问赋值语句（变量定义）"""
        for target in node.targets:
            if isinstance(target, ast.Name):
                scope = '.'.join(self.scope_stack) if self.scope_stack else 'global'
                
                element = CodeElement(
                    name=target.id,
                    element_type='variable',
                    file_path=self.file_path,
                    line_number=node.lineno,
                    scope=scope
                )
                
                self.elements.append(element)
        
        self.generic_visit(node)
    
    def _calculate_complexity(self, node: ast.FunctionDef) -> int:
        """计算圈复杂度（简化版）"""
        complexity = 1  # 基础复杂度
        
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.ExceptHandler)):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1
        
        return complexity
    
    def _get_name(self, node) -> str:
        """获取节点名称"""
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            return f"{self._get_name(node.value)}.{node.attr}"
        else:
            return str(node)

# 语义搜索引擎
class SemanticSearchEngine:
    """语义搜索引擎"""
    
    def __init__(self, project_path: str):
        self.project_path = Path(project_path)
        self.elements: List[CodeElement] = []
        self.index: Dict[str, List[CodeElement]] = defaultdict(list)
        self.logger = logging.getLogger(__name__)
        self.analyzer = PythonCodeAnalyzer()
    
    def build_index(self):
        """构建搜索索引"""
        self.logger.info("构建语义搜索索引")
        
        # 分析所有Python文件
        for py_file in self.project_path.rglob('*.py'):
            if self._should_analyze(py_file):
                elements = self.analyzer.analyze_file(py_file)
                self.elements.extend(elements)
        
        # 构建搜索索引
        self._build_search_index()
        
        self.logger.info(f"索引构建完成，共索引 {len(self.elements)} 个代码元素")
    
    def _should_analyze(self, file_path: Path) -> bool:
        """检查是否应该分析文件"""
        ignore_dirs = {'__pycache__', '.git', 'venv', 'env', '.env'}
        
        for part in file_path.parts:
            if part in ignore_dirs:
                return False
        
        return True
    
    def _build_search_index(self):
        """构建搜索索引"""
        for element in self.elements:
            # 索引名称
            self.index[element.name.lower()].append(element)
            
            # 索引文档字符串中的关键词
            if element.docstring:
                words = self._extract_words(element.docstring)
                for word in words:
                    self.index[word.lower()].append(element)
            
            # 索引签名中的关键词
            if element.signature:
                words = self._extract_words(element.signature)
                for word in words:
                    self.index[word.lower()].append(element)
    
    def _extract_words(self, text: str) -> List[str]:
        """从文本中提取关键词"""
        # 使用正则表达式提取单词
        words = re.findall(r'\b\w+\b', text)
        # 过滤短词和常见词
        common_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        return [word for word in words if len(word) > 2 and word.lower() not in common_words]
    
    def search(self, query: str, element_types: List[str] = None, max_results: int = 20) -> List[Dict[str, Any]]:
        """搜索代码元素"""
        query_words = query.lower().split()
        results = []
        
        for element in self.elements:
            # 过滤元素类型
            if element_types and element.element_type not in element_types:
                continue
            
            score = self._calculate_relevance_score(element, query_words)
            
            if score > 0:
                results.append({
                    'element': element,
                    'score': score,
                    'preview': self._generate_preview(element)
                })
        
        # 按相关性排序
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:max_results]
    
    def _calculate_relevance_score(self, element: CodeElement, query_words: List[str]) -> float:
        """计算相关性得分"""
        score = 0.0
        
        # 名称匹配（高权重）
        element_name = element.name.lower()
        for word in query_words:
            if word in element_name:
                if element_name == word:
                    score += 10  # 精确匹配
                else:
                    score += 5   # 部分匹配
        
        # 文档字符串匹配
        if element.docstring:
            docstring = element.docstring.lower()
            for word in query_words:
                score += docstring.count(word) * 2
        
        # 签名匹配
        if element.signature:
            signature = element.signature.lower()
            for word in query_words:
                score += signature.count(word) * 1.5
        
        # 文件路径匹配
        file_path = element.file_path.lower()
        for word in query_words:
            if word in file_path:
                score += 1
        
        return score
    
    def _generate_preview(self, element: CodeElement) -> str:
        """生成元素预览"""
        preview_parts = []
        
        if element.signature:
            preview_parts.append(f"签名: {element.signature}")
        
        if element.docstring:
            # 截取前100个字符作为预览
            doc_preview = element.docstring[:100]
            if len(element.docstring) > 100:
                doc_preview += "..."
            preview_parts.append(f"说明: {doc_preview}")
        
        preview_parts.append(f"文件: {element.file_path}:{element.line_number}")
        
        if element.complexity > 0:
            preview_parts.append(f"复杂度: {element.complexity}")
        
        return " | ".join(preview_parts)
    
    def find_similar_functions(self, function_name: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """查找相似函数"""
        target_functions = [e for e in self.elements if e.name == function_name and e.element_type == 'function']
        
        if not target_functions:
            return []
        
        target = target_functions[0]
        results = []
        
        for element in self.elements:
            if element.element_type != 'function' or element == target:
                continue
            
            similarity = self._calculate_function_similarity(target, element)
            
            if similarity > 0.3:  # 相似度阈值
                results.append({
                    'element': element,
                    'similarity': similarity,
                    'preview': self._generate_preview(element)
                })
        
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:max_results]
    
    def _calculate_function_similarity(self, func1: CodeElement, func2: CodeElement) -> float:
        """计算函数相似度"""
        similarity = 0.0
        
        # 名称相似度
        name1_words = set(re.findall(r'\w+', func1.name.lower()))
        name2_words = set(re.findall(r'\w+', func2.name.lower()))
        
        if name1_words and name2_words:
            name_similarity = len(name1_words & name2_words) / len(name1_words | name2_words)
            similarity += name_similarity * 0.4
        
        # 文档字符串相似度
        if func1.docstring and func2.docstring:
            doc1_words = set(self._extract_words(func1.docstring))
            doc2_words = set(self._extract_words(func2.docstring))
            
            if doc1_words and doc2_words:
                doc_similarity = len(doc1_words & doc2_words) / len(doc1_words | doc2_words)
                similarity += doc_similarity * 0.3
        
        # 复杂度相似度
        if func1.complexity > 0 and func2.complexity > 0:
            complexity_diff = abs(func1.complexity - func2.complexity)
            max_complexity = max(func1.complexity, func2.complexity)
            complexity_similarity = 1 - (complexity_diff / max_complexity)
            similarity += complexity_similarity * 0.2
        
        # 作用域相似度
        if func1.scope == func2.scope:
            similarity += 0.1
        
        return similarity
    
    def get_function_call_graph(self) -> Dict[str, List[str]]:
        """获取函数调用图（简化版）"""
        call_graph = defaultdict(list)
        
        # 这里需要更复杂的AST分析来识别函数调用
        # 简化实现仅作示例
        
        return dict(call_graph)
    
    def analyze_code_quality(self) -> Dict[str, Any]:
        """分析代码质量"""
        total_functions = len([e for e in self.elements if e.element_type == 'function'])
        total_classes = len([e for e in self.elements if e.element_type == 'class'])
        
        # 计算平均复杂度
        complexities = [e.complexity for e in self.elements if e.complexity > 0]
        avg_complexity = sum(complexities) / len(complexities) if complexities else 0
        
        # 识别高复杂度函数
        high_complexity_functions = [
            e for e in self.elements 
            if e.element_type == 'function' and e.complexity > 10
        ]
        
        # 识别缺少文档的函数
        undocumented_functions = [
            e for e in self.elements
            if e.element_type == 'function' and not e.docstring
        ]
        
        return {
            'total_elements': len(self.elements),
            'total_functions': total_functions,
            'total_classes': total_classes,
            'average_complexity': avg_complexity,
            'high_complexity_functions': len(high_complexity_functions),
            'undocumented_functions': len(undocumented_functions),
            'documentation_coverage': 1 - (len(undocumented_functions) / max(total_functions, 1)),
            'quality_issues': {
                'high_complexity': [
                    {
                        'name': f.name,
                        'file': f.file_path,
                        'line': f.line_number,
                        'complexity': f.complexity
                    }
                    for f in high_complexity_functions[:10]  # 前10个
                ],
                'missing_docs': [
                    {
                        'name': f.name,
                        'file': f.file_path,
                        'line': f.line_number
                    }
                    for f in undocumented_functions[:10]  # 前10个
                ]
            }
        }

# 使用示例
def demo_semantic_search():
    """演示语义搜索"""
    search_engine = SemanticSearchEngine('.')
    search_engine.build_index()
    
    # 搜索函数
    results = search_engine.search('performance', element_types=['function'])
    print("搜索结果:")
    for result in results[:5]:
        element = result['element']
        print(f"- {element.name} (得分: {result['score']:.2f})")
        print(f"  {result['preview']}")
        print()
    
    # 代码质量分析
    quality = search_engine.analyze_code_quality()
    print("代码质量分析:")
    print(json.dumps(quality, indent=2, ensure_ascii=False))

# 运行演示
# demo_semantic_search()
```

## 代码性能分析与优化

### 3.1 算法复杂度分析

实现自动化的算法复杂度分析工具：

```python
# performance/complexity_analyzer.py
import ast
import re
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from enum import Enum
import logging

class ComplexityType(Enum):
    """复杂度类型"""
    CONSTANT = "O(1)"
    LOGARITHMIC = "O(log n)"
    LINEAR = "O(n)"
    LINEARITHMIC = "O(n log n)"
    QUADRATIC = "O(n²)"
    CUBIC = "O(n³)"
    EXPONENTIAL = "O(2^n)"
    FACTORIAL = "O(n!)"
    UNKNOWN = "O(?)"

@dataclass
class ComplexityAnalysis:
    """复杂度分析结果"""
    function_name: str
    time_complexity: ComplexityType
    space_complexity: ComplexityType
    confidence: float  # 0-1之间的置信度
    analysis_details: Dict[str, Any]
    optimization_suggestions: List[str]

class AlgorithmComplexityAnalyzer:
    """算法复杂度分析器"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def analyze_function(self, func_node: ast.FunctionDef, source_code: str) -> ComplexityAnalysis:
        """分析函数的时间和空间复杂度"""
        analyzer = ComplexityVisitor()
        analyzer.visit(func_node)
        
        # 分析时间复杂度
        time_complexity = self._determine_time_complexity(analyzer)
        
        # 分析空间复杂度
        space_complexity = self._determine_space_complexity(analyzer)
        
        # 生成优化建议
        suggestions = self._generate_optimization_suggestions(analyzer, time_complexity, space_complexity)
        
        return ComplexityAnalysis(
            function_name=func_node.name,
            time_complexity=time_complexity,
            space_complexity=space_complexity,
            confidence=self._calculate_confidence(analyzer),
            analysis_details=analyzer.get_analysis_details(),
            optimization_suggestions=suggestions
        )
    
    def _determine_time_complexity(self, analyzer) -> ComplexityType:
        """确定时间复杂度"""
        details = analyzer.get_analysis_details()
        
        # 检查嵌套循环
        max_nested_loops = details.get('max_nested_loops', 0)
        has_recursive_calls = details.get('recursive_calls', 0) > 0
        
        if has_recursive_calls:
            # 递归函数需要更复杂的分析
            recursive_pattern = details.get('recursive_pattern', 'unknown')
            if recursive_pattern == 'divide_and_conquer':
                return ComplexityType.LINEARITHMIC
            elif recursive_pattern == 'tree_recursion':
                return ComplexityType.EXPONENTIAL
            else:
                return ComplexityType.UNKNOWN
        
        elif max_nested_loops >= 3:
            return ComplexityType.CUBIC
        elif max_nested_loops == 2:
            return ComplexityType.QUADRATIC
        elif max_nested_loops == 1:
            # 检查是否有对数操作
            if details.get('has_logarithmic_operations', False):
                return ComplexityType.LINEARITHMIC
            else:
                return ComplexityType.LINEAR
        else:
            # 检查是否有对数操作
            if details.get('has_logarithmic_operations', False):
                return ComplexityType.LOGARITHMIC
            else:
                return ComplexityType.CONSTANT
    
    def _determine_space_complexity(self, analyzer) -> ComplexityType:
        """确定空间复杂度"""
        details = analyzer.get_analysis_details()
        
        recursive_depth = details.get('max_recursive_depth', 0)
        data_structure_growth = details.get('data_structure_growth', 'constant')
        
        if recursive_depth > 0:
            if data_structure_growth == 'linear':
                return ComplexityType.LINEAR
            else:
                return ComplexityType.LINEAR  # 递归调用栈
        
        elif data_structure_growth == 'quadratic':
            return ComplexityType.QUADRATIC
        elif data_structure_growth == 'linear':
            return ComplexityType.LINEAR
        else:
            return ComplexityType.CONSTANT
    
    def _calculate_confidence(self, analyzer) -> float:
        """计算分析置信度"""
        details = analyzer.get_analysis_details()
        
        confidence = 0.5  # 基础置信度
        
        # 提高置信度的因素
        if details.get('has_clear_loops', False):
            confidence += 0.2
        
        if details.get('has_clear_recursion', False):
            confidence += 0.2
        
        if details.get('has_standard_patterns', False):
            confidence += 0.1
        
        # 降低置信度的因素
        if details.get('has_complex_control_flow', False):
            confidence -= 0.1
        
        if details.get('has_external_calls', False):
            confidence -= 0.1
        
        return max(0.0, min(1.0, confidence))
    
    def _generate_optimization_suggestions(self, analyzer, time_complexity: ComplexityType, 
                                         space_complexity: ComplexityType) -> List[str]:
        """生成优化建议"""
        suggestions = []
        details = analyzer.get_analysis_details()
        
        if time_complexity == ComplexityType.QUADRATIC:
            suggestions.append("考虑使用更高效的算法，如哈希表或排序+双指针技术")
            suggestions.append("检查是否可以通过缓存避免重复计算")
        
        elif time_complexity == ComplexityType.CUBIC:
            suggestions.append("三重嵌套循环通常可以优化，考虑动态规划或分治算法")
        
        elif time_complexity == ComplexityType.EXPONENTIAL:
            suggestions.append("指数时间复杂度通常可以通过动态规划或记忆化优化")
            suggestions.append("考虑是否可以使用贪心算法或启发式方法")
        
        if details.get('max_nested_loops', 0) > 2:
            suggestions.append("减少循环嵌套层数，考虑提取内层循环为独立函数")
        
        if space_complexity in [ComplexityType.QUADRATIC, ComplexityType.CUBIC]:
            suggestions.append("优化数据结构使用，考虑原地算法或流式处理")
        
        if details.get('recursive_calls', 0) > 0 and not details.get('tail_recursive', False):
            suggestions.append("考虑将递归转换为迭代以减少栈空间使用")
        
        return suggestions

class ComplexityVisitor(ast.NodeVisitor):
    """复杂度分析访问器"""
    
    def __init__(self):
        self.loop_depth = 0
        self.max_loop_depth = 0
        self.recursive_calls = 0
        self.has_logarithmic_ops = False
        self.data_structures = []
        self.control_flow_complexity = 0
        self.function_calls = []
    
    def visit_For(self, node):
        """访问for循环"""
        self.loop_depth += 1
        self.max_loop_depth = max(self.max_loop_depth, self.loop_depth)
        self.generic_visit(node)
        self.loop_depth -= 1
    
    def visit_While(self, node):
        """访问while循环"""
        self.loop_depth += 1
        self.max_loop_depth = max(self.max_loop_depth, self.loop_depth)
        self.generic_visit(node)
        self.loop_depth -= 1
    
    def visit_Call(self, node):
        """访问函数调用"""
        if isinstance(node.func, ast.Name):
            func_name = node.func.id
            self.function_calls.append(func_name)
            
            # 检查是否是递归调用（简化检测）
            if hasattr(self, 'current_function_name') and func_name == self.current_function_name:
                self.recursive_calls += 1
            
            # 检查对数操作
            if func_name in ['log', 'log2', 'log10', 'bisect', 'binary_search']:
                self.has_logarithmic_ops = True
        
        self.generic_visit(node)
    
    def visit_If(self, node):
        """访问if语句"""
        self.control_flow_complexity += 1
        self.generic_visit(node)
    
    def visit_ListComp(self, node):
        """访问列表推导式"""
        # 列表推导式相当于一层循环
        self.loop_depth += 1
        self.max_loop_depth = max(self.max_loop_depth, self.loop_depth)
        self.generic_visit(node)
        self.loop_depth -= 1
    
    def visit_Assign(self, node):
        """访问赋值语句"""
        # 检查数据结构创建
        if isinstance(node.value, (ast.List, ast.Dict, ast.Set)):
            self.data_structures.append(type(node.value).__name__)
        
        self.generic_visit(node)
    
    def get_analysis_details(self) -> Dict[str, Any]:
        """获取分析详情"""
        return {
            'max_nested_loops': self.max_loop_depth,
            'recursive_calls': self.recursive_calls,
            'has_logarithmic_operations': self.has_logarithmic_ops,
            'control_flow_complexity': self.control_flow_complexity,
            'data_structures_created': len(self.data_structures),
            'function_calls': self.function_calls,
            'has_clear_loops': self.max_loop_depth > 0,
            'has_clear_recursion': self.recursive_calls > 0,
            'has_standard_patterns': self._detect_standard_patterns(),
            'has_complex_control_flow': self.control_flow_complexity > 5,
            'has_external_calls': len(self.function_calls) > 0,
            'data_structure_growth': self._estimate_data_growth()
        }
    
    def _detect_standard_patterns(self) -> bool:
        """检测标准算法模式"""
        # 简化实现
        return self.max_loop_depth <= 2 and self.recursive_calls == 0
    
    def _estimate_data_growth(self) -> str:
        """估计数据结构增长"""
        if self.max_loop_depth >= 2:
            return 'quadratic'
        elif self.max_loop_depth == 1 or len(self.data_structures) > 0:
            return 'linear'
        else:
            return 'constant'

# 性能优化建议器
class PerformanceOptimizer:
    """性能优化建议器"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.complexity_analyzer = AlgorithmComplexityAnalyzer()
    
    def analyze_code_file(self, file_path: str) -> List[ComplexityAnalysis]:
        """分析代码文件中的所有函数"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                source_code = f.read()
            
            tree = ast.parse(source_code)
            results = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    analysis = self.complexity_analyzer.analyze_function(node, source_code)
                    results.append(analysis)
            
            return results
        
        except Exception as e:
            self.logger.error(f"分析文件失败 {file_path}: {e}")
            return []
    
    def generate_optimization_report(self, analyses: List[ComplexityAnalysis]) -> Dict[str, Any]:
        """生成优化报告"""
        if not analyses:
            return {"error": "没有分析结果"}
        
        # 统计复杂度分布
        time_complexity_dist = {}
        space_complexity_dist = {}
        
        high_complexity_functions = []
        optimization_candidates = []
        
        for analysis in analyses:
            # 统计时间复杂度分布
            tc = analysis.time_complexity.value
            time_complexity_dist[tc] = time_complexity_dist.get(tc, 0) + 1
            
            # 统计空间复杂度分布
            sc = analysis.space_complexity.value
            space_complexity_dist[sc] = space_complexity_dist.get(sc, 0) + 1
            
            # 识别高复杂度函数
            if analysis.time_complexity in [ComplexityType.QUADRATIC, ComplexityType.CUBIC, ComplexityType.EXPONENTIAL]:
                high_complexity_functions.append({
                    'name': analysis.function_name,
                    'time_complexity': tc,
                    'confidence': analysis.confidence,
                    'suggestions': analysis.optimization_suggestions
                })
            
            # 识别优化候选
            if analysis.optimization_suggestions:
                optimization_candidates.append({
                    'name': analysis.function_name,
                    'time_complexity': tc,
                    'space_complexity': sc,
                    'suggestions': analysis.optimization_suggestions[:3]  # 前3个建议
                })
        
        # 计算平均置信度
        avg_confidence = sum(a.confidence for a in analyses) / len(analyses)
        
        return {
            'summary': {
                'total_functions': len(analyses),
                'high_complexity_functions': len(high_complexity_functions),
                'functions_with_suggestions': len(optimization_candidates),
                'average_confidence': avg_confidence
            },
            'complexity_distribution': {
                'time_complexity': time_complexity_dist,
                'space_complexity': space_complexity_dist
            },
            'high_complexity_functions': high_complexity_functions,
            'optimization_candidates': optimization_candidates[:10],  # 前10个
            'recommendations': self._generate_general_recommendations(analyses)
        }
    
    def _generate_general_recommendations(self, analyses: List[ComplexityAnalysis]) -> List[str]:
        """生成通用优化建议"""
        recommendations = []
        
        # 统计常见问题
        quadratic_count = sum(1 for a in analyses if a.time_complexity == ComplexityType.QUADRATIC)
        recursive_count = sum(1 for a in analyses if 'recursive_calls' in a.analysis_details and a.analysis_details['recursive_calls'] > 0)
        
        if quadratic_count > len(analyses) * 0.2:  # 超过20%的函数是二次复杂度
            recommendations.append("项目中存在较多二次时间复杂度的函数，建议进行算法优化")
        
        if recursive_count > len(analyses) * 0.3:  # 超过30%的函数使用递归
            recommendations.append("项目中递归使用较多，注意栈溢出风险，考虑尾递归优化")
        
        # 通用建议
        recommendations.extend([
            "定期进行性能测试，使用profiler工具识别瓶颈",
            "考虑使用缓存机制减少重复计算",
            "对于数据密集型操作，考虑使用流式处理",
            "合理使用数据结构，根据使用场景选择最适合的容器"
        ])
        
        return recommendations

# 使用示例和测试代码
def example_linear_function(arr):
    """线性时间复杂度示例 - O(n)"""
    result = []
    for item in arr:
        result.append(item * 2)
    return result

def example_quadratic_function(arr):
    """二次时间复杂度示例 - O(n²)"""
    result = []
    for i in range(len(arr)):
        for j in range(len(arr)):
            if i != j:
                result.append(arr[i] + arr[j])
    return result

def example_recursive_function(n):
    """递归函数示例"""
    if n <= 1:
        return n
    return example_recursive_function(n - 1) + example_recursive_function(n - 2)

def demo_complexity_analysis():
    """演示复杂度分析"""
    # 分析当前文件
    optimizer = PerformanceOptimizer()
    analyses = optimizer.analyze_code_file(__file__)
    
    print("复杂度分析结果:")
    for analysis in analyses[:5]:  # 显示前5个
        print(f"\n函数: {analysis.function_name}")
        print(f"时间复杂度: {analysis.time_complexity.value}")
        print(f"空间复杂度: {analysis.space_complexity.value}")
        print(f"置信度: {analysis.confidence:.2f}")
        if analysis.optimization_suggestions:
            print("优化建议:")
            for suggestion in analysis.optimization_suggestions[:2]:
                print(f"  - {suggestion}")
    
    # 生成优化报告
    report = optimizer.generate_optimization_report(analyses)
    print("\n优化报告:")
    print(json.dumps(report, indent=2, ensure_ascii=False))

# 运行演示
# demo_complexity_analysis()
```

## 内存管理与资源优化

### 4.1 内存泄漏检测

实现内存泄漏检测和资源管理优化系统：

```python
# performance/memory_optimizer.py
import gc
import sys
import weakref
import threading
import time
import traceback
from typing import Dict, List, Any, Optional, Set, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict
import logging
import psutil
import os

@dataclass
class MemorySnapshot:
    """内存快照"""
    timestamp: datetime
    total_memory: int  # 总内存使用（字节）
    objects_count: int  # 对象总数
    gc_count: Dict[int, int]  # 各代垃圾回收次数
    top_objects: Dict[str, int]  # 按类型统计的对象数量
    process_memory: float  # 进程内存使用（MB）

@dataclass
class ObjectTracker:
    """对象追踪器"""
    obj_type: str
    creation_time: datetime
    size: int
    traceback_info: Optional[str] = None
    reference_count: int = 0

class MemoryLeakDetector:
    """内存泄漏检测器"""
    
    def __init__(self, monitoring_interval: float = 5.0):
        self.monitoring_interval = monitoring_interval
        self.snapshots: List[MemorySnapshot] = []
        self.tracked_objects: Dict[int, ObjectTracker] = {}
        self.weak_refs: Set[weakref.ref] = set()
        self.is_monitoring = False
        self.monitor_thread: Optional[threading.Thread] = None
        self.logger = logging.getLogger(__name__)
        self.process = psutil.Process()
        
        # 内存泄漏阈值
        self.leak_thresholds = {
            'memory_growth_rate': 50 * 1024 * 1024,  # 50MB/minute
            'object_growth_rate': 10000,  # 10k objects/minute
            'consecutive_growth_periods': 3
        }
    
    def start_monitoring(self):
        """开始内存监控"""
        if self.is_monitoring:
            return
        
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        self.logger.info("内存泄漏监控已启动")
    
    def stop_monitoring(self):
        """停止内存监控"""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        self.logger.info("内存泄漏监控已停止")
    
    def _monitor_loop(self):
        """监控主循环"""
        while self.is_monitoring:
            try:
                snapshot = self._take_snapshot()
                self.snapshots.append(snapshot)
                
                # 保持快照数量在合理范围内
                if len(self.snapshots) > 100:
                    self.snapshots = self.snapshots[-50:]
                
                # 检测潜在泄漏
                if len(self.snapshots) >= self.leak_thresholds['consecutive_growth_periods']:
                    self._check_for_leaks()
                
                time.sleep(self.monitoring_interval)
            
            except Exception as e:
                self.logger.error(f"内存监控出错: {e}")
    
    def _take_snapshot(self) -> MemorySnapshot:
        """拍摄内存快照"""
        # 强制垃圾回收
        gc.collect()
        
        # 获取对象统计
        object_counts = defaultdict(int)
        total_objects = 0
        
        for obj in gc.get_objects():
            obj_type = type(obj).__name__
            object_counts[obj_type] += 1
            total_objects += 1
        
        # 获取进程内存使用
        process_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        
        return MemorySnapshot(
            timestamp=datetime.now(),
            total_memory=sys.getsizeof(gc.get_objects()),
            objects_count=total_objects,
            gc_count=dict(enumerate(gc.get_count())),
            top_objects=dict(sorted(object_counts.items(), key=lambda x: x[1], reverse=True)[:20]),
            process_memory=process_memory
        )
    
    def _check_for_leaks(self):
        """检查内存泄漏"""
        if len(self.snapshots) < 3:
            return
        
        recent_snapshots = self.snapshots[-3:]
        
        # 检查内存增长趋势
        memory_growth = recent_snapshots[-1].process_memory - recent_snapshots[0].process_memory
        time_diff = (recent_snapshots[-1].timestamp - recent_snapshots[0].timestamp).total_seconds() / 60  # 分钟
        
        if time_diff > 0:
            growth_rate = memory_growth / time_diff  # MB/minute
            
            if growth_rate > self.leak_thresholds['memory_growth_rate'] / (1024 * 1024):  # 转换为MB
                self.logger.warning(f"检测到可能的内存泄漏: 增长率 {growth_rate:.2f} MB/minute")
                self._analyze_potential_leak()
    
    def _analyze_potential_leak(self):
        """分析潜在的内存泄漏"""
        if len(self.snapshots) < 2:
            return
        
        current = self.snapshots[-1]
        previous = self.snapshots[-2]
        
        # 找出增长最快的对象类型
        growing_objects = {}
        for obj_type, current_count in current.top_objects.items():
            previous_count = previous.top_objects.get(obj_type, 0)
            growth = current_count - previous_count
            if growth > 0:
                growing_objects[obj_type] = growth
        
        if growing_objects:
            sorted_growth = sorted(growing_objects.items(), key=lambda x: x[1], reverse=True)
            self.logger.warning(f"快速增长的对象类型: {sorted_growth[:5]}")
    
    def track_object(self, obj: Any, include_traceback: bool = False):
        """追踪特定对象"""
        obj_id = id(obj)
        obj_type = type(obj).__name__
        obj_size = sys.getsizeof(obj)
        
        traceback_info = None
        if include_traceback:
            traceback_info = ''.join(traceback.format_stack())
        
        tracker = ObjectTracker(
            obj_type=obj_type,
            creation_time=datetime.now(),
            size=obj_size,
            traceback_info=traceback_info,
            reference_count=sys.getrefcount(obj)
        )
        
        self.tracked_objects[obj_id] = tracker
        
        # 使用弱引用追踪对象生命周期
        def cleanup_callback(ref):
            if obj_id in self.tracked_objects:
                del self.tracked_objects[obj_id]
            self.weak_refs.discard(ref)
        
        weak_ref = weakref.ref(obj, cleanup_callback)
        self.weak_refs.add(weak_ref)
    
    def get_memory_report(self) -> Dict[str, Any]:
        """获取内存报告"""
        if not self.snapshots:
            return {"error": "没有内存快照数据"}
        
        current_snapshot = self.snapshots[-1]
        
        # 计算内存趋势
        trend_analysis = {}
        if len(self.snapshots) >= 2:
            first_snapshot = self.snapshots[0]
            total_time = (current_snapshot.timestamp - first_snapshot.timestamp).total_seconds()
            
            if total_time > 0:
                memory_trend = (current_snapshot.process_memory - first_snapshot.process_memory) / total_time * 60  # MB/minute
                objects_trend = (current_snapshot.objects_count - first_snapshot.objects_count) / total_time * 60  # objects/minute
                
                trend_analysis = {
                    'memory_growth_rate_mb_per_minute': memory_trend,
                    'objects_growth_rate_per_minute': objects_trend,
                    'monitoring_duration_minutes': total_time / 60
                }
        
        # 垃圾回收统计
        gc_stats = {
            'current_counts': current_snapshot.gc_count,
            'collections_performed': sum(gc.get_stats()[i]['collections'] for i in range(3) if i < len(gc.get_stats()))
        }
        
        return {
            'timestamp': current_snapshot.timestamp.isoformat(),
            'current_memory_mb': current_snapshot.process_memory,
            'total_objects': current_snapshot.objects_count,
            'top_object_types': current_snapshot.top_objects,
            'trend_analysis': trend_analysis,
            'garbage_collection': gc_stats,
            'tracked_objects_count': len(self.tracked_objects),
            'potential_issues': self._identify_potential_issues()
        }
    
    def _identify_potential_issues(self) -> List[str]:
        """识别潜在问题"""
        issues = []
        
        if len(self.snapshots) >= 3:
            recent_growth = self.snapshots[-1].process_memory - self.snapshots[-3].process_memory
            if recent_growth > 100:  # 100MB
                issues.append(f"最近内存增长 {recent_growth:.1f}MB，需要关注")
        
        # 检查长期存活的追踪对象
        now = datetime.now()
        long_lived_objects = [
            tracker for tracker in self.tracked_objects.values()
            if (now - tracker.creation_time).total_seconds() > 3600  # 超过1小时
        ]
        
        if long_lived_objects:
            issues.append(f"发现 {len(long_lived_objects)} 个长期存活的追踪对象")
        
        return issues

# 资源管理器
class ResourceManager:
    """资源管理器"""
    
    def __init__(self):
        self.managed_resources: Dict[str, Any] = {}
        self.resource_locks: Dict[str, threading.Lock] = {}
        self.cleanup_callbacks: Dict[str, Callable] = {}
        self.logger = logging.getLogger(__name__)
    
    def register_resource(self, resource_id: str, resource: Any, cleanup_callback: Optional[Callable] = None):
        """注册资源"""
        with self._get_lock(resource_id):
            self.managed_resources[resource_id] = resource
            if cleanup_callback:
                self.cleanup_callbacks[resource_id] = cleanup_callback
            
            self.logger.info(f"注册资源: {resource_id}")
    
    def get_resource(self, resource_id: str) -> Optional[Any]:
        """获取资源"""
        return self.managed_resources.get(resource_id)
    
    def release_resource(self, resource_id: str):
        """释放资源"""
        with self._get_lock(resource_id):
            if resource_id in self.managed_resources:
                resource = self.managed_resources.pop(resource_id)
                
                # 执行清理回调
                if resource_id in self.cleanup_callbacks:
                    try:
                        self.cleanup_callbacks[resource_id](resource)
                        del self.cleanup_callbacks[resource_id]
                    except Exception as e:
                        self.logger.error(f"清理资源 {resource_id} 时出错: {e}")
                
                self.logger.info(f"释放资源: {resource_id}")
    
    def _get_lock(self, resource_id: str) -> threading.Lock:
        """获取资源锁"""
        if resource_id not in self.resource_locks:
            self.resource_locks[resource_id] = threading.Lock()
        return self.resource_locks[resource_id]
    
    def cleanup_all(self):
        """清理所有资源"""
        resource_ids = list(self.managed_resources.keys())
        for resource_id in resource_ids:
            self.release_resource(resource_id)

# 内存池管理
class MemoryPool:
    """内存池管理器"""
    
    def __init__(self, chunk_size: int = 1024, max_chunks: int = 100):
        self.chunk_size = chunk_size
        self.max_chunks = max_chunks
        self.available_chunks: List[bytearray] = []
        self.allocated_chunks: Set[bytearray] = set()
        self.lock = threading.Lock()
        self.logger = logging.getLogger(__name__)
        
        # 预分配一些块
        self._preallocate_chunks(min(10, max_chunks))
    
    def _preallocate_chunks(self, count: int):
        """预分配内存块"""
        for _ in range(count):
            if len(self.available_chunks) + len(self.allocated_chunks) < self.max_chunks:
                chunk = bytearray(self.chunk_size)
                self.available_chunks.append(chunk)
    
    def acquire_chunk(self) -> Optional[bytearray]:
        """获取内存块"""
        with self.lock:
            if not self.available_chunks:
                # 尝试分配新块
                if len(self.allocated_chunks) < self.max_chunks:
                    chunk = bytearray(self.chunk_size)
                    self.allocated_chunks.add(chunk)
                    return chunk
                else:
                    self.logger.warning("内存池已满，无法分配新块")
                    return None
            
            chunk = self.available_chunks.pop()
            self.allocated_chunks.add(chunk)
            return chunk
    
    def release_chunk(self, chunk: bytearray):
        """释放内存块"""
        with self.lock:
            if chunk in self.allocated_chunks:
                self.allocated_chunks.remove(chunk)
                # 清零数据
                chunk[:] = b'\x00' * len(chunk)
                self.available_chunks.append(chunk)
    
    def get_pool_stats(self) -> Dict[str, Any]:
        """获取内存池统计"""
        with self.lock:
            return {
                'total_chunks': len(self.available_chunks) + len(self.allocated_chunks),
                'available_chunks': len(self.available_chunks),
                'allocated_chunks': len(self.allocated_chunks),
                'chunk_size': self.chunk_size,
                'utilization': len(self.allocated_chunks) / self.max_chunks * 100
            }

# 使用示例和测试
def demo_memory_optimization():
    """演示内存优化"""
    # 创建内存泄漏检测器
    detector = MemoryLeakDetector(monitoring_interval=1.0)
    detector.start_monitoring()
    
    # 模拟内存使用
    test_objects = []
    
    print("开始内存测试...")
    for i in range(10):
        # 创建一些对象
        large_list = [j for j in range(1000)]
        test_objects.append(large_list)
        
        # 追踪特定对象
        if i % 3 == 0:
            detector.track_object(large_list, include_traceback=True)
        
        time.sleep(0.5)
    
    # 等待几个监控周期
    time.sleep(3)
    
    # 获取内存报告
    report = detector.get_memory_report()
    print("内存报告:")
    print(json.dumps(report, indent=2, ensure_ascii=False, default=str))
    
    # 测试内存池
    print("\n测试内存池:")
    memory_pool = MemoryPool(chunk_size=1024, max_chunks=5)
    
    # 分配内存块
    chunks = []
    for i in range(7):  # 超过最大数量
        chunk = memory_pool.acquire_chunk()
        if chunk:
            chunks.append(chunk)
            print(f"分配内存块 {i+1}: 成功")
        else:
            print(f"分配内存块 {i+1}: 失败")
    
    # 获取内存池状态
    pool_stats = memory_pool.get_pool_stats()
    print(f"内存池统计: {pool_stats}")
    
    # 释放一些内存块
    for chunk in chunks[:3]:
        memory_pool.release_chunk(chunk)
    
    pool_stats = memory_pool.get_pool_stats()
    print(f"释放后内存池统计: {pool_stats}")
    
    # 停止监控
    detector.stop_monitoring()

# 运行演示
# demo_memory_optimization()
```

## 总结

本文详细介绍了使用 Claude Code 进行性能优化和调试的完整体系，包括：

1. **性能监控体系** - 建立全面的性能指标收集和分析框架
2. **Claude Code优化** - 智能上下文管理和语义搜索系统  
3. **算法复杂度分析** - 自动化的复杂度分析和优化建议
4. **内存管理优化** - 内存泄漏检测和资源管理系统
5. **性能调试工具** - 集成化的调试和性能分析工具

通过这些技术和工具，开发者可以：
- 系统化地监控和分析应用性能
- 及时发现和解决性能瓶颈
- 优化算法和数据结构选择  
- 有效管理内存和系统资源
- 建立持续的性能改进流程

Claude Code 作为智能编程助手，不仅能帮助实现这些复杂的优化系统，还能在开发过程中提供实时的性能建议和调试支持，大大提升开发效率和代码质量。
```